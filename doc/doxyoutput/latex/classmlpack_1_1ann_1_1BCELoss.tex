\section{B\+C\+E\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$ Class Template Reference}
\label{classmlpack_1_1ann_1_1BCELoss}\index{B\+C\+E\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$@{B\+C\+E\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$}}


The binary-\/cross-\/entropy performance function measures the Binary Cross Entropy between the target and the output.  


\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\textbf{ B\+C\+E\+Loss} (const double eps=1e-\/10, const bool reduction=true)
\begin{DoxyCompactList}\small\item\em Create the Binary\+Cross\+Entropy\+Loss object. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Prediction\+Type , typename Target\+Type , typename Loss\+Type $>$ }\\void \textbf{ Backward} (const Prediction\+Type \&prediction, const Target\+Type \&target, Loss\+Type \&loss)
\begin{DoxyCompactList}\small\item\em Ordinary feed backward pass of a neural network. \end{DoxyCompactList}\item 
double \textbf{ Eps} () const
\begin{DoxyCompactList}\small\item\em Get the epsilon. \end{DoxyCompactList}\item 
double \& \textbf{ Eps} ()
\begin{DoxyCompactList}\small\item\em Modify the epsilon. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Prediction\+Type , typename Target\+Type $>$ }\\Prediction\+Type\+::elem\+\_\+type \textbf{ Forward} (const Prediction\+Type \&prediction, const Target\+Type \&target)
\begin{DoxyCompactList}\small\item\em Computes the cross-\/entropy function. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Output\+Parameter} () const
\begin{DoxyCompactList}\small\item\em Get the output parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Output\+Parameter} ()
\begin{DoxyCompactList}\small\item\em Modify the output parameter. \end{DoxyCompactList}\item 
bool \textbf{ Reduction} () const
\begin{DoxyCompactList}\small\item\em Get the reduction. \end{DoxyCompactList}\item 
bool \& \textbf{ Reduction} ()
\begin{DoxyCompactList}\small\item\em Set the reduction. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Archive $>$ }\\void \textbf{ serialize} (Archive \&ar, const uint32\+\_\+t)
\begin{DoxyCompactList}\small\item\em Serialize the layer. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Input\+Data\+Type = arma\+::mat, typename Output\+Data\+Type = arma\+::mat$>$\newline
class mlpack\+::ann\+::\+B\+C\+E\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$}

The binary-\/cross-\/entropy performance function measures the Binary Cross Entropy between the target and the output. 


\begin{DoxyTemplParams}{Template Parameters}
{\em Input\+Data\+Type} & Type of the input data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
{\em Output\+Data\+Type} & Type of the output data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
\end{DoxyTemplParams}


Definition at line 33 of file binary\+\_\+cross\+\_\+entropy\+\_\+loss.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\mbox{\label{classmlpack_1_1ann_1_1BCELoss_a402089b368c1139c98bf07852e36987f}} 
\index{mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}!B\+C\+E\+Loss@{B\+C\+E\+Loss}}
\index{B\+C\+E\+Loss@{B\+C\+E\+Loss}!mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}}
\subsubsection{B\+C\+E\+Loss()}
{\footnotesize\ttfamily \textbf{ B\+C\+E\+Loss} (\begin{DoxyParamCaption}\item[{const double}]{eps = {\ttfamily 1e-\/10},  }\item[{const bool}]{reduction = {\ttfamily true} }\end{DoxyParamCaption})}



Create the Binary\+Cross\+Entropy\+Loss object. 


\begin{DoxyParams}{Parameters}
{\em eps} & The minimum value used for computing logarithms and denominators in a numerically stable way. \\
\hline
{\em reduction} & Reduction type. If true, it returns the mean of the loss. Else, it returns the sum. \\
\hline
\end{DoxyParams}


\subsection{Member Function Documentation}
\mbox{\label{classmlpack_1_1ann_1_1BCELoss_add41dbaf358dc099750dc6064cb7e0d7}} 
\index{mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}!Backward@{Backward}}
\index{Backward@{Backward}!mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}}
\subsubsection{Backward()}
{\footnotesize\ttfamily void Backward (\begin{DoxyParamCaption}\item[{const Prediction\+Type \&}]{prediction,  }\item[{const Target\+Type \&}]{target,  }\item[{Loss\+Type \&}]{loss }\end{DoxyParamCaption})}



Ordinary feed backward pass of a neural network. 


\begin{DoxyParams}{Parameters}
{\em prediction} & Predictions used for evaluating the specified loss function. \\
\hline
{\em target} & The target vector. \\
\hline
{\em loss} & The calculated error. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1BCELoss_a6b1a203165d5e3a6a30534a95c5ea339}} 
\index{mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}!Eps@{Eps}}
\index{Eps@{Eps}!mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}}
\subsubsection{Eps()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily double Eps (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the epsilon. 



Definition at line 76 of file binary\+\_\+cross\+\_\+entropy\+\_\+loss.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1BCELoss_ab50f77742d49705ce1a2a0fa1feff24e}} 
\index{mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}!Eps@{Eps}}
\index{Eps@{Eps}!mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}}
\subsubsection{Eps()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily double\& Eps (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the epsilon. 



Definition at line 78 of file binary\+\_\+cross\+\_\+entropy\+\_\+loss.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1BCELoss_ab3640059898ea76c13709b8099316fe8}} 
\index{mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}!Forward@{Forward}}
\index{Forward@{Forward}!mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}}
\subsubsection{Forward()}
{\footnotesize\ttfamily Prediction\+Type\+::elem\+\_\+type Forward (\begin{DoxyParamCaption}\item[{const Prediction\+Type \&}]{prediction,  }\item[{const Target\+Type \&}]{target }\end{DoxyParamCaption})}



Computes the cross-\/entropy function. 


\begin{DoxyParams}{Parameters}
{\em prediction} & Predictions used for evaluating the specified loss function. \\
\hline
{\em target} & The target vector. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1BCELoss_a8bae962cc603d1cab8d80ec78f8d505d}} 
\index{mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}}
\subsubsection{Output\+Parameter()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the output parameter. 



Definition at line 71 of file binary\+\_\+cross\+\_\+entropy\+\_\+loss.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1BCELoss_a21d5f745f02c709625a4ee0907f004a5}} 
\index{mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}}
\subsubsection{Output\+Parameter()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the output parameter. 



Definition at line 73 of file binary\+\_\+cross\+\_\+entropy\+\_\+loss.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1BCELoss_afb1123035456ff8aa73e7f7c08e6acbc}} 
\index{mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}!Reduction@{Reduction}}
\index{Reduction@{Reduction}!mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}}
\subsubsection{Reduction()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily bool Reduction (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the reduction. 



Definition at line 81 of file binary\+\_\+cross\+\_\+entropy\+\_\+loss.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1BCELoss_ace5e97393cf08c81177ad0929d1c3496}} 
\index{mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}!Reduction@{Reduction}}
\index{Reduction@{Reduction}!mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}}
\subsubsection{Reduction()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily bool\& Reduction (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Set the reduction. 



Definition at line 83 of file binary\+\_\+cross\+\_\+entropy\+\_\+loss.\+hpp.



References B\+C\+E\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::serialize().

\mbox{\label{classmlpack_1_1ann_1_1BCELoss_a65cba07328997659bec80b9879b15a51}} 
\index{mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}!serialize@{serialize}}
\index{serialize@{serialize}!mlpack\+::ann\+::\+B\+C\+E\+Loss@{mlpack\+::ann\+::\+B\+C\+E\+Loss}}
\subsubsection{serialize()}
{\footnotesize\ttfamily void serialize (\begin{DoxyParamCaption}\item[{Archive \&}]{ar,  }\item[{const uint32\+\_\+t}]{ }\end{DoxyParamCaption})}



Serialize the layer. 



Referenced by B\+C\+E\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Reduction().



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/aakash/mlpack/src/mlpack/methods/ann/loss\+\_\+functions/\textbf{ binary\+\_\+cross\+\_\+entropy\+\_\+loss.\+hpp}\end{DoxyCompactItemize}

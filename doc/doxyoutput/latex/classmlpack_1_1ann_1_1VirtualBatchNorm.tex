\section{Virtual\+Batch\+Norm$<$ Input\+Data\+Type, Output\+Data\+Type $>$ Class Template Reference}
\label{classmlpack_1_1ann_1_1VirtualBatchNorm}\index{Virtual\+Batch\+Norm$<$ Input\+Data\+Type, Output\+Data\+Type $>$@{Virtual\+Batch\+Norm$<$ Input\+Data\+Type, Output\+Data\+Type $>$}}


Declaration of the \doxyref{Virtual\+Batch\+Norm}{p.}{classmlpack_1_1ann_1_1VirtualBatchNorm} layer class.  


\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\textbf{ Virtual\+Batch\+Norm} ()
\begin{DoxyCompactList}\small\item\em Create the \doxyref{Virtual\+Batch\+Norm}{p.}{classmlpack_1_1ann_1_1VirtualBatchNorm} object. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\\textbf{ Virtual\+Batch\+Norm} (const arma\+::\+Mat$<$ eT $>$ \&reference\+Batch, const size\+\_\+t size, const double eps=1e-\/8)
\begin{DoxyCompactList}\small\item\em Create the \doxyref{Virtual\+Batch\+Norm}{p.}{classmlpack_1_1ann_1_1VirtualBatchNorm} layer object for a specified number of input units. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void \textbf{ Backward} (const arma\+::\+Mat$<$ eT $>$ \&, const arma\+::\+Mat$<$ eT $>$ \&gy, arma\+::\+Mat$<$ eT $>$ \&g)
\begin{DoxyCompactList}\small\item\em Backward pass through the layer. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Delta} () const
\begin{DoxyCompactList}\small\item\em Get the delta. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Delta} ()
\begin{DoxyCompactList}\small\item\em Modify the delta. \end{DoxyCompactList}\item 
double \textbf{ Epsilon} () const
\begin{DoxyCompactList}\small\item\em Get the epsilon value. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void \textbf{ Forward} (const arma\+::\+Mat$<$ eT $>$ \&input, arma\+::\+Mat$<$ eT $>$ \&output)
\begin{DoxyCompactList}\small\item\em Forward pass of the Virtual Batch Normalization layer. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void \textbf{ Gradient} (const arma\+::\+Mat$<$ eT $>$ \&, const arma\+::\+Mat$<$ eT $>$ \&error, arma\+::\+Mat$<$ eT $>$ \&gradient)
\begin{DoxyCompactList}\small\item\em Calculate the gradient using the output delta and the input activations. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Gradient} () const
\begin{DoxyCompactList}\small\item\em Get the gradient. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Gradient} ()
\begin{DoxyCompactList}\small\item\em Modify the gradient. \end{DoxyCompactList}\item 
size\+\_\+t \textbf{ In\+Size} () const
\begin{DoxyCompactList}\small\item\em Get the number of input units. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Output\+Parameter} () const
\begin{DoxyCompactList}\small\item\em Get the output parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Output\+Parameter} ()
\begin{DoxyCompactList}\small\item\em Modify the output parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Parameters} () const
\begin{DoxyCompactList}\small\item\em Get the parameters. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Parameters} ()
\begin{DoxyCompactList}\small\item\em Modify the parameters. \end{DoxyCompactList}\item 
void \textbf{ Reset} ()
\begin{DoxyCompactList}\small\item\em Reset the layer parameters. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Archive $>$ }\\void \textbf{ serialize} (Archive \&ar, const uint32\+\_\+t)
\begin{DoxyCompactList}\small\item\em Serialize the layer. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Input\+Data\+Type = arma\+::mat, typename Output\+Data\+Type = arma\+::mat$>$\newline
class mlpack\+::ann\+::\+Virtual\+Batch\+Norm$<$ Input\+Data\+Type, Output\+Data\+Type $>$}

Declaration of the \doxyref{Virtual\+Batch\+Norm}{p.}{classmlpack_1_1ann_1_1VirtualBatchNorm} layer class. 

Instead of using the batch statistics for normalizing on a mini-\/batch, it uses a reference subset of the data for calculating the normalization statistics.

For more information, refer to the following paper,


\begin{DoxyCode}
@article\{Goodfellow2016,
  author  = \{Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung,
             Alec Radford, Xi Chen\},
  title   = \{Improved Techniques \textcolor{keywordflow}{for} Training GANs\},
  year    = \{2016\},
  url     = \{https:\textcolor{comment}{//arxiv.org/abs/1606.03498\},}
\}
\end{DoxyCode}



\begin{DoxyTemplParams}{Template Parameters}
{\em Input\+Data\+Type} & Type of the input data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
{\em Output\+Data\+Type} & Type of the output data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
\end{DoxyTemplParams}


Definition at line 113 of file layer\+\_\+types.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_a2685dbd7be918be6aa1f4b9272a9ac6c}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Virtual\+Batch\+Norm@{Virtual\+Batch\+Norm}}
\index{Virtual\+Batch\+Norm@{Virtual\+Batch\+Norm}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Virtual\+Batch\+Norm()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily \textbf{ Virtual\+Batch\+Norm} (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Create the \doxyref{Virtual\+Batch\+Norm}{p.}{classmlpack_1_1ann_1_1VirtualBatchNorm} object. 

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_ad26280b8db514b8961646cfc41586354}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Virtual\+Batch\+Norm@{Virtual\+Batch\+Norm}}
\index{Virtual\+Batch\+Norm@{Virtual\+Batch\+Norm}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Virtual\+Batch\+Norm()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily \textbf{ Virtual\+Batch\+Norm} (\begin{DoxyParamCaption}\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{reference\+Batch,  }\item[{const size\+\_\+t}]{size,  }\item[{const double}]{eps = {\ttfamily 1e-\/8} }\end{DoxyParamCaption})}



Create the \doxyref{Virtual\+Batch\+Norm}{p.}{classmlpack_1_1ann_1_1VirtualBatchNorm} layer object for a specified number of input units. 


\begin{DoxyParams}{Parameters}
{\em reference\+Batch} & The data from which the normalization statistics are computed. \\
\hline
{\em size} & The number of input units / channels. \\
\hline
{\em eps} & The epsilon added to variance to ensure numerical stability. \\
\hline
\end{DoxyParams}


\subsection{Member Function Documentation}
\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_ad9ad1a3bdb0f3fff5c839ed155e4bbf8}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Backward@{Backward}}
\index{Backward@{Backward}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Backward()}
{\footnotesize\ttfamily void Backward (\begin{DoxyParamCaption}\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{,  }\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{gy,  }\item[{arma\+::\+Mat$<$ eT $>$ \&}]{g }\end{DoxyParamCaption})}



Backward pass through the layer. 


\begin{DoxyParams}{Parameters}
{\em $\ast$} & (input) The input activations. \\
\hline
{\em gy} & The backpropagated error. \\
\hline
{\em g} & The calculated gradient. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_a797f7edb44dd081e5e2b3cc316eef6bd}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Delta@{Delta}}
\index{Delta@{Delta}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the delta. 



Definition at line 116 of file virtual\+\_\+batch\+\_\+norm.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_ad6601342d560219ce951d554e69e5e87}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Delta@{Delta}}
\index{Delta@{Delta}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the delta. 



Definition at line 118 of file virtual\+\_\+batch\+\_\+norm.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_af6d960193bb5db37e51416e12bf720de}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Epsilon@{Epsilon}}
\index{Epsilon@{Epsilon}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Epsilon()}
{\footnotesize\ttfamily double Epsilon (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the epsilon value. 



Definition at line 129 of file virtual\+\_\+batch\+\_\+norm.\+hpp.



References Virtual\+Batch\+Norm$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::serialize().

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_a461f849bc638c15bec262dc9c3a58abe}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Forward@{Forward}}
\index{Forward@{Forward}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Forward()}
{\footnotesize\ttfamily void Forward (\begin{DoxyParamCaption}\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{input,  }\item[{arma\+::\+Mat$<$ eT $>$ \&}]{output }\end{DoxyParamCaption})}



Forward pass of the Virtual Batch Normalization layer. 

Transforms the input data into zero mean and unit variance, scales the data by a factor gamma and shifts it by beta.


\begin{DoxyParams}{Parameters}
{\em input} & Input data for the layer. \\
\hline
{\em output} & Resulting output activations. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_a9eb69a40c783e082307be5cb1ffab921}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Gradient()\hspace{0.1cm}{\footnotesize\ttfamily [1/3]}}
{\footnotesize\ttfamily void Gradient (\begin{DoxyParamCaption}\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{,  }\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{error,  }\item[{arma\+::\+Mat$<$ eT $>$ \&}]{gradient }\end{DoxyParamCaption})}



Calculate the gradient using the output delta and the input activations. 


\begin{DoxyParams}{Parameters}
{\em $\ast$} & (input) The input activations. \\
\hline
{\em error} & The calculated error. \\
\hline
{\em gradient} & The calculated gradient. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_a0f1f4e6d93472d83852731a96c8c3f59}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Gradient()\hspace{0.1cm}{\footnotesize\ttfamily [2/3]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Gradient (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the gradient. 



Definition at line 121 of file virtual\+\_\+batch\+\_\+norm.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_a19abce4739c3b0b658b612537e21956a}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Gradient@{Gradient}}
\index{Gradient@{Gradient}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Gradient()\hspace{0.1cm}{\footnotesize\ttfamily [3/3]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Gradient (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the gradient. 



Definition at line 123 of file virtual\+\_\+batch\+\_\+norm.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_adc2669016a821704c4c4aeb4651e9e87}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!In\+Size@{In\+Size}}
\index{In\+Size@{In\+Size}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{In\+Size()}
{\footnotesize\ttfamily size\+\_\+t In\+Size (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the number of input units. 



Definition at line 126 of file virtual\+\_\+batch\+\_\+norm.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_a0ee21c2a36e5abad1e7a9d5dd00849f9}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Output\+Parameter()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the output parameter. 



Definition at line 111 of file virtual\+\_\+batch\+\_\+norm.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_a21d5f745f02c709625a4ee0907f004a5}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Output\+Parameter()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the output parameter. 



Definition at line 113 of file virtual\+\_\+batch\+\_\+norm.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_aa530552c7ef915c952fbacc77b965c90}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Parameters@{Parameters}}
\index{Parameters@{Parameters}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Parameters()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Parameters (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the parameters. 



Definition at line 106 of file virtual\+\_\+batch\+\_\+norm.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_a9c5c5900772a689d5a6b59778ec67120}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Parameters@{Parameters}}
\index{Parameters@{Parameters}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Parameters()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Parameters (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the parameters. 



Definition at line 108 of file virtual\+\_\+batch\+\_\+norm.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_a372de693ad40b3f42839c8ec6ac845f4}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!Reset@{Reset}}
\index{Reset@{Reset}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{Reset()}
{\footnotesize\ttfamily void Reset (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Reset the layer parameters. 

\mbox{\label{classmlpack_1_1ann_1_1VirtualBatchNorm_a65cba07328997659bec80b9879b15a51}} 
\index{mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}!serialize@{serialize}}
\index{serialize@{serialize}!mlpack\+::ann\+::\+Virtual\+Batch\+Norm@{mlpack\+::ann\+::\+Virtual\+Batch\+Norm}}
\subsubsection{serialize()}
{\footnotesize\ttfamily void serialize (\begin{DoxyParamCaption}\item[{Archive \&}]{ar,  }\item[{const uint32\+\_\+t}]{ }\end{DoxyParamCaption})}



Serialize the layer. 



Referenced by Virtual\+Batch\+Norm$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Epsilon().



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
/home/aakash/mlpack/src/mlpack/methods/ann/layer/\textbf{ layer\+\_\+types.\+hpp}\item 
/home/aakash/mlpack/src/mlpack/methods/ann/layer/\textbf{ virtual\+\_\+batch\+\_\+norm.\+hpp}\end{DoxyCompactItemize}

\section{Dropout$<$ Input\+Data\+Type, Output\+Data\+Type $>$ Class Template Reference}
\label{classmlpack_1_1ann_1_1Dropout}\index{Dropout$<$ Input\+Data\+Type, Output\+Data\+Type $>$@{Dropout$<$ Input\+Data\+Type, Output\+Data\+Type $>$}}


The dropout layer is a regularizer that randomly with probability \textquotesingle{}ratio\textquotesingle{} sets input values to zero and scales the remaining elements by factor 1 / (1 -\/ ratio) rather than during test time so as to keep the expected sum same.  


\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\textbf{ Dropout} (const double ratio=0.\+5)
\begin{DoxyCompactList}\small\item\em Create the \doxyref{Dropout}{p.}{classmlpack_1_1ann_1_1Dropout} object using the specified ratio parameter. \end{DoxyCompactList}\item 
\textbf{ Dropout} (const \textbf{ Dropout} \&layer)
\begin{DoxyCompactList}\small\item\em Copy Constructor. \end{DoxyCompactList}\item 
\textbf{ Dropout} (const \textbf{ Dropout} \&\&)
\begin{DoxyCompactList}\small\item\em Move Constructor. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void \textbf{ Backward} (const arma\+::\+Mat$<$ eT $>$ \&, const arma\+::\+Mat$<$ eT $>$ \&gy, arma\+::\+Mat$<$ eT $>$ \&g)
\begin{DoxyCompactList}\small\item\em Ordinary feed backward pass of the dropout layer. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Delta} () const
\begin{DoxyCompactList}\small\item\em Get the detla. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Delta} ()
\begin{DoxyCompactList}\small\item\em Modify the delta. \end{DoxyCompactList}\item 
bool \textbf{ Deterministic} () const
\begin{DoxyCompactList}\small\item\em The value of the deterministic parameter. \end{DoxyCompactList}\item 
bool \& \textbf{ Deterministic} ()
\begin{DoxyCompactList}\small\item\em Modify the value of the deterministic parameter. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void \textbf{ Forward} (const arma\+::\+Mat$<$ eT $>$ \&input, arma\+::\+Mat$<$ eT $>$ \&output)
\begin{DoxyCompactList}\small\item\em Ordinary feed forward pass of the dropout layer. \end{DoxyCompactList}\item 
\textbf{ Dropout} \& \textbf{ operator=} (const \textbf{ Dropout} \&layer)
\begin{DoxyCompactList}\small\item\em Copy assignment operator. \end{DoxyCompactList}\item 
\textbf{ Dropout} \& \textbf{ operator=} (\textbf{ Dropout} \&\&layer)
\begin{DoxyCompactList}\small\item\em Move assignment operator. \end{DoxyCompactList}\item 
Output\+Data\+Type const  \& \textbf{ Output\+Parameter} () const
\begin{DoxyCompactList}\small\item\em Get the output parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Output\+Parameter} ()
\begin{DoxyCompactList}\small\item\em Modify the output parameter. \end{DoxyCompactList}\item 
double \textbf{ Ratio} () const
\begin{DoxyCompactList}\small\item\em The probability of setting a value to zero. \end{DoxyCompactList}\item 
void \textbf{ Ratio} (const double r)
\begin{DoxyCompactList}\small\item\em Modify the probability of setting a value to zero. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Archive $>$ }\\void \textbf{ serialize} (Archive \&ar, const uint32\+\_\+t)
\begin{DoxyCompactList}\small\item\em Serialize the layer. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Input\+Data\+Type = arma\+::mat, typename Output\+Data\+Type = arma\+::mat$>$\newline
class mlpack\+::ann\+::\+Dropout$<$ Input\+Data\+Type, Output\+Data\+Type $>$}

The dropout layer is a regularizer that randomly with probability \textquotesingle{}ratio\textquotesingle{} sets input values to zero and scales the remaining elements by factor 1 / (1 -\/ ratio) rather than during test time so as to keep the expected sum same. 

In the deterministic mode (during testing), there is no change in the input.

Note\+: During training you should set deterministic to false and during testing you should set deterministic to true.

For more information, see the following.


\begin{DoxyCode}
@article\{Hinton2012,
  author  = \{Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
             Ilya Sutskever, Ruslan Salakhutdinov\},
  title   = \{Improving neural networks by preventing co-adaptation of feature
             detectors\},
  journal = \{CoRR\},
  volume  = \{abs/1207.0580\},
  year    = \{2012\},
  url     = \{https:\textcolor{comment}{//arxiv.org/abs/1207.0580\}}
\}
\end{DoxyCode}



\begin{DoxyTemplParams}{Template Parameters}
{\em Input\+Data\+Type} & Type of the input data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
{\em Output\+Data\+Type} & Type of the output data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
\end{DoxyTemplParams}


Definition at line 53 of file dropout.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\mbox{\label{classmlpack_1_1ann_1_1Dropout_a64085a3ae3820eef527a03699f7c284e}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Dropout@{Dropout}}
\index{Dropout@{Dropout}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Dropout()\hspace{0.1cm}{\footnotesize\ttfamily [1/3]}}
{\footnotesize\ttfamily \textbf{ Dropout} (\begin{DoxyParamCaption}\item[{const double}]{ratio = {\ttfamily 0.5} }\end{DoxyParamCaption})}



Create the \doxyref{Dropout}{p.}{classmlpack_1_1ann_1_1Dropout} object using the specified ratio parameter. 


\begin{DoxyParams}{Parameters}
{\em ratio} & The probability of setting a value to zero. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1Dropout_a558d0e9d9c3b0de4013339ce3cba8242}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Dropout@{Dropout}}
\index{Dropout@{Dropout}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Dropout()\hspace{0.1cm}{\footnotesize\ttfamily [2/3]}}
{\footnotesize\ttfamily \textbf{ Dropout} (\begin{DoxyParamCaption}\item[{const \textbf{ Dropout}$<$ Input\+Data\+Type, Output\+Data\+Type $>$ \&}]{layer }\end{DoxyParamCaption})}



Copy Constructor. 

\mbox{\label{classmlpack_1_1ann_1_1Dropout_a40f04c73de792f56190959f8cdc9a7c1}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Dropout@{Dropout}}
\index{Dropout@{Dropout}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Dropout()\hspace{0.1cm}{\footnotesize\ttfamily [3/3]}}
{\footnotesize\ttfamily \textbf{ Dropout} (\begin{DoxyParamCaption}\item[{const \textbf{ Dropout}$<$ Input\+Data\+Type, Output\+Data\+Type $>$ \&\&}]{ }\end{DoxyParamCaption})}



Move Constructor. 



\subsection{Member Function Documentation}
\mbox{\label{classmlpack_1_1ann_1_1Dropout_ad9ad1a3bdb0f3fff5c839ed155e4bbf8}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Backward@{Backward}}
\index{Backward@{Backward}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Backward()}
{\footnotesize\ttfamily void Backward (\begin{DoxyParamCaption}\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{,  }\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{gy,  }\item[{arma\+::\+Mat$<$ eT $>$ \&}]{g }\end{DoxyParamCaption})}



Ordinary feed backward pass of the dropout layer. 


\begin{DoxyParams}{Parameters}
{\em $\ast$} & (input) The propagated input activation. \\
\hline
{\em gy} & The backpropagated error. \\
\hline
{\em g} & The calculated gradient. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1Dropout_a797f7edb44dd081e5e2b3cc316eef6bd}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Delta@{Delta}}
\index{Delta@{Delta}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the detla. 



Definition at line 102 of file dropout.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1Dropout_ad6601342d560219ce951d554e69e5e87}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Delta@{Delta}}
\index{Delta@{Delta}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the delta. 



Definition at line 104 of file dropout.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1Dropout_a9f4103707f4d199ce5594d239b60443e}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Deterministic()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily bool Deterministic (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



The value of the deterministic parameter. 



Definition at line 107 of file dropout.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1Dropout_a42d4ee3da432cff20d3a41b8b1ec801c}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Deterministic@{Deterministic}}
\index{Deterministic@{Deterministic}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Deterministic()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily bool\& Deterministic (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the value of the deterministic parameter. 



Definition at line 109 of file dropout.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1Dropout_a461f849bc638c15bec262dc9c3a58abe}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Forward@{Forward}}
\index{Forward@{Forward}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Forward()}
{\footnotesize\ttfamily void Forward (\begin{DoxyParamCaption}\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{input,  }\item[{arma\+::\+Mat$<$ eT $>$ \&}]{output }\end{DoxyParamCaption})}



Ordinary feed forward pass of the dropout layer. 


\begin{DoxyParams}{Parameters}
{\em input} & Input data used for evaluating the specified function. \\
\hline
{\em output} & Resulting output activation. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1Dropout_aad12434108f82259494e85b817992ffb}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!operator=@{operator=}}
\index{operator=@{operator=}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{operator=()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily \textbf{ Dropout}\& operator= (\begin{DoxyParamCaption}\item[{const \textbf{ Dropout}$<$ Input\+Data\+Type, Output\+Data\+Type $>$ \&}]{layer }\end{DoxyParamCaption})}



Copy assignment operator. 

\mbox{\label{classmlpack_1_1ann_1_1Dropout_a29375b6364a044e8a9a7590b6585463e}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!operator=@{operator=}}
\index{operator=@{operator=}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{operator=()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily \textbf{ Dropout}\& operator= (\begin{DoxyParamCaption}\item[{\textbf{ Dropout}$<$ Input\+Data\+Type, Output\+Data\+Type $>$ \&\&}]{layer }\end{DoxyParamCaption})}



Move assignment operator. 

\mbox{\label{classmlpack_1_1ann_1_1Dropout_a0ee21c2a36e5abad1e7a9d5dd00849f9}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Output\+Parameter()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type const\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the output parameter. 



Definition at line 97 of file dropout.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1Dropout_a21d5f745f02c709625a4ee0907f004a5}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Output\+Parameter()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the output parameter. 



Definition at line 99 of file dropout.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1Dropout_a0052f2d1427761ea25fb01a05d966a40}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Ratio@{Ratio}}
\index{Ratio@{Ratio}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Ratio()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily double Ratio (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



The probability of setting a value to zero. 



Definition at line 112 of file dropout.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1Dropout_a9ca65c8f62aa7b45b5b08899d83d2821}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!Ratio@{Ratio}}
\index{Ratio@{Ratio}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{Ratio()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily void Ratio (\begin{DoxyParamCaption}\item[{const double}]{r }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the probability of setting a value to zero. 



Definition at line 115 of file dropout.\+hpp.



References Dropout$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::serialize().

\mbox{\label{classmlpack_1_1ann_1_1Dropout_a65cba07328997659bec80b9879b15a51}} 
\index{mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}!serialize@{serialize}}
\index{serialize@{serialize}!mlpack\+::ann\+::\+Dropout@{mlpack\+::ann\+::\+Dropout}}
\subsubsection{serialize()}
{\footnotesize\ttfamily void serialize (\begin{DoxyParamCaption}\item[{Archive \&}]{ar,  }\item[{const uint32\+\_\+t}]{ }\end{DoxyParamCaption})}



Serialize the layer. 



Referenced by Dropout$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Ratio().



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/aakash/mlpack/src/mlpack/methods/ann/layer/\textbf{ dropout.\+hpp}\end{DoxyCompactItemize}

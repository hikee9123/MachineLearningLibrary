\section{Random\+Replay$<$ Environment\+Type $>$ Class Template Reference}
\label{classmlpack_1_1rl_1_1RandomReplay}\index{Random\+Replay$<$ Environment\+Type $>$@{Random\+Replay$<$ Environment\+Type $>$}}


Implementation of random experience replay.  


\subsection*{Classes}
\begin{DoxyCompactItemize}
\item 
struct \textbf{ Transition}
\end{DoxyCompactItemize}
\subsection*{Public Types}
\begin{DoxyCompactItemize}
\item 
using \textbf{ Action\+Type} = typename Environment\+Type\+::\+Action
\begin{DoxyCompactList}\small\item\em Convenient typedef for action. \end{DoxyCompactList}\item 
using \textbf{ State\+Type} = typename Environment\+Type\+::\+State
\begin{DoxyCompactList}\small\item\em Convenient typedef for state. \end{DoxyCompactList}\end{DoxyCompactItemize}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\textbf{ Random\+Replay} ()
\item 
\textbf{ Random\+Replay} (const size\+\_\+t batch\+Size, const size\+\_\+t capacity, const size\+\_\+t n\+Steps=1, const size\+\_\+t dimension=State\+Type\+::dimension)
\begin{DoxyCompactList}\small\item\em Construct an instance of random experience replay class. \end{DoxyCompactList}\item 
void \textbf{ Get\+N\+Step\+Info} (double \&reward, \textbf{ State\+Type} \&next\+State, bool \&is\+End, const double \&discount)
\begin{DoxyCompactList}\small\item\em Get the reward, next state and terminal boolean for nth step. \end{DoxyCompactList}\item 
const size\+\_\+t \& \textbf{ N\+Steps} () const
\begin{DoxyCompactList}\small\item\em Get the number of steps for n-\/step agent. \end{DoxyCompactList}\item 
void \textbf{ Sample} (arma\+::mat \&sampled\+States, std\+::vector$<$ \textbf{ Action\+Type} $>$ \&sampled\+Actions, arma\+::rowvec \&sampled\+Rewards, arma\+::mat \&sampled\+Next\+States, arma\+::irowvec \&is\+Terminal)
\begin{DoxyCompactList}\small\item\em Sample some experiences. \end{DoxyCompactList}\item 
const size\+\_\+t \& \textbf{ Size} ()
\begin{DoxyCompactList}\small\item\em Get the number of transitions in the memory. \end{DoxyCompactList}\item 
void \textbf{ Store} (\textbf{ State\+Type} state, \textbf{ Action\+Type} action, double reward, \textbf{ State\+Type} next\+State, bool is\+End, const double \&discount)
\begin{DoxyCompactList}\small\item\em Store the given experience. \end{DoxyCompactList}\item 
void \textbf{ Update} (arma\+::mat, std\+::vector$<$ \textbf{ Action\+Type} $>$, arma\+::mat, arma\+::mat \&)
\begin{DoxyCompactList}\small\item\em Update the priorities of transitions and Update the gradients. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Environment\+Type$>$\newline
class mlpack\+::rl\+::\+Random\+Replay$<$ Environment\+Type $>$}

Implementation of random experience replay. 

At each time step, interactions between the agent and the environment will be saved to a memory buffer. When necessary, we can simply sample previous experiences from the buffer to train the agent. Typically this would be a random sample and the memory will be a First-\/\+In-\/\+First-\/\+Out buffer.

For more information, see the following.


\begin{DoxyCode}
@phdthesis\{lin1993reinforcement,
 title  = \{Reinforcement learning \textcolor{keywordflow}{for} robots \textcolor{keyword}{using} neural networks\},
 author = \{Lin, Long-Ji\},
 year   = \{1993\},
 school = \{Fujitsu Laboratories Ltd\}
\}
\end{DoxyCode}



\begin{DoxyTemplParams}{Template Parameters}
{\em Environment\+Type} & Desired task. \\
\hline
\end{DoxyTemplParams}


Definition at line 44 of file random\+\_\+replay.\+hpp.



\subsection{Member Typedef Documentation}
\mbox{\label{classmlpack_1_1rl_1_1RandomReplay_aaf7b2dc5d49d01961601c7c16be76777}} 
\index{mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}!Action\+Type@{Action\+Type}}
\index{Action\+Type@{Action\+Type}!mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}}
\subsubsection{Action\+Type}
{\footnotesize\ttfamily using \textbf{ Action\+Type} =  typename Environment\+Type\+::\+Action}



Convenient typedef for action. 



Definition at line 48 of file random\+\_\+replay.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1RandomReplay_ada68ef405b7c331a2bee337614f00088}} 
\index{mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}!State\+Type@{State\+Type}}
\index{State\+Type@{State\+Type}!mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}}
\subsubsection{State\+Type}
{\footnotesize\ttfamily using \textbf{ State\+Type} =  typename Environment\+Type\+::\+State}



Convenient typedef for state. 



Definition at line 51 of file random\+\_\+replay.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\mbox{\label{classmlpack_1_1rl_1_1RandomReplay_a63672681bb61c707da6ca94ada59164a}} 
\index{mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}!Random\+Replay@{Random\+Replay}}
\index{Random\+Replay@{Random\+Replay}!mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}}
\subsubsection{Random\+Replay()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily \textbf{ Random\+Replay} (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Definition at line 62 of file random\+\_\+replay.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1RandomReplay_ad222f724aae50a27cb0f3f4f761cad01}} 
\index{mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}!Random\+Replay@{Random\+Replay}}
\index{Random\+Replay@{Random\+Replay}!mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}}
\subsubsection{Random\+Replay()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily \textbf{ Random\+Replay} (\begin{DoxyParamCaption}\item[{const size\+\_\+t}]{batch\+Size,  }\item[{const size\+\_\+t}]{capacity,  }\item[{const size\+\_\+t}]{n\+Steps = {\ttfamily 1},  }\item[{const size\+\_\+t}]{dimension = {\ttfamily StateType\+:\+:dimension} }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Construct an instance of random experience replay class. 


\begin{DoxyParams}{Parameters}
{\em batch\+Size} & Number of examples returned at each sample. \\
\hline
{\em capacity} & Total memory size in terms of number of examples. \\
\hline
{\em n\+Steps} & Number of steps to look in the future. \\
\hline
{\em dimension} & The dimension of an encoded state. \\
\hline
\end{DoxyParams}


Definition at line 78 of file random\+\_\+replay.\+hpp.



\subsection{Member Function Documentation}
\mbox{\label{classmlpack_1_1rl_1_1RandomReplay_abf36129b66f5b30a65d96d11ebfde027}} 
\index{mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}!Get\+N\+Step\+Info@{Get\+N\+Step\+Info}}
\index{Get\+N\+Step\+Info@{Get\+N\+Step\+Info}!mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}}
\subsubsection{Get\+N\+Step\+Info()}
{\footnotesize\ttfamily void Get\+N\+Step\+Info (\begin{DoxyParamCaption}\item[{double \&}]{reward,  }\item[{\textbf{ State\+Type} \&}]{next\+State,  }\item[{bool \&}]{is\+End,  }\item[{const double \&}]{discount }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get the reward, next state and terminal boolean for nth step. 


\begin{DoxyParams}{Parameters}
{\em reward} & Given reward. \\
\hline
{\em next\+State} & Given next state. \\
\hline
{\em is\+End} & Whether next state is terminal state. \\
\hline
{\em discount} & The discount parameter. \\
\hline
\end{DoxyParams}


Definition at line 151 of file random\+\_\+replay.\+hpp.



Referenced by Random\+Replay$<$ Environment\+Type $>$\+::\+Store().

\mbox{\label{classmlpack_1_1rl_1_1RandomReplay_a48a86a6254329a98e1f15d4722c4e85b}} 
\index{mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}!N\+Steps@{N\+Steps}}
\index{N\+Steps@{N\+Steps}!mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}}
\subsubsection{N\+Steps()}
{\footnotesize\ttfamily const size\+\_\+t\& N\+Steps (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the number of steps for n-\/step agent. 



Definition at line 228 of file random\+\_\+replay.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1RandomReplay_a6ecc6da2d5f83f0eefdc74be3465925a}} 
\index{mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}!Sample@{Sample}}
\index{Sample@{Sample}!mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}}
\subsubsection{Sample()}
{\footnotesize\ttfamily void Sample (\begin{DoxyParamCaption}\item[{arma\+::mat \&}]{sampled\+States,  }\item[{std\+::vector$<$ \textbf{ Action\+Type} $>$ \&}]{sampled\+Actions,  }\item[{arma\+::rowvec \&}]{sampled\+Rewards,  }\item[{arma\+::mat \&}]{sampled\+Next\+States,  }\item[{arma\+::irowvec \&}]{is\+Terminal }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Sample some experiences. 


\begin{DoxyParams}{Parameters}
{\em sampled\+States} & Sampled encoded states. \\
\hline
{\em sampled\+Actions} & Sampled actions. \\
\hline
{\em sampled\+Rewards} & Sampled rewards. \\
\hline
{\em sampled\+Next\+States} & Sampled encoded next states. \\
\hline
{\em is\+Terminal} & Indicate whether corresponding next state is terminal state. \\
\hline
\end{DoxyParams}


Definition at line 183 of file random\+\_\+replay.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1RandomReplay_ab8983dc8f7847b4c77148b86d0e7fc8d}} 
\index{mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}!Size@{Size}}
\index{Size@{Size}!mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}}
\subsubsection{Size()}
{\footnotesize\ttfamily const size\+\_\+t\& Size (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Get the number of transitions in the memory. 

\begin{DoxyReturn}{Returns}
Actual used memory size 
\end{DoxyReturn}


Definition at line 206 of file random\+\_\+replay.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1RandomReplay_ab17ee90540cf7b26647b57acf16116d5}} 
\index{mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}!Store@{Store}}
\index{Store@{Store}!mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}}
\subsubsection{Store()}
{\footnotesize\ttfamily void Store (\begin{DoxyParamCaption}\item[{\textbf{ State\+Type}}]{state,  }\item[{\textbf{ Action\+Type}}]{action,  }\item[{double}]{reward,  }\item[{\textbf{ State\+Type}}]{next\+State,  }\item[{bool}]{is\+End,  }\item[{const double \&}]{discount }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Store the given experience. 


\begin{DoxyParams}{Parameters}
{\em state} & Given state. \\
\hline
{\em action} & Given action. \\
\hline
{\em reward} & Given reward. \\
\hline
{\em next\+State} & Given next state. \\
\hline
{\em is\+End} & Whether next state is terminal state. \\
\hline
{\em discount} & The discount parameter. \\
\hline
\end{DoxyParams}


Definition at line 104 of file random\+\_\+replay.\+hpp.



References Random\+Replay$<$ Environment\+Type $>$\+::\+Transition\+::action, Random\+Replay$<$ Environment\+Type $>$\+::\+Get\+N\+Step\+Info(), Random\+Replay$<$ Environment\+Type $>$\+::\+Transition\+::is\+End, Random\+Replay$<$ Environment\+Type $>$\+::\+Transition\+::next\+State, Random\+Replay$<$ Environment\+Type $>$\+::\+Transition\+::reward, and Random\+Replay$<$ Environment\+Type $>$\+::\+Transition\+::state.

\mbox{\label{classmlpack_1_1rl_1_1RandomReplay_ab1989879b7d6a3110f2391a5b239e7aa}} 
\index{mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}!Update@{Update}}
\index{Update@{Update}!mlpack\+::rl\+::\+Random\+Replay@{mlpack\+::rl\+::\+Random\+Replay}}
\subsubsection{Update()}
{\footnotesize\ttfamily void Update (\begin{DoxyParamCaption}\item[{arma\+::mat}]{,  }\item[{std\+::vector$<$ \textbf{ Action\+Type} $>$}]{,  }\item[{arma\+::mat}]{,  }\item[{arma\+::mat \&}]{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Update the priorities of transitions and Update the gradients. 


\begin{DoxyParams}{Parameters}
{\em $\ast$} & (target) The learned value \\
\hline
{\em $\ast$} & (sampled\+Actions) Agent\textquotesingle{}s sampled action \\
\hline
{\em $\ast$} & (next\+Action\+Values) Agent\textquotesingle{}s next action \\
\hline
{\em $\ast$} & (gradients) The model\textquotesingle{}s gradients \\
\hline
\end{DoxyParams}


Definition at line 219 of file random\+\_\+replay.\+hpp.



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/aakash/mlpack/src/mlpack/methods/reinforcement\+\_\+learning/replay/\textbf{ random\+\_\+replay.\+hpp}\end{DoxyCompactItemize}

\section{One\+Step\+Q\+Learning\+Worker$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$ Class Template Reference}
\label{classmlpack_1_1rl_1_1OneStepQLearningWorker}\index{One\+Step\+Q\+Learning\+Worker$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$@{One\+Step\+Q\+Learning\+Worker$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$}}


Forward declaration of \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker}.  


\subsection*{Public Types}
\begin{DoxyCompactItemize}
\item 
using \textbf{ Action\+Type} = typename Environment\+Type\+::\+Action
\item 
using \textbf{ State\+Type} = typename Environment\+Type\+::\+State
\item 
using \textbf{ Transition\+Type} = std\+::tuple$<$ \textbf{ State\+Type}, \textbf{ Action\+Type}, double, \textbf{ State\+Type} $>$
\end{DoxyCompactItemize}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\textbf{ One\+Step\+Q\+Learning\+Worker} (const Updater\+Type \&updater, const Environment\+Type \&environment, const \textbf{ Training\+Config} \&config, bool deterministic)
\begin{DoxyCompactList}\small\item\em Construct one step Q-\/\+Learning worker with the given parameters and environment. \end{DoxyCompactList}\item 
\textbf{ One\+Step\+Q\+Learning\+Worker} (const \textbf{ One\+Step\+Q\+Learning\+Worker} \&other)
\begin{DoxyCompactList}\small\item\em Copy another \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker}. \end{DoxyCompactList}\item 
\textbf{ One\+Step\+Q\+Learning\+Worker} (\textbf{ One\+Step\+Q\+Learning\+Worker} \&\&other)
\begin{DoxyCompactList}\small\item\em Take ownership of another \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker}. \end{DoxyCompactList}\item 
\textbf{ $\sim$\+One\+Step\+Q\+Learning\+Worker} ()
\begin{DoxyCompactList}\small\item\em Clean memory. \end{DoxyCompactList}\item 
void \textbf{ Initialize} (Network\+Type \&learning\+Network)
\begin{DoxyCompactList}\small\item\em Initialize the worker. \end{DoxyCompactList}\item 
\textbf{ One\+Step\+Q\+Learning\+Worker} \& \textbf{ operator=} (const \textbf{ One\+Step\+Q\+Learning\+Worker} \&other)
\begin{DoxyCompactList}\small\item\em Copy another \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker}. \end{DoxyCompactList}\item 
\textbf{ One\+Step\+Q\+Learning\+Worker} \& \textbf{ operator=} (\textbf{ One\+Step\+Q\+Learning\+Worker} \&\&other)
\begin{DoxyCompactList}\small\item\em Take ownership of another \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker}. \end{DoxyCompactList}\item 
bool \textbf{ Step} (Network\+Type \&learning\+Network, Network\+Type \&target\+Network, size\+\_\+t \&total\+Steps, Policy\+Type \&policy, double \&total\+Reward)
\begin{DoxyCompactList}\small\item\em The agent will execute one step. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Environment\+Type, typename Network\+Type, typename Updater\+Type, typename Policy\+Type$>$\newline
class mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$}

Forward declaration of \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker}. 

One step Q-\/\+Learning worker.


\begin{DoxyTemplParams}{Template Parameters}
{\em Environment\+Type} & The type of the reinforcement learning task. \\
\hline
{\em Network\+Type} & The type of the network model. \\
\hline
{\em Updater\+Type} & The type of the optimizer. \\
\hline
{\em Policy\+Type} & The type of the behavior policy.\\
\hline
{\em Environment\+Type} & The type of the reinforcement learning task. \\
\hline
{\em Network\+Type} & The type of the network model. \\
\hline
{\em Updater\+Type} & The type of the optimizer. \\
\hline
{\em Policy\+Type} & The type of the behavior policy. $\ast$ \\
\hline
\end{DoxyTemplParams}


Definition at line 147 of file async\+\_\+learning.\+hpp.



\subsection{Member Typedef Documentation}
\mbox{\label{classmlpack_1_1rl_1_1OneStepQLearningWorker_aaf7b2dc5d49d01961601c7c16be76777}} 
\index{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}!Action\+Type@{Action\+Type}}
\index{Action\+Type@{Action\+Type}!mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}}
\subsubsection{Action\+Type}
{\footnotesize\ttfamily using \textbf{ Action\+Type} =  typename Environment\+Type\+::\+Action}



Definition at line 39 of file one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1OneStepQLearningWorker_ada68ef405b7c331a2bee337614f00088}} 
\index{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}!State\+Type@{State\+Type}}
\index{State\+Type@{State\+Type}!mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}}
\subsubsection{State\+Type}
{\footnotesize\ttfamily using \textbf{ State\+Type} =  typename Environment\+Type\+::\+State}



Definition at line 38 of file one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1OneStepQLearningWorker_a810951afc4d825b2409bf5378e871dc5}} 
\index{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}!Transition\+Type@{Transition\+Type}}
\index{Transition\+Type@{Transition\+Type}!mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}}
\subsubsection{Transition\+Type}
{\footnotesize\ttfamily using \textbf{ Transition\+Type} =  std\+::tuple$<$\textbf{ State\+Type}, \textbf{ Action\+Type}, double, \textbf{ State\+Type}$>$}



Definition at line 40 of file one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\mbox{\label{classmlpack_1_1rl_1_1OneStepQLearningWorker_a5863ff650bba4a49f9b451080947bf15}} 
\index{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}!One\+Step\+Q\+Learning\+Worker@{One\+Step\+Q\+Learning\+Worker}}
\index{One\+Step\+Q\+Learning\+Worker@{One\+Step\+Q\+Learning\+Worker}!mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}}
\subsubsection{One\+Step\+Q\+Learning\+Worker()\hspace{0.1cm}{\footnotesize\ttfamily [1/3]}}
{\footnotesize\ttfamily \textbf{ One\+Step\+Q\+Learning\+Worker} (\begin{DoxyParamCaption}\item[{const Updater\+Type \&}]{updater,  }\item[{const Environment\+Type \&}]{environment,  }\item[{const \textbf{ Training\+Config} \&}]{config,  }\item[{bool}]{deterministic }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Construct one step Q-\/\+Learning worker with the given parameters and environment. 


\begin{DoxyParams}{Parameters}
{\em updater} & The optimizer. \\
\hline
{\em environment} & The reinforcement learning task. \\
\hline
{\em config} & Hyper-\/parameters. \\
\hline
{\em deterministic} & Whether it should be deterministic. \\
\hline
\end{DoxyParams}


Definition at line 51 of file one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1OneStepQLearningWorker_ac8d57e5785a9b8251b7e1544bf9cd7fe}} 
\index{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}!One\+Step\+Q\+Learning\+Worker@{One\+Step\+Q\+Learning\+Worker}}
\index{One\+Step\+Q\+Learning\+Worker@{One\+Step\+Q\+Learning\+Worker}!mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}}
\subsubsection{One\+Step\+Q\+Learning\+Worker()\hspace{0.1cm}{\footnotesize\ttfamily [2/3]}}
{\footnotesize\ttfamily \textbf{ One\+Step\+Q\+Learning\+Worker} (\begin{DoxyParamCaption}\item[{const \textbf{ One\+Step\+Q\+Learning\+Worker}$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$ \&}]{other }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Copy another \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker}. 


\begin{DoxyParams}{Parameters}
{\em other} & \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker} to copy. \\
\hline
\end{DoxyParams}


Definition at line 71 of file one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1OneStepQLearningWorker_ad2f11f17c964a059c5632de9fcf0214f}} 
\index{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}!One\+Step\+Q\+Learning\+Worker@{One\+Step\+Q\+Learning\+Worker}}
\index{One\+Step\+Q\+Learning\+Worker@{One\+Step\+Q\+Learning\+Worker}!mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}}
\subsubsection{One\+Step\+Q\+Learning\+Worker()\hspace{0.1cm}{\footnotesize\ttfamily [3/3]}}
{\footnotesize\ttfamily \textbf{ One\+Step\+Q\+Learning\+Worker} (\begin{DoxyParamCaption}\item[{\textbf{ One\+Step\+Q\+Learning\+Worker}$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$ \&\&}]{other }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Take ownership of another \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker}. 


\begin{DoxyParams}{Parameters}
{\em other} & \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker} to take ownership of. \\
\hline
\end{DoxyParams}


Definition at line 101 of file one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1OneStepQLearningWorker_aaac57063c4ff1692d601b73d0d5fcef6}} 
\index{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}!````~One\+Step\+Q\+Learning\+Worker@{$\sim$\+One\+Step\+Q\+Learning\+Worker}}
\index{````~One\+Step\+Q\+Learning\+Worker@{$\sim$\+One\+Step\+Q\+Learning\+Worker}!mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}}
\subsubsection{$\sim$\+One\+Step\+Q\+Learning\+Worker()}
{\footnotesize\ttfamily $\sim$\textbf{ One\+Step\+Q\+Learning\+Worker} (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Clean memory. 



Definition at line 203 of file one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp.



\subsection{Member Function Documentation}
\mbox{\label{classmlpack_1_1rl_1_1OneStepQLearningWorker_acf86e5b1010bd1f15be7b3fab93dfd75}} 
\index{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}!Initialize@{Initialize}}
\index{Initialize@{Initialize}!mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}}
\subsubsection{Initialize()}
{\footnotesize\ttfamily void Initialize (\begin{DoxyParamCaption}\item[{Network\+Type \&}]{learning\+Network }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Initialize the worker. 


\begin{DoxyParams}{Parameters}
{\em learning\+Network} & The shared network. \\
\hline
\end{DoxyParams}


Definition at line 214 of file one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1OneStepQLearningWorker_ad05040e0b2fe04fea03a1f1389aa69fb}} 
\index{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}!operator=@{operator=}}
\index{operator=@{operator=}!mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}}
\subsubsection{operator=()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily \textbf{ One\+Step\+Q\+Learning\+Worker}\& operator= (\begin{DoxyParamCaption}\item[{const \textbf{ One\+Step\+Q\+Learning\+Worker}$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$ \&}]{other }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Copy another \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker}. 


\begin{DoxyParams}{Parameters}
{\em other} & \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker} to copy. \\
\hline
\end{DoxyParams}


Definition at line 131 of file one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1OneStepQLearningWorker_a49e977f76bd3b96e012294fdd03f0823}} 
\index{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}!operator=@{operator=}}
\index{operator=@{operator=}!mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}}
\subsubsection{operator=()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily \textbf{ One\+Step\+Q\+Learning\+Worker}\& operator= (\begin{DoxyParamCaption}\item[{\textbf{ One\+Step\+Q\+Learning\+Worker}$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$ \&\&}]{other }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Take ownership of another \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker}. 


\begin{DoxyParams}{Parameters}
{\em other} & \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker} to take ownership of. \\
\hline
\end{DoxyParams}


Definition at line 168 of file one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp.

\mbox{\label{classmlpack_1_1rl_1_1OneStepQLearningWorker_ad3920ee0951fcd4ce236584e19db8256}} 
\index{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}!Step@{Step}}
\index{Step@{Step}!mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker@{mlpack\+::rl\+::\+One\+Step\+Q\+Learning\+Worker}}
\subsubsection{Step()}
{\footnotesize\ttfamily bool Step (\begin{DoxyParamCaption}\item[{Network\+Type \&}]{learning\+Network,  }\item[{Network\+Type \&}]{target\+Network,  }\item[{size\+\_\+t \&}]{total\+Steps,  }\item[{Policy\+Type \&}]{policy,  }\item[{double \&}]{total\+Reward }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



The agent will execute one step. 


\begin{DoxyParams}{Parameters}
{\em learning\+Network} & The shared learning network. \\
\hline
{\em target\+Network} & The shared target network. \\
\hline
{\em total\+Steps} & The shared counter for total steps. \\
\hline
{\em policy} & The shared behavior policy. \\
\hline
{\em total\+Reward} & This will be the episode return if the episode ends after this step. Otherwise this is invalid. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
Indicate whether current episode ends after this step. 
\end{DoxyReturn}


Definition at line 243 of file one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp.



References Training\+Config\+::\+Discount(), Training\+Config\+::\+Gradient\+Limit(), Training\+Config\+::\+Step\+Limit(), Training\+Config\+::\+Step\+Size(), Training\+Config\+::\+Target\+Network\+Sync\+Interval(), and Training\+Config\+::\+Update\+Interval().



The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
/home/aakash/mlpack/src/mlpack/methods/reinforcement\+\_\+learning/\textbf{ async\+\_\+learning.\+hpp}\item 
/home/aakash/mlpack/src/mlpack/methods/reinforcement\+\_\+learning/worker/\textbf{ one\+\_\+step\+\_\+q\+\_\+learning\+\_\+worker.\+hpp}\end{DoxyCompactItemize}

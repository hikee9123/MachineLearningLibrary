\section{Class List}
Here are the classes, structs, unions and interfaces with brief descriptions\+:\begin{DoxyCompactList}
\item\contentsline{section}{\textbf{ Array\+Wrapper$<$ T $>$} \\*This class is used as a shim for cereal to be able to serialize a raw pointer array }{\pageref{classcereal_1_1ArrayWrapper}}{}
\item\contentsline{section}{\textbf{ is\+\_\+cereal\+\_\+archive$<$ Archive $>$} }{\pageref{structcereal_1_1is__cereal__archive}}{}
\item\contentsline{section}{\textbf{ is\+\_\+cereal\+\_\+archive\+\_\+saving$<$ Archive $>$} }{\pageref{structcereal_1_1is__cereal__archive__saving}}{}
\item\contentsline{section}{\textbf{ load\+\_\+visitor$<$ T $>$} }{\pageref{structcereal_1_1load__visitor}}{}
\item\contentsline{section}{\textbf{ Pointer\+Variant\+Wrapper$<$ Variant\+Types $>$} \\*The objective of this class is to create a wrapper for boost\+::variant }{\pageref{classcereal_1_1PointerVariantWrapper}}{}
\item\contentsline{section}{\textbf{ Pointer\+Vector\+Variant\+Wrapper$<$ Variant\+Types $>$} \\*The objective of this class is to create a wrapper for a vector of boost\+::variant that holds pointer }{\pageref{classcereal_1_1PointerVectorVariantWrapper}}{}
\item\contentsline{section}{\textbf{ Pointer\+Vector\+Wrapper$<$ T $>$} \\*The objective of this class is to create a wrapper for std\+::vector that hold pointers by adding also the size of the vector }{\pageref{classcereal_1_1PointerVectorWrapper}}{}
\item\contentsline{section}{\textbf{ Pointer\+Wrapper$<$ T $>$} \\*The objective of this class is to create a wrapper for raw pointer by encapsulating them in a smart pointer of type unique\+\_\+ptr }{\pageref{classcereal_1_1PointerWrapper}}{}
\item\contentsline{section}{\textbf{ save\+\_\+visitor$<$ Archive $>$} }{\pageref{structcereal_1_1save__visitor}}{}
\item\contentsline{section}{\textbf{ Init\+H\+M\+M\+Model} }{\pageref{structInitHMMModel}}{}
\item\contentsline{section}{\textbf{ Is\+Vector$<$ Vec\+Type $>$} \\*If value == true, then Vec\+Type is some sort of Armadillo vector or subview }{\pageref{structIsVector}}{}
\item\contentsline{section}{\textbf{ Is\+Vector$<$ arma\+::\+Col$<$ e\+T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1Col_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Is\+Vector$<$ arma\+::\+Row$<$ e\+T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1Row_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Is\+Vector$<$ arma\+::\+Sp\+Col$<$ e\+T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1SpCol_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Is\+Vector$<$ arma\+::\+Sp\+Row$<$ e\+T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1SpRow_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Is\+Vector$<$ arma\+::\+Sp\+Subview$<$ e\+T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1SpSubview_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Is\+Vector$<$ arma\+::subview\+\_\+col$<$ e\+T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1subview__col_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Is\+Vector$<$ arma\+::subview\+\_\+row$<$ e\+T $>$ $>$} }{\pageref{structIsVector_3_01arma_1_1subview__row_3_01eT_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ K\+Means\+Plus\+Plus\+Initialization} \\*This class implements the k-\/means++ initialization, as described in the following paper\+: }{\pageref{classKMeansPlusPlusInitialization}}{}
\item\contentsline{section}{\textbf{ Layer\+Name\+Visitor} \\*Implementation of a class that returns the string representation of the name of the given layer }{\pageref{classLayerNameVisitor}}{}
\item\contentsline{section}{\textbf{ Ada\+Boost$<$ Weak\+Learner\+Type, Mat\+Type $>$} \\*The \doxyref{Ada\+Boost}{p.}{classmlpack_1_1adaboost_1_1AdaBoost} class }{\pageref{classmlpack_1_1adaboost_1_1AdaBoost}}{}
\item\contentsline{section}{\textbf{ Ada\+Boost\+Model} \\*The model to save to disk }{\pageref{classmlpack_1_1adaboost_1_1AdaBoostModel}}{}
\item\contentsline{section}{\textbf{ A\+M\+F$<$ Termination\+Policy\+Type, Initialization\+Rule\+Type, Update\+Rule\+Type $>$} \\*This class implements \doxyref{A\+MF}{p.}{classmlpack_1_1amf_1_1AMF} (alternating matrix factorization) on the given matrix V }{\pageref{classmlpack_1_1amf_1_1AMF}}{}
\item\contentsline{section}{\textbf{ Average\+Initialization} \\*This initialization rule initializes matrix W and H to root of the average of V, perturbed with uniform noise }{\pageref{classmlpack_1_1amf_1_1AverageInitialization}}{}
\item\contentsline{section}{\textbf{ Complete\+Incremental\+Termination$<$ Termination\+Policy $>$} \\*This class acts as a wrapper for basic termination policies to be used by \doxyref{S\+V\+D\+Complete\+Incremental\+Learning}{p.}{classmlpack_1_1amf_1_1SVDCompleteIncrementalLearning} }{\pageref{classmlpack_1_1amf_1_1CompleteIncrementalTermination}}{}
\item\contentsline{section}{\textbf{ Given\+Initialization} \\*This initialization rule for \doxyref{A\+MF}{p.}{classmlpack_1_1amf_1_1AMF} simply fills the W and H matrices with the matrices given to the constructor of this object }{\pageref{classmlpack_1_1amf_1_1GivenInitialization}}{}
\item\contentsline{section}{\textbf{ Incomplete\+Incremental\+Termination$<$ Termination\+Policy $>$} \\*This class acts as a wrapper for basic termination policies to be used by \doxyref{S\+V\+D\+Incomplete\+Incremental\+Learning}{p.}{classmlpack_1_1amf_1_1SVDIncompleteIncrementalLearning} }{\pageref{classmlpack_1_1amf_1_1IncompleteIncrementalTermination}}{}
\item\contentsline{section}{\textbf{ Max\+Iteration\+Termination} \\*This termination policy only terminates when the maximum number of iterations has been reached }{\pageref{classmlpack_1_1amf_1_1MaxIterationTermination}}{}
\item\contentsline{section}{\textbf{ Merge\+Initialization$<$ W\+Initialization\+Rule\+Type, H\+Initialization\+Rule\+Type $>$} \\*This initialization rule for \doxyref{A\+MF}{p.}{classmlpack_1_1amf_1_1AMF} simply takes in two initialization rules, and initialize W with the first rule and H with the second rule }{\pageref{classmlpack_1_1amf_1_1MergeInitialization}}{}
\item\contentsline{section}{\textbf{ N\+M\+F\+A\+L\+S\+Update} \\*This class implements a method titled \textquotesingle{}Alternating Least Squares\textquotesingle{} described in the following paper\+: }{\pageref{classmlpack_1_1amf_1_1NMFALSUpdate}}{}
\item\contentsline{section}{\textbf{ N\+M\+F\+Multiplicative\+Distance\+Update} \\*The multiplicative distance update rules for matrices W and H }{\pageref{classmlpack_1_1amf_1_1NMFMultiplicativeDistanceUpdate}}{}
\item\contentsline{section}{\textbf{ N\+M\+F\+Multiplicative\+Divergence\+Update} \\*This follows a method described in the paper \textquotesingle{}Algorithms for Non-\/negative }{\pageref{classmlpack_1_1amf_1_1NMFMultiplicativeDivergenceUpdate}}{}
\item\contentsline{section}{\textbf{ Random\+Acol\+Initialization$<$ columns\+To\+Average $>$} \\*This class initializes the W matrix of the \doxyref{A\+MF}{p.}{classmlpack_1_1amf_1_1AMF} algorithm by averaging p randomly chosen columns of V }{\pageref{classmlpack_1_1amf_1_1RandomAcolInitialization}}{}
\item\contentsline{section}{\textbf{ Random\+Initialization} \\*This initialization rule for \doxyref{A\+MF}{p.}{classmlpack_1_1amf_1_1AMF} simply fills the W and H matrices with uniform random noise in [0, 1] }{\pageref{classmlpack_1_1amf_1_1RandomInitialization}}{}
\item\contentsline{section}{\textbf{ Simple\+Residue\+Termination} \\*This class implements a simple residue-\/based termination policy }{\pageref{classmlpack_1_1amf_1_1SimpleResidueTermination}}{}
\item\contentsline{section}{\textbf{ Simple\+Tolerance\+Termination$<$ Mat\+Type $>$} \\*This class implements residue tolerance termination policy }{\pageref{classmlpack_1_1amf_1_1SimpleToleranceTermination}}{}
\item\contentsline{section}{\textbf{ S\+V\+D\+Batch\+Learning} \\*This class implements S\+VD batch learning with momentum }{\pageref{classmlpack_1_1amf_1_1SVDBatchLearning}}{}
\item\contentsline{section}{\textbf{ S\+V\+D\+Complete\+Incremental\+Learning$<$ Mat\+Type $>$} \\*This class computes S\+VD using complete incremental batch learning, as described in the following paper\+: }{\pageref{classmlpack_1_1amf_1_1SVDCompleteIncrementalLearning}}{}
\item\contentsline{section}{\textbf{ S\+V\+D\+Complete\+Incremental\+Learning$<$ arma\+::sp\+\_\+mat $>$} \\*T\+O\+DO \+: Merge this template specialized function for sparse matrix using common row\+\_\+col\+\_\+iterator }{\pageref{classmlpack_1_1amf_1_1SVDCompleteIncrementalLearning_3_01arma_1_1sp__mat_01_4}}{}
\item\contentsline{section}{\textbf{ S\+V\+D\+Incomplete\+Incremental\+Learning} \\*This class computes S\+VD using incomplete incremental batch learning, as described in the following paper\+: }{\pageref{classmlpack_1_1amf_1_1SVDIncompleteIncrementalLearning}}{}
\item\contentsline{section}{\textbf{ Validation\+R\+M\+S\+E\+Termination$<$ Mat\+Type $>$} \\*This class implements validation termination policy based on R\+M\+SE index }{\pageref{classmlpack_1_1amf_1_1ValidationRMSETermination}}{}
\item\contentsline{section}{\textbf{ Adaptive\+Max\+Pooling$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Adaptive\+Max\+Pooling}{p.}{classmlpack_1_1ann_1_1AdaptiveMaxPooling} layer }{\pageref{classmlpack_1_1ann_1_1AdaptiveMaxPooling}}{}
\item\contentsline{section}{\textbf{ Adaptive\+Mean\+Pooling$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Adaptive\+Mean\+Pooling}{p.}{classmlpack_1_1ann_1_1AdaptiveMeanPooling} }{\pageref{classmlpack_1_1ann_1_1AdaptiveMeanPooling}}{}
\item\contentsline{section}{\textbf{ Add$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Add}{p.}{classmlpack_1_1ann_1_1Add} module class }{\pageref{classmlpack_1_1ann_1_1Add}}{}
\item\contentsline{section}{\textbf{ Add\+Merge$<$ Input\+Data\+Type, Output\+Data\+Type, Custom\+Layers $>$} \\*Implementation of the \doxyref{Add\+Merge}{p.}{classmlpack_1_1ann_1_1AddMerge} module class }{\pageref{classmlpack_1_1ann_1_1AddMerge}}{}
\item\contentsline{section}{\textbf{ Add\+Visitor$<$ Custom\+Layers $>$} \\*\doxyref{Add\+Visitor}{p.}{classmlpack_1_1ann_1_1AddVisitor} exposes the Add() method of the given module }{\pageref{classmlpack_1_1ann_1_1AddVisitor}}{}
\item\contentsline{section}{\textbf{ Alpha\+Dropout$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The alpha -\/ dropout layer is a regularizer that randomly with probability \textquotesingle{}ratio\textquotesingle{} sets input values to alpha\+Dash }{\pageref{classmlpack_1_1ann_1_1AlphaDropout}}{}
\item\contentsline{section}{\textbf{ Atrous\+Convolution$<$ Forward\+Convolution\+Rule, Backward\+Convolution\+Rule, Gradient\+Convolution\+Rule, Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the Atrous \doxyref{Convolution}{p.}{classmlpack_1_1ann_1_1Convolution} class }{\pageref{classmlpack_1_1ann_1_1AtrousConvolution}}{}
\item\contentsline{section}{\textbf{ Add\+Task} \\*Generator of instances of the binary addition task }{\pageref{classmlpack_1_1ann_1_1augmented_1_1tasks_1_1AddTask}}{}
\item\contentsline{section}{\textbf{ Copy\+Task} \\*Generator of instances of the binary sequence copy task }{\pageref{classmlpack_1_1ann_1_1augmented_1_1tasks_1_1CopyTask}}{}
\item\contentsline{section}{\textbf{ Sort\+Task} \\*Generator of instances of the sequence sort task }{\pageref{classmlpack_1_1ann_1_1augmented_1_1tasks_1_1SortTask}}{}
\item\contentsline{section}{\textbf{ Backward\+Visitor} \\*\doxyref{Backward\+Visitor}{p.}{classmlpack_1_1ann_1_1BackwardVisitor} executes the Backward() function given the input, error and delta parameter }{\pageref{classmlpack_1_1ann_1_1BackwardVisitor}}{}
\item\contentsline{section}{\textbf{ Base\+Layer$<$ Activation\+Function, Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the base layer }{\pageref{classmlpack_1_1ann_1_1BaseLayer}}{}
\item\contentsline{section}{\textbf{ Batch\+Norm$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Declaration of the Batch Normalization layer class }{\pageref{classmlpack_1_1ann_1_1BatchNorm}}{}
\item\contentsline{section}{\textbf{ B\+C\+E\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The binary-\/cross-\/entropy performance function measures the Binary Cross Entropy between the target and the output }{\pageref{classmlpack_1_1ann_1_1BCELoss}}{}
\item\contentsline{section}{\textbf{ Bernoulli\+Distribution$<$ Data\+Type $>$} \\*Multiple independent Bernoulli distributions }{\pageref{classmlpack_1_1ann_1_1BernoulliDistribution}}{}
\item\contentsline{section}{\textbf{ Bias\+Set\+Visitor} \\*\doxyref{Bias\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1BiasSetVisitor} updates the module bias parameters given the parameters set }{\pageref{classmlpack_1_1ann_1_1BiasSetVisitor}}{}
\item\contentsline{section}{\textbf{ Bilinear\+Interpolation$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Definition and Implementation of the Bilinear Interpolation Layer }{\pageref{classmlpack_1_1ann_1_1BilinearInterpolation}}{}
\item\contentsline{section}{\textbf{ Binary\+R\+BM} \\*For more information, see the following paper\+: }{\pageref{classmlpack_1_1ann_1_1BinaryRBM}}{}
\item\contentsline{section}{\textbf{ B\+R\+N\+N$<$ Output\+Layer\+Type, Merge\+Layer\+Type, Merge\+Output\+Type, Initialization\+Rule\+Type, Custom\+Layers $>$} \\*Implementation of a standard bidirectional recurrent neural network container }{\pageref{classmlpack_1_1ann_1_1BRNN}}{}
\item\contentsline{section}{\textbf{ C\+E\+L\+U$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The \doxyref{C\+E\+LU}{p.}{classmlpack_1_1ann_1_1CELU} activation function, defined by }{\pageref{classmlpack_1_1ann_1_1CELU}}{}
\item\contentsline{section}{\textbf{ Concat$<$ Input\+Data\+Type, Output\+Data\+Type, Custom\+Layers $>$} \\*Implementation of the \doxyref{Concat}{p.}{classmlpack_1_1ann_1_1Concat} class }{\pageref{classmlpack_1_1ann_1_1Concat}}{}
\item\contentsline{section}{\textbf{ Concatenate$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Concatenate}{p.}{classmlpack_1_1ann_1_1Concatenate} module class }{\pageref{classmlpack_1_1ann_1_1Concatenate}}{}
\item\contentsline{section}{\textbf{ Concat\+Performance$<$ Output\+Layer\+Type, Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the concat performance class }{\pageref{classmlpack_1_1ann_1_1ConcatPerformance}}{}
\item\contentsline{section}{\textbf{ Constant$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the constant layer }{\pageref{classmlpack_1_1ann_1_1Constant}}{}
\item\contentsline{section}{\textbf{ Const\+Initialization} \\*This class is used to initialize weight matrix with constant values }{\pageref{classmlpack_1_1ann_1_1ConstInitialization}}{}
\item\contentsline{section}{\textbf{ Convolution$<$ Forward\+Convolution\+Rule, Backward\+Convolution\+Rule, Gradient\+Convolution\+Rule, Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Convolution}{p.}{classmlpack_1_1ann_1_1Convolution} class }{\pageref{classmlpack_1_1ann_1_1Convolution}}{}
\item\contentsline{section}{\textbf{ Copy\+Visitor$<$ Custom\+Layers $>$} \\*This visitor is to support copy constructor for neural network module }{\pageref{classmlpack_1_1ann_1_1CopyVisitor}}{}
\item\contentsline{section}{\textbf{ Cosine\+Embedding\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Cosine Embedding Loss function is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-\/supervised learning }{\pageref{classmlpack_1_1ann_1_1CosineEmbeddingLoss}}{}
\item\contentsline{section}{\textbf{ C\+Re\+L\+U$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*A concatenated Re\+LU has two outputs, one Re\+LU and one negative Re\+LU, concatenated together }{\pageref{classmlpack_1_1ann_1_1CReLU}}{}
\item\contentsline{section}{\textbf{ D\+C\+G\+AN} \\*For more information, see the following paper\+: }{\pageref{classmlpack_1_1ann_1_1DCGAN}}{}
\item\contentsline{section}{\textbf{ Delete\+Visitor} \\*\doxyref{Delete\+Visitor}{p.}{classmlpack_1_1ann_1_1DeleteVisitor} executes the destructor of the instantiated object }{\pageref{classmlpack_1_1ann_1_1DeleteVisitor}}{}
\item\contentsline{section}{\textbf{ Delta\+Visitor} \\*\doxyref{Delta\+Visitor}{p.}{classmlpack_1_1ann_1_1DeltaVisitor} exposes the delta parameter of the given module }{\pageref{classmlpack_1_1ann_1_1DeltaVisitor}}{}
\item\contentsline{section}{\textbf{ Deterministic\+Set\+Visitor} \\*\doxyref{Deterministic\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1DeterministicSetVisitor} set the deterministic parameter given the deterministic value }{\pageref{classmlpack_1_1ann_1_1DeterministicSetVisitor}}{}
\item\contentsline{section}{\textbf{ Dice\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The dice loss performance function measures the network\textquotesingle{}s performance according to the dice coefficient between the input and target distributions }{\pageref{classmlpack_1_1ann_1_1DiceLoss}}{}
\item\contentsline{section}{\textbf{ Drop\+Connect$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The \doxyref{Drop\+Connect}{p.}{classmlpack_1_1ann_1_1DropConnect} layer is a regularizer that randomly with probability ratio sets the connection values to zero and scales the remaining elements by factor 1 /(1 -\/ ratio) }{\pageref{classmlpack_1_1ann_1_1DropConnect}}{}
\item\contentsline{section}{\textbf{ Dropout$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The dropout layer is a regularizer that randomly with probability \textquotesingle{}ratio\textquotesingle{} sets input values to zero and scales the remaining elements by factor 1 / (1 -\/ ratio) rather than during test time so as to keep the expected sum same }{\pageref{classmlpack_1_1ann_1_1Dropout}}{}
\item\contentsline{section}{\textbf{ Earth\+Mover\+Distance$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The earth mover distance function measures the network\textquotesingle{}s performance according to the Kantorovich-\/\+Rubinstein duality approximation }{\pageref{classmlpack_1_1ann_1_1EarthMoverDistance}}{}
\item\contentsline{section}{\textbf{ Elish\+Function} \\*The E\+Li\+SH function, defined by }{\pageref{classmlpack_1_1ann_1_1ElishFunction}}{}
\item\contentsline{section}{\textbf{ Elliot\+Function} \\*The Elliot function, defined by }{\pageref{classmlpack_1_1ann_1_1ElliotFunction}}{}
\item\contentsline{section}{\textbf{ E\+L\+U$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The \doxyref{E\+LU}{p.}{classmlpack_1_1ann_1_1ELU} activation function, defined by }{\pageref{classmlpack_1_1ann_1_1ELU}}{}
\item\contentsline{section}{\textbf{ Empty\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The empty loss does nothing, letting the user calculate the loss outside the model }{\pageref{classmlpack_1_1ann_1_1EmptyLoss}}{}
\item\contentsline{section}{\textbf{ Fast\+L\+S\+T\+M$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*An implementation of a faster version of the Fast \doxyref{L\+S\+TM}{p.}{classmlpack_1_1ann_1_1LSTM} network layer }{\pageref{classmlpack_1_1ann_1_1FastLSTM}}{}
\item\contentsline{section}{\textbf{ F\+F\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type, Custom\+Layers $>$} \\*Implementation of a standard feed forward network }{\pageref{classmlpack_1_1ann_1_1FFN}}{}
\item\contentsline{section}{\textbf{ F\+F\+T\+Convolution$<$ Border\+Mode, pad\+Last\+Dim $>$} \\*Computes the two-\/dimensional convolution through fft }{\pageref{classmlpack_1_1ann_1_1FFTConvolution}}{}
\item\contentsline{section}{\textbf{ Flatten\+T\+Swish$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The Flatten T Swish activation function, defined by }{\pageref{classmlpack_1_1ann_1_1FlattenTSwish}}{}
\item\contentsline{section}{\textbf{ Flexible\+Re\+L\+U$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The \doxyref{Flexible\+Re\+LU}{p.}{classmlpack_1_1ann_1_1FlexibleReLU} activation function, defined by }{\pageref{classmlpack_1_1ann_1_1FlexibleReLU}}{}
\item\contentsline{section}{\textbf{ Forward\+Visitor} \\*\doxyref{Forward\+Visitor}{p.}{classmlpack_1_1ann_1_1ForwardVisitor} executes the Forward() function given the input and output parameter }{\pageref{classmlpack_1_1ann_1_1ForwardVisitor}}{}
\item\contentsline{section}{\textbf{ Full\+Convolution} }{\pageref{classmlpack_1_1ann_1_1FullConvolution}}{}
\item\contentsline{section}{\textbf{ G\+A\+N$<$ Model, Initialization\+Rule\+Type, Noise, Policy\+Type $>$} \\*The implementation of the standard \doxyref{G\+AN}{p.}{classmlpack_1_1ann_1_1GAN} module }{\pageref{classmlpack_1_1ann_1_1GAN}}{}
\item\contentsline{section}{\textbf{ Gaussian\+Function} \\*The gaussian function, defined by }{\pageref{classmlpack_1_1ann_1_1GaussianFunction}}{}
\item\contentsline{section}{\textbf{ Gaussian\+Initialization} \\*This class is used to initialize weigth matrix with a gaussian }{\pageref{classmlpack_1_1ann_1_1GaussianInitialization}}{}
\item\contentsline{section}{\textbf{ G\+E\+L\+U\+Function} \\*The G\+E\+LU function, defined by }{\pageref{classmlpack_1_1ann_1_1GELUFunction}}{}
\item\contentsline{section}{\textbf{ Glimpse$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The glimpse layer returns a retina-\/like representation (down-\/scaled cropped images) of increasing scale around a given location in a given image }{\pageref{classmlpack_1_1ann_1_1Glimpse}}{}
\item\contentsline{section}{\textbf{ Glorot\+Initialization\+Type$<$ Uniform $>$} \\*This class is used to initialize the weight matrix with the Glorot Initialization method }{\pageref{classmlpack_1_1ann_1_1GlorotInitializationType}}{}
\item\contentsline{section}{\textbf{ Gradient\+Set\+Visitor} \\*\doxyref{Gradient\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1GradientSetVisitor} update the gradient parameter given the gradient set }{\pageref{classmlpack_1_1ann_1_1GradientSetVisitor}}{}
\item\contentsline{section}{\textbf{ Gradient\+Update\+Visitor} \\*\doxyref{Gradient\+Update\+Visitor}{p.}{classmlpack_1_1ann_1_1GradientUpdateVisitor} update the gradient parameter given the gradient set }{\pageref{classmlpack_1_1ann_1_1GradientUpdateVisitor}}{}
\item\contentsline{section}{\textbf{ Gradient\+Visitor} \\*Search\+Mode\+Visitor executes the Gradient() method of the given module using the input and delta parameter }{\pageref{classmlpack_1_1ann_1_1GradientVisitor}}{}
\item\contentsline{section}{\textbf{ Gradient\+Zero\+Visitor} }{\pageref{classmlpack_1_1ann_1_1GradientZeroVisitor}}{}
\item\contentsline{section}{\textbf{ G\+R\+U$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*An implementation of a gru network layer }{\pageref{classmlpack_1_1ann_1_1GRU}}{}
\item\contentsline{section}{\textbf{ Hard\+Shrink$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Hard Shrink operator is defined as, }{\pageref{classmlpack_1_1ann_1_1HardShrink}}{}
\item\contentsline{section}{\textbf{ Hard\+Sigmoid\+Function} \\*The hard sigmoid function, defined by }{\pageref{classmlpack_1_1ann_1_1HardSigmoidFunction}}{}
\item\contentsline{section}{\textbf{ Hard\+Swish\+Function} \\*The Hard Swish function, defined by }{\pageref{classmlpack_1_1ann_1_1HardSwishFunction}}{}
\item\contentsline{section}{\textbf{ Hard\+Tan\+H$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The Hard Tanh activation function, defined by }{\pageref{classmlpack_1_1ann_1_1HardTanH}}{}
\item\contentsline{section}{\textbf{ He\+Initialization} \\*This class is used to initialize weight matrix with the He initialization rule given by He et }{\pageref{classmlpack_1_1ann_1_1HeInitialization}}{}
\item\contentsline{section}{\textbf{ Highway$<$ Input\+Data\+Type, Output\+Data\+Type, Custom\+Layers $>$} \\*Implementation of the \doxyref{Highway}{p.}{classmlpack_1_1ann_1_1Highway} layer }{\pageref{classmlpack_1_1ann_1_1Highway}}{}
\item\contentsline{section}{\textbf{ Hinge\+Embedding\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The Hinge Embedding loss function is often used to compute the loss between y\+\_\+true and y\+\_\+pred }{\pageref{classmlpack_1_1ann_1_1HingeEmbeddingLoss}}{}
\item\contentsline{section}{\textbf{ Hinge\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Computes the hinge loss between $y_true$ and $y_pred$ }{\pageref{classmlpack_1_1ann_1_1HingeLoss}}{}
\item\contentsline{section}{\textbf{ Huber\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The Huber loss is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss }{\pageref{classmlpack_1_1ann_1_1HuberLoss}}{}
\item\contentsline{section}{\textbf{ Identity\+Function} \\*The identity function, defined by }{\pageref{classmlpack_1_1ann_1_1IdentityFunction}}{}
\item\contentsline{section}{\textbf{ Init\+Traits$<$ Init\+Rule\+Type $>$} \\*This is a template class that can provide information about various initialization methods }{\pageref{classmlpack_1_1ann_1_1InitTraits}}{}
\item\contentsline{section}{\textbf{ Init\+Traits$<$ Kathirvalavakumar\+Subavathi\+Initialization $>$} \\*Initialization traits of the kathirvalavakumar subavath initialization rule }{\pageref{classmlpack_1_1ann_1_1InitTraits_3_01KathirvalavakumarSubavathiInitialization_01_4}}{}
\item\contentsline{section}{\textbf{ Init\+Traits$<$ Nguyen\+Widrow\+Initialization $>$} \\*Initialization traits of the Nguyen-\/\+Widrow initialization rule }{\pageref{classmlpack_1_1ann_1_1InitTraits_3_01NguyenWidrowInitialization_01_4}}{}
\item\contentsline{section}{\textbf{ In\+Shape\+Visitor} \\*\doxyref{In\+Shape\+Visitor}{p.}{classmlpack_1_1ann_1_1InShapeVisitor} returns the input shape a Layer expects }{\pageref{classmlpack_1_1ann_1_1InShapeVisitor}}{}
\item\contentsline{section}{\textbf{ Inv\+Quad\+Function} \\*The Inverse Quadratic function, defined by }{\pageref{classmlpack_1_1ann_1_1InvQuadFunction}}{}
\item\contentsline{section}{\textbf{ I\+S\+R\+L\+U$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The \doxyref{I\+S\+R\+LU}{p.}{classmlpack_1_1ann_1_1ISRLU} activation function, defined by }{\pageref{classmlpack_1_1ann_1_1ISRLU}}{}
\item\contentsline{section}{\textbf{ Join$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Join}{p.}{classmlpack_1_1ann_1_1Join} module class }{\pageref{classmlpack_1_1ann_1_1Join}}{}
\item\contentsline{section}{\textbf{ Kathirvalavakumar\+Subavathi\+Initialization} \\*This class is used to initialize the weight matrix with the method proposed by T }{\pageref{classmlpack_1_1ann_1_1KathirvalavakumarSubavathiInitialization}}{}
\item\contentsline{section}{\textbf{ K\+L\+Divergence$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The Kullback–\+Leibler divergence is often used for continuous distributions (direct regression) }{\pageref{classmlpack_1_1ann_1_1KLDivergence}}{}
\item\contentsline{section}{\textbf{ L1\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The L1 loss is a loss function that measures the mean absolute error (M\+AE) between each element in the input x and target y }{\pageref{classmlpack_1_1ann_1_1L1Loss}}{}
\item\contentsline{section}{\textbf{ Layer\+Norm$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Declaration of the Layer Normalization class }{\pageref{classmlpack_1_1ann_1_1LayerNorm}}{}
\item\contentsline{section}{\textbf{ Layer\+Traits$<$ Layer\+Type $>$} \\*This is a template class that can provide information about various layers }{\pageref{classmlpack_1_1ann_1_1LayerTraits}}{}
\item\contentsline{section}{\textbf{ Leaky\+Re\+L\+U$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The \doxyref{Leaky\+Re\+LU}{p.}{classmlpack_1_1ann_1_1LeakyReLU} activation function, defined by }{\pageref{classmlpack_1_1ann_1_1LeakyReLU}}{}
\item\contentsline{section}{\textbf{ Lecun\+Normal\+Initialization} \\*This class is used to initialize weight matrix with the Lecun Normalization initialization rule }{\pageref{classmlpack_1_1ann_1_1LecunNormalInitialization}}{}
\item\contentsline{section}{\textbf{ Linear$<$ Input\+Data\+Type, Output\+Data\+Type, Regularizer\+Type $>$} \\*Implementation of the \doxyref{Linear}{p.}{classmlpack_1_1ann_1_1Linear} layer class }{\pageref{classmlpack_1_1ann_1_1Linear}}{}
\item\contentsline{section}{\textbf{ Linear3\+D$<$ Input\+Data\+Type, Output\+Data\+Type, Regularizer\+Type $>$} \\*Implementation of the \doxyref{Linear3D}{p.}{classmlpack_1_1ann_1_1Linear3D} layer class }{\pageref{classmlpack_1_1ann_1_1Linear3D}}{}
\item\contentsline{section}{\textbf{ Linear\+No\+Bias$<$ Input\+Data\+Type, Output\+Data\+Type, Regularizer\+Type $>$} \\*Implementation of the \doxyref{Linear\+No\+Bias}{p.}{classmlpack_1_1ann_1_1LinearNoBias} class }{\pageref{classmlpack_1_1ann_1_1LinearNoBias}}{}
\item\contentsline{section}{\textbf{ Li\+S\+H\+T\+Function} \\*The Li\+S\+HT function, defined by }{\pageref{classmlpack_1_1ann_1_1LiSHTFunction}}{}
\item\contentsline{section}{\textbf{ Load\+Output\+Parameter\+Visitor} \\*\doxyref{Load\+Output\+Parameter\+Visitor}{p.}{classmlpack_1_1ann_1_1LoadOutputParameterVisitor} restores the output parameter using the given parameter set }{\pageref{classmlpack_1_1ann_1_1LoadOutputParameterVisitor}}{}
\item\contentsline{section}{\textbf{ Log\+Cosh\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The Log-\/\+Hyperbolic-\/\+Cosine loss function is often used to improve variational auto encoder }{\pageref{classmlpack_1_1ann_1_1LogCoshLoss}}{}
\item\contentsline{section}{\textbf{ Logistic\+Function} \\*The logistic function, defined by }{\pageref{classmlpack_1_1ann_1_1LogisticFunction}}{}
\item\contentsline{section}{\textbf{ Log\+Soft\+Max$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the log softmax layer }{\pageref{classmlpack_1_1ann_1_1LogSoftMax}}{}
\item\contentsline{section}{\textbf{ Lookup$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The \doxyref{Lookup}{p.}{classmlpack_1_1ann_1_1Lookup} class stores word embeddings and retrieves them using tokens }{\pageref{classmlpack_1_1ann_1_1Lookup}}{}
\item\contentsline{section}{\textbf{ Loss\+Visitor} \\*\doxyref{Loss\+Visitor}{p.}{classmlpack_1_1ann_1_1LossVisitor} exposes the Loss() method of the given module }{\pageref{classmlpack_1_1ann_1_1LossVisitor}}{}
\item\contentsline{section}{\textbf{ Lp\+Pooling$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the L\+P\+Pooling }{\pageref{classmlpack_1_1ann_1_1LpPooling}}{}
\item\contentsline{section}{\textbf{ L\+Regularizer$<$ T\+Power $>$} \\*The L\+\_\+p regularizer for arbitrary integer p }{\pageref{classmlpack_1_1ann_1_1LRegularizer}}{}
\item\contentsline{section}{\textbf{ L\+S\+T\+M$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{L\+S\+TM}{p.}{classmlpack_1_1ann_1_1LSTM} module class }{\pageref{classmlpack_1_1ann_1_1LSTM}}{}
\item\contentsline{section}{\textbf{ Margin\+Ranking\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Margin ranking loss measures the loss given inputs and a label vector with values of 1 or -\/1 }{\pageref{classmlpack_1_1ann_1_1MarginRankingLoss}}{}
\item\contentsline{section}{\textbf{ Max\+Pooling$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Max\+Pooling}{p.}{classmlpack_1_1ann_1_1MaxPooling} layer }{\pageref{classmlpack_1_1ann_1_1MaxPooling}}{}
\item\contentsline{section}{\textbf{ Max\+Pooling\+Rule} }{\pageref{classmlpack_1_1ann_1_1MaxPoolingRule}}{}
\item\contentsline{section}{\textbf{ Mean\+Absolute\+Percentage\+Error$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The mean absolute percentage error performance function measures the network\textquotesingle{}s performance according to the mean of the absolute difference between input and target divided by target }{\pageref{classmlpack_1_1ann_1_1MeanAbsolutePercentageError}}{}
\item\contentsline{section}{\textbf{ Mean\+Bias\+Error$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The mean bias error performance function measures the network\textquotesingle{}s performance according to the mean of errors }{\pageref{classmlpack_1_1ann_1_1MeanBiasError}}{}
\item\contentsline{section}{\textbf{ Mean\+Pooling$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Mean\+Pooling}{p.}{classmlpack_1_1ann_1_1MeanPooling} }{\pageref{classmlpack_1_1ann_1_1MeanPooling}}{}
\item\contentsline{section}{\textbf{ Mean\+Pooling\+Rule} }{\pageref{classmlpack_1_1ann_1_1MeanPoolingRule}}{}
\item\contentsline{section}{\textbf{ Mean\+Squared\+Error$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The mean squared error performance function measures the network\textquotesingle{}s performance according to the mean of squared errors }{\pageref{classmlpack_1_1ann_1_1MeanSquaredError}}{}
\item\contentsline{section}{\textbf{ Mean\+Squared\+Logarithmic\+Error$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The mean squared logarithmic error performance function measures the network\textquotesingle{}s performance according to the mean of squared logarithmic errors }{\pageref{classmlpack_1_1ann_1_1MeanSquaredLogarithmicError}}{}
\item\contentsline{section}{\textbf{ Mini\+Batch\+Discrimination$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Mini\+Batch\+Discrimination}{p.}{classmlpack_1_1ann_1_1MiniBatchDiscrimination} layer }{\pageref{classmlpack_1_1ann_1_1MiniBatchDiscrimination}}{}
\item\contentsline{section}{\textbf{ Mish\+Function} \\*The Mish function, defined by }{\pageref{classmlpack_1_1ann_1_1MishFunction}}{}
\item\contentsline{section}{\textbf{ Multihead\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type, Regularizer\+Type $>$} \\*Multihead Attention allows the model to jointly attend to information from different representation subspaces at different positions }{\pageref{classmlpack_1_1ann_1_1MultiheadAttention}}{}
\item\contentsline{section}{\textbf{ Multiply\+Constant$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the multiply constant layer }{\pageref{classmlpack_1_1ann_1_1MultiplyConstant}}{}
\item\contentsline{section}{\textbf{ Multiply\+Merge$<$ Input\+Data\+Type, Output\+Data\+Type, Custom\+Layers $>$} \\*Implementation of the \doxyref{Multiply\+Merge}{p.}{classmlpack_1_1ann_1_1MultiplyMerge} module class }{\pageref{classmlpack_1_1ann_1_1MultiplyMerge}}{}
\item\contentsline{section}{\textbf{ Multi\+Quad\+Function} \\*The Multi Quadratic function, defined by }{\pageref{classmlpack_1_1ann_1_1MultiQuadFunction}}{}
\item\contentsline{section}{\textbf{ Naive\+Convolution$<$ Border\+Mode $>$} \\*Computes the two-\/dimensional convolution }{\pageref{classmlpack_1_1ann_1_1NaiveConvolution}}{}
\item\contentsline{section}{\textbf{ Negative\+Log\+Likelihood$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the negative log likelihood layer }{\pageref{classmlpack_1_1ann_1_1NegativeLogLikelihood}}{}
\item\contentsline{section}{\textbf{ Network\+Initialization$<$ Initialization\+Rule\+Type, Custom\+Layers $>$} \\*This class is used to initialize the network with the given initialization rule }{\pageref{classmlpack_1_1ann_1_1NetworkInitialization}}{}
\item\contentsline{section}{\textbf{ Nguyen\+Widrow\+Initialization} \\*This class is used to initialize the weight matrix with the Nguyen-\/\+Widrow method }{\pageref{classmlpack_1_1ann_1_1NguyenWidrowInitialization}}{}
\item\contentsline{section}{\textbf{ Noisy\+Linear$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Noisy\+Linear}{p.}{classmlpack_1_1ann_1_1NoisyLinear} layer class }{\pageref{classmlpack_1_1ann_1_1NoisyLinear}}{}
\item\contentsline{section}{\textbf{ No\+Regularizer} \\*Implementation of the \doxyref{No\+Regularizer}{p.}{classmlpack_1_1ann_1_1NoRegularizer} }{\pageref{classmlpack_1_1ann_1_1NoRegularizer}}{}
\item\contentsline{section}{\textbf{ Normal\+Distribution$<$ Data\+Type $>$} \\*Implementation of the Normal Distribution function }{\pageref{classmlpack_1_1ann_1_1NormalDistribution}}{}
\item\contentsline{section}{\textbf{ Oivs\+Initialization$<$ Activation\+Function $>$} \\*This class is used to initialize the weight matrix with the oivs method }{\pageref{classmlpack_1_1ann_1_1OivsInitialization}}{}
\item\contentsline{section}{\textbf{ Orthogonal\+Initialization} \\*This class is used to initialize the weight matrix with the orthogonal matrix initialization }{\pageref{classmlpack_1_1ann_1_1OrthogonalInitialization}}{}
\item\contentsline{section}{\textbf{ Orthogonal\+Regularizer} \\*Implementation of the \doxyref{Orthogonal\+Regularizer}{p.}{classmlpack_1_1ann_1_1OrthogonalRegularizer} }{\pageref{classmlpack_1_1ann_1_1OrthogonalRegularizer}}{}
\item\contentsline{section}{\textbf{ Output\+Height\+Visitor} \\*\doxyref{Output\+Height\+Visitor}{p.}{classmlpack_1_1ann_1_1OutputHeightVisitor} exposes the Output\+Height() method of the given module }{\pageref{classmlpack_1_1ann_1_1OutputHeightVisitor}}{}
\item\contentsline{section}{\textbf{ Output\+Parameter\+Visitor} \\*\doxyref{Output\+Parameter\+Visitor}{p.}{classmlpack_1_1ann_1_1OutputParameterVisitor} exposes the output parameter of the given module }{\pageref{classmlpack_1_1ann_1_1OutputParameterVisitor}}{}
\item\contentsline{section}{\textbf{ Output\+Width\+Visitor} \\*\doxyref{Output\+Width\+Visitor}{p.}{classmlpack_1_1ann_1_1OutputWidthVisitor} exposes the Output\+Width() method of the given module }{\pageref{classmlpack_1_1ann_1_1OutputWidthVisitor}}{}
\item\contentsline{section}{\textbf{ Padding$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Padding}{p.}{classmlpack_1_1ann_1_1Padding} module class }{\pageref{classmlpack_1_1ann_1_1Padding}}{}
\item\contentsline{section}{\textbf{ Parameters\+Set\+Visitor} \\*\doxyref{Parameters\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1ParametersSetVisitor} update the parameters set using the given matrix }{\pageref{classmlpack_1_1ann_1_1ParametersSetVisitor}}{}
\item\contentsline{section}{\textbf{ Parameters\+Visitor} \\*\doxyref{Parameters\+Visitor}{p.}{classmlpack_1_1ann_1_1ParametersVisitor} exposes the parameters set of the given module and stores the parameters set into the given matrix }{\pageref{classmlpack_1_1ann_1_1ParametersVisitor}}{}
\item\contentsline{section}{\textbf{ Pixel\+Shuffle$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Pixel\+Shuffle}{p.}{classmlpack_1_1ann_1_1PixelShuffle} layer }{\pageref{classmlpack_1_1ann_1_1PixelShuffle}}{}
\item\contentsline{section}{\textbf{ Poisson1\+Function} \\*The Poisson one function, defined by }{\pageref{classmlpack_1_1ann_1_1Poisson1Function}}{}
\item\contentsline{section}{\textbf{ Poisson\+N\+L\+L\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the Poisson negative log likelihood loss }{\pageref{classmlpack_1_1ann_1_1PoissonNLLLoss}}{}
\item\contentsline{section}{\textbf{ Positional\+Encoding$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Positional Encoding injects some information about the relative or absolute position of the tokens in the sequence }{\pageref{classmlpack_1_1ann_1_1PositionalEncoding}}{}
\item\contentsline{section}{\textbf{ P\+Re\+L\+U$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The \doxyref{P\+Re\+LU}{p.}{classmlpack_1_1ann_1_1PReLU} activation function, defined by (where alpha is trainable) }{\pageref{classmlpack_1_1ann_1_1PReLU}}{}
\item\contentsline{section}{\textbf{ Quadratic\+Function} \\*The Quadratic function, defined by }{\pageref{classmlpack_1_1ann_1_1QuadraticFunction}}{}
\item\contentsline{section}{\textbf{ Random\+Initialization} \\*This class is used to initialize randomly the weight matrix }{\pageref{classmlpack_1_1ann_1_1RandomInitialization}}{}
\item\contentsline{section}{\textbf{ R\+B\+F$<$ Input\+Data\+Type, Output\+Data\+Type, Activation $>$} \\*Implementation of the Radial Basis Function layer }{\pageref{classmlpack_1_1ann_1_1RBF}}{}
\item\contentsline{section}{\textbf{ R\+B\+M$<$ Initialization\+Rule\+Type, Data\+Type, Policy\+Type $>$} \\*The implementation of the \doxyref{R\+BM}{p.}{classmlpack_1_1ann_1_1RBM} module }{\pageref{classmlpack_1_1ann_1_1RBM}}{}
\item\contentsline{section}{\textbf{ Reconstruction\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type, Dist\+Type $>$} \\*The reconstruction loss performance function measures the network\textquotesingle{}s performance equal to the negative log probability of the target with the input distribution }{\pageref{classmlpack_1_1ann_1_1ReconstructionLoss}}{}
\item\contentsline{section}{\textbf{ Rectifier\+Function} \\*The rectifier function, defined by }{\pageref{classmlpack_1_1ann_1_1RectifierFunction}}{}
\item\contentsline{section}{\textbf{ Recurrent$<$ Input\+Data\+Type, Output\+Data\+Type, Custom\+Layers $>$} \\*Implementation of the Recurrent\+Layer class }{\pageref{classmlpack_1_1ann_1_1Recurrent}}{}
\item\contentsline{section}{\textbf{ Recurrent\+Attention$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*This class implements the \doxyref{Recurrent}{p.}{classmlpack_1_1ann_1_1Recurrent} Model for Visual Attention, using a variety of possible layer implementations }{\pageref{classmlpack_1_1ann_1_1RecurrentAttention}}{}
\item\contentsline{section}{\textbf{ Reinforce\+Normal$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the reinforce normal layer }{\pageref{classmlpack_1_1ann_1_1ReinforceNormal}}{}
\item\contentsline{section}{\textbf{ Reparametrization$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Reparametrization}{p.}{classmlpack_1_1ann_1_1Reparametrization} layer class }{\pageref{classmlpack_1_1ann_1_1Reparametrization}}{}
\item\contentsline{section}{\textbf{ Reset\+Cell\+Visitor} \\*\doxyref{Reset\+Cell\+Visitor}{p.}{classmlpack_1_1ann_1_1ResetCellVisitor} executes the Reset\+Cell() function }{\pageref{classmlpack_1_1ann_1_1ResetCellVisitor}}{}
\item\contentsline{section}{\textbf{ Reset\+Visitor} \\*\doxyref{Reset\+Visitor}{p.}{classmlpack_1_1ann_1_1ResetVisitor} executes the Reset() function }{\pageref{classmlpack_1_1ann_1_1ResetVisitor}}{}
\item\contentsline{section}{\textbf{ Reward\+Set\+Visitor} \\*\doxyref{Reward\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1RewardSetVisitor} set the reward parameter given the reward value }{\pageref{classmlpack_1_1ann_1_1RewardSetVisitor}}{}
\item\contentsline{section}{\textbf{ R\+N\+N$<$ Output\+Layer\+Type, Initialization\+Rule\+Type, Custom\+Layers $>$} \\*Implementation of a standard recurrent neural network container }{\pageref{classmlpack_1_1ann_1_1RNN}}{}
\item\contentsline{section}{\textbf{ Run\+Set\+Visitor} \\*\doxyref{Run\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1RunSetVisitor} set the run parameter given the run value }{\pageref{classmlpack_1_1ann_1_1RunSetVisitor}}{}
\item\contentsline{section}{\textbf{ Save\+Output\+Parameter\+Visitor} \\*\doxyref{Save\+Output\+Parameter\+Visitor}{p.}{classmlpack_1_1ann_1_1SaveOutputParameterVisitor} saves the output parameter into the given parameter set }{\pageref{classmlpack_1_1ann_1_1SaveOutputParameterVisitor}}{}
\item\contentsline{section}{\textbf{ Select$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The select module selects the specified column from a given input matrix }{\pageref{classmlpack_1_1ann_1_1Select}}{}
\item\contentsline{section}{\textbf{ Sequential$<$ Input\+Data\+Type, Output\+Data\+Type, Residual, Custom\+Layers $>$} \\*Implementation of the \doxyref{Sequential}{p.}{classmlpack_1_1ann_1_1Sequential} class }{\pageref{classmlpack_1_1ann_1_1Sequential}}{}
\item\contentsline{section}{\textbf{ Set\+Input\+Height\+Visitor} \\*\doxyref{Set\+Input\+Height\+Visitor}{p.}{classmlpack_1_1ann_1_1SetInputHeightVisitor} updates the input height parameter with the given input height }{\pageref{classmlpack_1_1ann_1_1SetInputHeightVisitor}}{}
\item\contentsline{section}{\textbf{ Set\+Input\+Width\+Visitor} \\*\doxyref{Set\+Input\+Width\+Visitor}{p.}{classmlpack_1_1ann_1_1SetInputWidthVisitor} updates the input width parameter with the given input width }{\pageref{classmlpack_1_1ann_1_1SetInputWidthVisitor}}{}
\item\contentsline{section}{\textbf{ Sigmoid\+Cross\+Entropy\+Error$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The \doxyref{Sigmoid\+Cross\+Entropy\+Error}{p.}{classmlpack_1_1ann_1_1SigmoidCrossEntropyError} performance function measures the network\textquotesingle{}s performance according to the cross-\/entropy function between the input and target distributions }{\pageref{classmlpack_1_1ann_1_1SigmoidCrossEntropyError}}{}
\item\contentsline{section}{\textbf{ S\+I\+L\+U\+Function} \\*The S\+I\+LU function, defined by }{\pageref{classmlpack_1_1ann_1_1SILUFunction}}{}
\item\contentsline{section}{\textbf{ Soft\+Margin\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} }{\pageref{classmlpack_1_1ann_1_1SoftMarginLoss}}{}
\item\contentsline{section}{\textbf{ Softmax$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Softmax}{p.}{classmlpack_1_1ann_1_1Softmax} layer }{\pageref{classmlpack_1_1ann_1_1Softmax}}{}
\item\contentsline{section}{\textbf{ Softmin$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Softmin}{p.}{classmlpack_1_1ann_1_1Softmin} layer }{\pageref{classmlpack_1_1ann_1_1Softmin}}{}
\item\contentsline{section}{\textbf{ Softplus\+Function} \\*The softplus function, defined by }{\pageref{classmlpack_1_1ann_1_1SoftplusFunction}}{}
\item\contentsline{section}{\textbf{ Soft\+Shrink$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Soft Shrink operator is defined as, \begin{eqnarray*} f(x) &=& \begin{cases} x - \lambda & : x > \lambda \\ x + \lambda & : x < -\lambda \\ 0 & : otherwise. \\ \end{cases} \\ f'(x) &=& \begin{cases} 1 & : x > \lambda \\ 1 & : x < -\lambda \\ 0 & : otherwise. \end{cases} \end{eqnarray*} }{\pageref{classmlpack_1_1ann_1_1SoftShrink}}{}
\item\contentsline{section}{\textbf{ Softsign\+Function} \\*The softsign function, defined by }{\pageref{classmlpack_1_1ann_1_1SoftsignFunction}}{}
\item\contentsline{section}{\textbf{ Spatial\+Dropout$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the \doxyref{Spatial\+Dropout}{p.}{classmlpack_1_1ann_1_1SpatialDropout} layer }{\pageref{classmlpack_1_1ann_1_1SpatialDropout}}{}
\item\contentsline{section}{\textbf{ Spike\+Slab\+R\+BM} \\*For more information, see the following paper\+: }{\pageref{classmlpack_1_1ann_1_1SpikeSlabRBM}}{}
\item\contentsline{section}{\textbf{ Spline\+Function} \\*The Spline function, defined by }{\pageref{classmlpack_1_1ann_1_1SplineFunction}}{}
\item\contentsline{section}{\textbf{ Standard\+G\+AN} \\*For more information, see the following paper\+: }{\pageref{classmlpack_1_1ann_1_1StandardGAN}}{}
\item\contentsline{section}{\textbf{ Subview$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the subview layer }{\pageref{classmlpack_1_1ann_1_1Subview}}{}
\item\contentsline{section}{\textbf{ S\+V\+D\+Convolution$<$ Border\+Mode $>$} \\*Computes the two-\/dimensional convolution using singular value decomposition }{\pageref{classmlpack_1_1ann_1_1SVDConvolution}}{}
\item\contentsline{section}{\textbf{ Swish\+Function} \\*The swish function, defined by }{\pageref{classmlpack_1_1ann_1_1SwishFunction}}{}
\item\contentsline{section}{\textbf{ Tanh\+Exp\+Function} \\*The Tanh\+Exp function, defined by }{\pageref{classmlpack_1_1ann_1_1TanhExpFunction}}{}
\item\contentsline{section}{\textbf{ Tanh\+Function} \\*The tanh function, defined by }{\pageref{classmlpack_1_1ann_1_1TanhFunction}}{}
\item\contentsline{section}{\textbf{ Transposed\+Convolution$<$ Forward\+Convolution\+Rule, Backward\+Convolution\+Rule, Gradient\+Convolution\+Rule, Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the Transposed \doxyref{Convolution}{p.}{classmlpack_1_1ann_1_1Convolution} class }{\pageref{classmlpack_1_1ann_1_1TransposedConvolution}}{}
\item\contentsline{section}{\textbf{ Triplet\+Margin\+Loss$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*The Triplet Margin Loss performance function measures the network\textquotesingle{}s performance according to the relative distance from the anchor input of the positive (truthy) and negative (falsy) inputs }{\pageref{classmlpack_1_1ann_1_1TripletMarginLoss}}{}
\item\contentsline{section}{\textbf{ Valid\+Convolution} }{\pageref{classmlpack_1_1ann_1_1ValidConvolution}}{}
\item\contentsline{section}{\textbf{ Virtual\+Batch\+Norm$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Declaration of the \doxyref{Virtual\+Batch\+Norm}{p.}{classmlpack_1_1ann_1_1VirtualBatchNorm} layer class }{\pageref{classmlpack_1_1ann_1_1VirtualBatchNorm}}{}
\item\contentsline{section}{\textbf{ V\+R\+Class\+Reward$<$ Input\+Data\+Type, Output\+Data\+Type $>$} \\*Implementation of the variance reduced classification reinforcement layer }{\pageref{classmlpack_1_1ann_1_1VRClassReward}}{}
\item\contentsline{section}{\textbf{ Weight\+Norm$<$ Input\+Data\+Type, Output\+Data\+Type, Custom\+Layers $>$} \\*Declaration of the \doxyref{Weight\+Norm}{p.}{classmlpack_1_1ann_1_1WeightNorm} layer class }{\pageref{classmlpack_1_1ann_1_1WeightNorm}}{}
\item\contentsline{section}{\textbf{ Weight\+Set\+Visitor} \\*\doxyref{Weight\+Set\+Visitor}{p.}{classmlpack_1_1ann_1_1WeightSetVisitor} update the module parameters given the parameters set }{\pageref{classmlpack_1_1ann_1_1WeightSetVisitor}}{}
\item\contentsline{section}{\textbf{ Weight\+Size\+Visitor} \\*\doxyref{Weight\+Size\+Visitor}{p.}{classmlpack_1_1ann_1_1WeightSizeVisitor} returns the number of weights of the given module }{\pageref{classmlpack_1_1ann_1_1WeightSizeVisitor}}{}
\item\contentsline{section}{\textbf{ W\+G\+AN} \\*For more information, see the following paper\+: }{\pageref{classmlpack_1_1ann_1_1WGAN}}{}
\item\contentsline{section}{\textbf{ W\+G\+A\+N\+GP} \\*For more information, see the following paper\+: }{\pageref{classmlpack_1_1ann_1_1WGANGP}}{}
\item\contentsline{section}{\textbf{ Backtrace} \\*Provides a backtrace }{\pageref{classmlpack_1_1Backtrace}}{}
\item\contentsline{section}{\textbf{ Ball\+Bound$<$ Metric\+Type, Vec\+Type $>$} \\*Ball bound encloses a set of points at a specific distance (radius) from a specific point (center) }{\pageref{classmlpack_1_1bound_1_1BallBound}}{}
\item\contentsline{section}{\textbf{ Bound\+Traits$<$ Bound\+Type $>$} \\*A class to obtain compile-\/time traits about Bound\+Type classes }{\pageref{structmlpack_1_1bound_1_1BoundTraits}}{}
\item\contentsline{section}{\textbf{ Bound\+Traits$<$ Ball\+Bound$<$ Metric\+Type, Vec\+Type $>$ $>$} \\*A specialization of \doxyref{Bound\+Traits}{p.}{structmlpack_1_1bound_1_1BoundTraits} for this bound type }{\pageref{structmlpack_1_1bound_1_1BoundTraits_3_01BallBound_3_01MetricType_00_01VecType_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Bound\+Traits$<$ Cell\+Bound$<$ Metric\+Type, Elem\+Type $>$ $>$} }{\pageref{structmlpack_1_1bound_1_1BoundTraits_3_01CellBound_3_01MetricType_00_01ElemType_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Bound\+Traits$<$ Hollow\+Ball\+Bound$<$ Metric\+Type, Elem\+Type $>$ $>$} \\*A specialization of \doxyref{Bound\+Traits}{p.}{structmlpack_1_1bound_1_1BoundTraits} for this bound type }{\pageref{structmlpack_1_1bound_1_1BoundTraits_3_01HollowBallBound_3_01MetricType_00_01ElemType_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Bound\+Traits$<$ H\+Rect\+Bound$<$ Metric\+Type, Elem\+Type $>$ $>$} }{\pageref{structmlpack_1_1bound_1_1BoundTraits_3_01HRectBound_3_01MetricType_00_01ElemType_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Cell\+Bound$<$ Metric\+Type, Elem\+Type $>$} \\*The \doxyref{Cell\+Bound}{p.}{classmlpack_1_1bound_1_1CellBound} class describes a bound that consists of a number of hyperrectangles }{\pageref{classmlpack_1_1bound_1_1CellBound}}{}
\item\contentsline{section}{\textbf{ Hollow\+Ball\+Bound$<$ T\+Metric\+Type, Elem\+Type $>$} \\*Hollow ball bound encloses a set of points at a specific distance (radius) from a specific point (center) except points at a specific distance from another point (the center of the hole) }{\pageref{classmlpack_1_1bound_1_1HollowBallBound}}{}
\item\contentsline{section}{\textbf{ H\+Rect\+Bound$<$ Metric\+Type, Elem\+Type $>$} \\*Hyper-\/rectangle bound for an L-\/metric }{\pageref{classmlpack_1_1bound_1_1HRectBound}}{}
\item\contentsline{section}{\textbf{ Is\+L\+Metric$<$ Metric\+Type $>$} \\*Utility struct where Value is true if and only if the argument is of type L\+Metric }{\pageref{structmlpack_1_1bound_1_1meta_1_1IsLMetric}}{}
\item\contentsline{section}{\textbf{ Is\+L\+Metric$<$ metric\+::\+L\+Metric$<$ Power, Take\+Root $>$ $>$} \\*Specialization for \doxyref{Is\+L\+Metric}{p.}{structmlpack_1_1bound_1_1meta_1_1IsLMetric} when the argument is of type L\+Metric }{\pageref{structmlpack_1_1bound_1_1meta_1_1IsLMetric_3_01metric_1_1LMetric_3_01Power_00_01TakeRoot_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Average\+Interpolation} \\*This class performs average interpolation to generate interpolation weights for neighborhood-\/based collaborative filtering }{\pageref{classmlpack_1_1cf_1_1AverageInterpolation}}{}
\item\contentsline{section}{\textbf{ Batch\+S\+V\+D\+Policy} \\*Implementation of the Batch S\+VD policy to act as a wrapper when accessing Batch S\+VD from within \doxyref{C\+F\+Type}{p.}{classmlpack_1_1cf_1_1CFType} }{\pageref{classmlpack_1_1cf_1_1BatchSVDPolicy}}{}
\item\contentsline{section}{\textbf{ Bias\+S\+V\+D\+Policy} \\*Implementation of the Bias S\+VD policy to act as a wrapper when accessing Bias S\+VD from within \doxyref{C\+F\+Type}{p.}{classmlpack_1_1cf_1_1CFType} }{\pageref{classmlpack_1_1cf_1_1BiasSVDPolicy}}{}
\item\contentsline{section}{\textbf{ C\+F\+Model} \\*The model to save to disk }{\pageref{classmlpack_1_1cf_1_1CFModel}}{}
\item\contentsline{section}{\textbf{ C\+F\+Type$<$ Decomposition\+Policy, Normalization\+Type $>$} \\*This class implements Collaborative Filtering (CF) }{\pageref{classmlpack_1_1cf_1_1CFType}}{}
\item\contentsline{section}{\textbf{ C\+F\+Wrapper$<$ Decomposition\+Policy, Normalization\+Policy $>$} \\*The \doxyref{C\+F\+Wrapper}{p.}{classmlpack_1_1cf_1_1CFWrapper} class wraps the functionality of all CF types }{\pageref{classmlpack_1_1cf_1_1CFWrapper}}{}
\item\contentsline{section}{\textbf{ C\+F\+Wrapper\+Base} \\*Unified interface that can be used by the \doxyref{C\+F\+Model}{p.}{classmlpack_1_1cf_1_1CFModel} class to interact with all different CF types at runtime }{\pageref{classmlpack_1_1cf_1_1CFWrapperBase}}{}
\item\contentsline{section}{\textbf{ Combined\+Normalization$<$ Normalization\+Types $>$} \\*This normalization class performs a sequence of normalization methods on raw ratings }{\pageref{classmlpack_1_1cf_1_1CombinedNormalization}}{}
\item\contentsline{section}{\textbf{ Cosine\+Search} \\*Nearest neighbor search with cosine distance }{\pageref{classmlpack_1_1cf_1_1CosineSearch}}{}
\item\contentsline{section}{\textbf{ Dummy\+Class} \\*This class acts as a dummy class for passing as template parameter }{\pageref{classmlpack_1_1cf_1_1DummyClass}}{}
\item\contentsline{section}{\textbf{ Item\+Mean\+Normalization} \\*This normalization class performs item mean normalization on raw ratings }{\pageref{classmlpack_1_1cf_1_1ItemMeanNormalization}}{}
\item\contentsline{section}{\textbf{ L\+Metric\+Search$<$ T\+Power $>$} \\*Nearest neighbor search with L\+\_\+p distance }{\pageref{classmlpack_1_1cf_1_1LMetricSearch}}{}
\item\contentsline{section}{\textbf{ N\+M\+F\+Policy} \\*Implementation of the N\+MF policy to act as a wrapper when accessing N\+MF from within \doxyref{C\+F\+Type}{p.}{classmlpack_1_1cf_1_1CFType} }{\pageref{classmlpack_1_1cf_1_1NMFPolicy}}{}
\item\contentsline{section}{\textbf{ No\+Normalization} \\*This normalization class doesn\textquotesingle{}t perform any normalization }{\pageref{classmlpack_1_1cf_1_1NoNormalization}}{}
\item\contentsline{section}{\textbf{ Overall\+Mean\+Normalization} \\*This normalization class performs overall mean normalization on raw ratings }{\pageref{classmlpack_1_1cf_1_1OverallMeanNormalization}}{}
\item\contentsline{section}{\textbf{ Pearson\+Search} \\*Nearest neighbor search with pearson distance (or furthest neighbor search with pearson correlation) }{\pageref{classmlpack_1_1cf_1_1PearsonSearch}}{}
\item\contentsline{section}{\textbf{ Randomized\+S\+V\+D\+Policy} \\*Implementation of the Randomized S\+VD policy to act as a wrapper when accessing Randomized S\+VD from within \doxyref{C\+F\+Type}{p.}{classmlpack_1_1cf_1_1CFType} }{\pageref{classmlpack_1_1cf_1_1RandomizedSVDPolicy}}{}
\item\contentsline{section}{\textbf{ Regression\+Interpolation} \\*Implementation of regression-\/based interpolation method }{\pageref{classmlpack_1_1cf_1_1RegressionInterpolation}}{}
\item\contentsline{section}{\textbf{ Reg\+S\+V\+D\+Policy} \\*Implementation of the Regularized S\+VD policy to act as a wrapper when accessing Regularized S\+VD from within \doxyref{C\+F\+Type}{p.}{classmlpack_1_1cf_1_1CFType} }{\pageref{classmlpack_1_1cf_1_1RegSVDPolicy}}{}
\item\contentsline{section}{\textbf{ Similarity\+Interpolation} \\*With \doxyref{Similarity\+Interpolation}{p.}{classmlpack_1_1cf_1_1SimilarityInterpolation}, interpolation weights are based on similarities between query user and its neighbors }{\pageref{classmlpack_1_1cf_1_1SimilarityInterpolation}}{}
\item\contentsline{section}{\textbf{ S\+V\+D\+Complete\+Policy} \\*Implementation of the S\+VD complete incremental policy to act as a wrapper when accessing S\+VD complete decomposition from within \doxyref{C\+F\+Type}{p.}{classmlpack_1_1cf_1_1CFType} }{\pageref{classmlpack_1_1cf_1_1SVDCompletePolicy}}{}
\item\contentsline{section}{\textbf{ S\+V\+D\+Incomplete\+Policy} \\*Implementation of the S\+VD incomplete incremental to act as a wrapper when accessing S\+VD incomplete incremental from within \doxyref{C\+F\+Type}{p.}{classmlpack_1_1cf_1_1CFType} }{\pageref{classmlpack_1_1cf_1_1SVDIncompletePolicy}}{}
\item\contentsline{section}{\textbf{ S\+V\+D\+Plus\+Plus\+Policy} \\*Implementation of the S\+V\+D\+Plus\+Plus policy to act as a wrapper when accessing S\+V\+D\+Plus\+Plus from within \doxyref{C\+F\+Type}{p.}{classmlpack_1_1cf_1_1CFType} }{\pageref{classmlpack_1_1cf_1_1SVDPlusPlusPolicy}}{}
\item\contentsline{section}{\textbf{ S\+V\+D\+Wrapper$<$ Factorizer $>$} \\*This class acts as the wrapper for all S\+VD factorizers which are incompatible with CF module }{\pageref{classmlpack_1_1cf_1_1SVDWrapper}}{}
\item\contentsline{section}{\textbf{ User\+Mean\+Normalization} \\*This normalization class performs user mean normalization on raw ratings }{\pageref{classmlpack_1_1cf_1_1UserMeanNormalization}}{}
\item\contentsline{section}{\textbf{ Z\+Score\+Normalization} \\*This normalization class performs z-\/score normalization on raw ratings }{\pageref{classmlpack_1_1cf_1_1ZScoreNormalization}}{}
\item\contentsline{section}{\textbf{ Accuracy} \\*The \doxyref{Accuracy}{p.}{classmlpack_1_1cv_1_1Accuracy} is a metric of performance for classification algorithms that is equal to a proportion of correctly labeled test items among all ones for given test items }{\pageref{classmlpack_1_1cv_1_1Accuracy}}{}
\item\contentsline{section}{\textbf{ C\+V\+Base$<$ M\+L\+Algorithm, Mat\+Type, Predictions\+Type, Weights\+Type $>$} \\*An auxiliary class for cross-\/validation }{\pageref{classmlpack_1_1cv_1_1CVBase}}{}
\item\contentsline{section}{\textbf{ F1$<$ A\+S, Positive\+Class $>$} \\*\doxyref{F1}{p.}{classmlpack_1_1cv_1_1F1} is a metric of performance for classification algorithms that for binary classification is equal to $ 2 * precision * recall / (precision + recall) $ }{\pageref{classmlpack_1_1cv_1_1F1}}{}
\item\contentsline{section}{\textbf{ K\+Fold\+C\+V$<$ M\+L\+Algorithm, Metric, Mat\+Type, Predictions\+Type, Weights\+Type $>$} \\*The class \doxyref{K\+Fold\+CV}{p.}{classmlpack_1_1cv_1_1KFoldCV} implements k-\/fold cross-\/validation for regression and classification algorithms }{\pageref{classmlpack_1_1cv_1_1KFoldCV}}{}
\item\contentsline{section}{\textbf{ Meta\+Info\+Extractor$<$ M\+L\+Algorithm, M\+T, P\+T, W\+T $>$} \\*\doxyref{Meta\+Info\+Extractor}{p.}{classmlpack_1_1cv_1_1MetaInfoExtractor} is a tool for extracting meta information about a given machine learning algorithm }{\pageref{classmlpack_1_1cv_1_1MetaInfoExtractor}}{}
\item\contentsline{section}{\textbf{ M\+SE} \\*The Mean\+Squared\+Error is a metric of performance for regression algorithms that is equal to the mean squared error between predicted values and ground truth (correct) values for given test items }{\pageref{classmlpack_1_1cv_1_1MSE}}{}
\item\contentsline{section}{\textbf{ Not\+Found\+Method\+Form} }{\pageref{structmlpack_1_1cv_1_1NotFoundMethodForm}}{}
\item\contentsline{section}{\textbf{ Precision$<$ A\+S, Positive\+Class $>$} \\*\doxyref{Precision}{p.}{classmlpack_1_1cv_1_1Precision} is a metric of performance for classification algorithms that for binary classification is equal to $ tp / (tp + fp) $, where $ tp $ and $ fp $ are the numbers of true positives and false positives respectively }{\pageref{classmlpack_1_1cv_1_1Precision}}{}
\item\contentsline{section}{\textbf{ R2\+Score$<$ Adjusted\+R2 $>$} \\*The R2 Score is a metric of performance for regression algorithms that represents the proportion of variance (here y) that has been explained by the independent variables in the model }{\pageref{classmlpack_1_1cv_1_1R2Score}}{}
\item\contentsline{section}{\textbf{ Recall$<$ A\+S, Positive\+Class $>$} \\*\doxyref{Recall}{p.}{classmlpack_1_1cv_1_1Recall} is a metric of performance for classification algorithms that for binary classification is equal to $ tp / (tp + fn) $, where $ tp $ and $ fn $ are the numbers of true positives and false negatives respectively }{\pageref{classmlpack_1_1cv_1_1Recall}}{}
\item\contentsline{section}{\textbf{ Select\+Method\+Form$<$ M\+L\+Algorithm, H\+M\+Fs $>$} \\*A type function that selects a right method form }{\pageref{structmlpack_1_1cv_1_1SelectMethodForm}}{}
\item\contentsline{section}{\textbf{ Select\+Method\+Form$<$ M\+L\+Algorithm $>$} }{\pageref{structmlpack_1_1cv_1_1SelectMethodForm_3_01MLAlgorithm_01_4}}{}
\item\contentsline{section}{\textbf{ Select\+Method\+Form$<$ M\+L\+Algorithm $>$\+::\+From$<$ Forms $>$} }{\pageref{structmlpack_1_1cv_1_1SelectMethodForm_3_01MLAlgorithm_01_4_1_1From}}{}
\item\contentsline{section}{\textbf{ Select\+Method\+Form$<$ M\+L\+Algorithm, Has\+Method\+Form, H\+M\+Fs... $>$} }{\pageref{structmlpack_1_1cv_1_1SelectMethodForm_3_01MLAlgorithm_00_01HasMethodForm_00_01HMFs_8_8_8_01_4}}{}
\item\contentsline{section}{\textbf{ Select\+Method\+Form$<$ M\+L\+Algorithm, Has\+Method\+Form, H\+M\+Fs... $>$\+::\+From$<$ Forms $>$} }{\pageref{classmlpack_1_1cv_1_1SelectMethodForm_3_01MLAlgorithm_00_01HasMethodForm_00_01HMFs_8_8_8_01_4_1_1From}}{}
\item\contentsline{section}{\textbf{ Silhouette\+Score} \\*The Silhouette Score is a metric of performance for clustering that represents the quality of clusters made as a result }{\pageref{classmlpack_1_1cv_1_1SilhouetteScore}}{}
\item\contentsline{section}{\textbf{ Simple\+C\+V$<$ M\+L\+Algorithm, Metric, Mat\+Type, Predictions\+Type, Weights\+Type $>$} \\*\doxyref{Simple\+CV}{p.}{classmlpack_1_1cv_1_1SimpleCV} splits data into two sets -\/ training and validation sets -\/ and then runs training on the training set and evaluates performance on the validation set }{\pageref{classmlpack_1_1cv_1_1SimpleCV}}{}
\item\contentsline{section}{\textbf{ Train\+Form$<$ Mat\+Type, Predictions\+Type, Weights\+Type, Dataset\+Info, Num\+Classes $>$} \\*A wrapper struct for holding a Train form }{\pageref{structmlpack_1_1cv_1_1TrainForm}}{}
\item\contentsline{section}{\textbf{ Train\+Form$<$ M\+T, P\+T, void, false, false $>$} }{\pageref{structmlpack_1_1cv_1_1TrainForm_3_01MT_00_01PT_00_01void_00_01false_00_01false_01_4}}{}
\item\contentsline{section}{\textbf{ Train\+Form$<$ M\+T, P\+T, void, false, true $>$} }{\pageref{structmlpack_1_1cv_1_1TrainForm_3_01MT_00_01PT_00_01void_00_01false_00_01true_01_4}}{}
\item\contentsline{section}{\textbf{ Train\+Form$<$ M\+T, P\+T, void, true, false $>$} }{\pageref{structmlpack_1_1cv_1_1TrainForm_3_01MT_00_01PT_00_01void_00_01true_00_01false_01_4}}{}
\item\contentsline{section}{\textbf{ Train\+Form$<$ M\+T, P\+T, void, true, true $>$} }{\pageref{structmlpack_1_1cv_1_1TrainForm_3_01MT_00_01PT_00_01void_00_01true_00_01true_01_4}}{}
\item\contentsline{section}{\textbf{ Train\+Form$<$ M\+T, P\+T, W\+T, false, false $>$} }{\pageref{structmlpack_1_1cv_1_1TrainForm_3_01MT_00_01PT_00_01WT_00_01false_00_01false_01_4}}{}
\item\contentsline{section}{\textbf{ Train\+Form$<$ M\+T, P\+T, W\+T, false, true $>$} }{\pageref{structmlpack_1_1cv_1_1TrainForm_3_01MT_00_01PT_00_01WT_00_01false_00_01true_01_4}}{}
\item\contentsline{section}{\textbf{ Train\+Form$<$ M\+T, P\+T, W\+T, true, false $>$} }{\pageref{structmlpack_1_1cv_1_1TrainForm_3_01MT_00_01PT_00_01WT_00_01true_00_01false_01_4}}{}
\item\contentsline{section}{\textbf{ Train\+Form$<$ M\+T, P\+T, W\+T, true, true $>$} }{\pageref{structmlpack_1_1cv_1_1TrainForm_3_01MT_00_01PT_00_01WT_00_01true_00_01true_01_4}}{}
\item\contentsline{section}{\textbf{ Train\+Form\+Base4$<$ P\+T, W\+T, T1, T2 $>$} }{\pageref{structmlpack_1_1cv_1_1TrainFormBase4}}{}
\item\contentsline{section}{\textbf{ Train\+Form\+Base5$<$ P\+T, W\+T, T1, T2, T3 $>$} }{\pageref{structmlpack_1_1cv_1_1TrainFormBase5}}{}
\item\contentsline{section}{\textbf{ Train\+Form\+Base6$<$ P\+T, W\+T, T1, T2, T3, T4 $>$} }{\pageref{structmlpack_1_1cv_1_1TrainFormBase6}}{}
\item\contentsline{section}{\textbf{ Train\+Form\+Base7$<$ P\+T, W\+T, T1, T2, T3, T4, T5 $>$} }{\pageref{structmlpack_1_1cv_1_1TrainFormBase7}}{}
\item\contentsline{section}{\textbf{ Bag\+Of\+Words\+Encoding\+Policy} \\*Definition of the \doxyref{Bag\+Of\+Words\+Encoding\+Policy}{p.}{classmlpack_1_1data_1_1BagOfWordsEncodingPolicy} class }{\pageref{classmlpack_1_1data_1_1BagOfWordsEncodingPolicy}}{}
\item\contentsline{section}{\textbf{ Char\+Extract} \\*The class is used to split a string into characters }{\pageref{classmlpack_1_1data_1_1CharExtract}}{}
\item\contentsline{section}{\textbf{ Custom\+Imputation$<$ T $>$} \\*A simple custom imputation class }{\pageref{classmlpack_1_1data_1_1CustomImputation}}{}
\item\contentsline{section}{\textbf{ Dataset\+Mapper$<$ Policy\+Type, Input\+Type $>$} \\*Auxiliary information for a dataset, including mappings to/from strings (or other types) and the datatype of each dimension }{\pageref{classmlpack_1_1data_1_1DatasetMapper}}{}
\item\contentsline{section}{\textbf{ Dictionary\+Encoding\+Policy} \\*Dicitonary\+Enocding\+Policy is used as a helper class for \doxyref{String\+Encoding}{p.}{classmlpack_1_1data_1_1StringEncoding} }{\pageref{classmlpack_1_1data_1_1DictionaryEncodingPolicy}}{}
\item\contentsline{section}{\textbf{ Has\+Serialize$<$ T $>$} }{\pageref{structmlpack_1_1data_1_1HasSerialize}}{}
\item\contentsline{section}{\textbf{ Has\+Serialize$<$ T $>$\+::check$<$ U, V, W $>$} }{\pageref{structmlpack_1_1data_1_1HasSerialize_1_1check}}{}
\item\contentsline{section}{\textbf{ Has\+Serialize\+Function$<$ T $>$} }{\pageref{structmlpack_1_1data_1_1HasSerializeFunction}}{}
\item\contentsline{section}{\textbf{ Image\+Info} \\*Implements meta-\/data of images required by \doxyref{data\+::\+Load}{p.}{namespacemlpack_1_1data_abbff2a667bf247e00b1fc09b7ca5f831} and \doxyref{data\+::\+Save}{p.}{namespacemlpack_1_1data_accd1605a1d160c09ee75c93a587dc313} for loading and saving images into arma\+::\+Mat }{\pageref{classmlpack_1_1data_1_1ImageInfo}}{}
\item\contentsline{section}{\textbf{ Imputer$<$ T, Mapper\+Type, Strategy\+Type $>$} \\*Given a dataset of a particular datatype, replace user-\/specified missing value with a variable dependent on the Strategy\+Type and Mapper\+Type }{\pageref{classmlpack_1_1data_1_1Imputer}}{}
\item\contentsline{section}{\textbf{ Increment\+Policy} \\*\doxyref{Increment\+Policy}{p.}{classmlpack_1_1data_1_1IncrementPolicy} is used as a helper class for \doxyref{Dataset\+Mapper}{p.}{classmlpack_1_1data_1_1DatasetMapper} }{\pageref{classmlpack_1_1data_1_1IncrementPolicy}}{}
\item\contentsline{section}{\textbf{ Listwise\+Deletion$<$ T $>$} \\*A complete-\/case analysis to remove the values containing mapped\+Value }{\pageref{classmlpack_1_1data_1_1ListwiseDeletion}}{}
\item\contentsline{section}{\textbf{ Load\+C\+SV} \\*Load the csv file.\+This class use boost\+::spirit to implement the parser, please refer to following link {\tt http\+://theboostcpplibraries.\+com/boost.\+spirit} for quick review }{\pageref{classmlpack_1_1data_1_1LoadCSV}}{}
\item\contentsline{section}{\textbf{ Max\+Abs\+Scaler} \\*A simple Max\+Abs Scaler class }{\pageref{classmlpack_1_1data_1_1MaxAbsScaler}}{}
\item\contentsline{section}{\textbf{ Mean\+Imputation$<$ T $>$} \\*A simple mean imputation class }{\pageref{classmlpack_1_1data_1_1MeanImputation}}{}
\item\contentsline{section}{\textbf{ Mean\+Normalization} \\*A simple Mean Normalization class }{\pageref{classmlpack_1_1data_1_1MeanNormalization}}{}
\item\contentsline{section}{\textbf{ Median\+Imputation$<$ T $>$} \\*This is a class implementation of simple median imputation }{\pageref{classmlpack_1_1data_1_1MedianImputation}}{}
\item\contentsline{section}{\textbf{ Min\+Max\+Scaler} \\*A simple Min\+Max Scaler class }{\pageref{classmlpack_1_1data_1_1MinMaxScaler}}{}
\item\contentsline{section}{\textbf{ Missing\+Policy} \\*\doxyref{Missing\+Policy}{p.}{classmlpack_1_1data_1_1MissingPolicy} is used as a helper class for \doxyref{Dataset\+Mapper}{p.}{classmlpack_1_1data_1_1DatasetMapper} }{\pageref{classmlpack_1_1data_1_1MissingPolicy}}{}
\item\contentsline{section}{\textbf{ P\+C\+A\+Whitening} \\*A simple \doxyref{P\+C\+A\+Whitening}{p.}{classmlpack_1_1data_1_1PCAWhitening} class }{\pageref{classmlpack_1_1data_1_1PCAWhitening}}{}
\item\contentsline{section}{\textbf{ Scaling\+Model} \\*The model to save to disk }{\pageref{classmlpack_1_1data_1_1ScalingModel}}{}
\item\contentsline{section}{\textbf{ Split\+By\+Any\+Of} \\*Tokenizes a string using a set of delimiters }{\pageref{classmlpack_1_1data_1_1SplitByAnyOf}}{}
\item\contentsline{section}{\textbf{ Standard\+Scaler} \\*A simple Standard Scaler class }{\pageref{classmlpack_1_1data_1_1StandardScaler}}{}
\item\contentsline{section}{\textbf{ String\+Encoding$<$ Encoding\+Policy\+Type, Dictionary\+Type $>$} \\*The class translates a set of strings into numbers using various encoding algorithms }{\pageref{classmlpack_1_1data_1_1StringEncoding}}{}
\item\contentsline{section}{\textbf{ String\+Encoding\+Dictionary$<$ Token $>$} \\*This class provides a dictionary interface for the purpose of string encoding }{\pageref{classmlpack_1_1data_1_1StringEncodingDictionary}}{}
\item\contentsline{section}{\textbf{ String\+Encoding\+Dictionary$<$ boost\+::string\+\_\+view $>$} }{\pageref{classmlpack_1_1data_1_1StringEncodingDictionary_3_01boost_1_1string__view_01_4}}{}
\item\contentsline{section}{\textbf{ String\+Encoding\+Dictionary$<$ int $>$} }{\pageref{classmlpack_1_1data_1_1StringEncodingDictionary_3_01int_01_4}}{}
\item\contentsline{section}{\textbf{ String\+Encoding\+Policy\+Traits$<$ Policy\+Type $>$} \\*This is a template struct that provides some information about various encoding policies }{\pageref{structmlpack_1_1data_1_1StringEncodingPolicyTraits}}{}
\item\contentsline{section}{\textbf{ String\+Encoding\+Policy\+Traits$<$ Dictionary\+Encoding\+Policy $>$} \\*The specialization provides some information about the dictionary encoding policy }{\pageref{structmlpack_1_1data_1_1StringEncodingPolicyTraits_3_01DictionaryEncodingPolicy_01_4}}{}
\item\contentsline{section}{\textbf{ Tf\+Idf\+Encoding\+Policy} \\*Definition of the \doxyref{Tf\+Idf\+Encoding\+Policy}{p.}{classmlpack_1_1data_1_1TfIdfEncodingPolicy} class }{\pageref{classmlpack_1_1data_1_1TfIdfEncodingPolicy}}{}
\item\contentsline{section}{\textbf{ Z\+C\+A\+Whitening} \\*A simple \doxyref{Z\+C\+A\+Whitening}{p.}{classmlpack_1_1data_1_1ZCAWhitening} class }{\pageref{classmlpack_1_1data_1_1ZCAWhitening}}{}
\item\contentsline{section}{\textbf{ D\+B\+S\+C\+A\+N$<$ Range\+Search\+Type, Point\+Selection\+Policy $>$} \\*\doxyref{D\+B\+S\+C\+AN}{p.}{classmlpack_1_1dbscan_1_1DBSCAN} (Density-\/\+Based Spatial Clustering of Applications with Noise) is a clustering technique described in the following paper\+: }{\pageref{classmlpack_1_1dbscan_1_1DBSCAN}}{}
\item\contentsline{section}{\textbf{ Ordered\+Point\+Selection} \\*This class can be used to sequentially select the next point to use for \doxyref{D\+B\+S\+C\+AN}{p.}{classmlpack_1_1dbscan_1_1DBSCAN} }{\pageref{classmlpack_1_1dbscan_1_1OrderedPointSelection}}{}
\item\contentsline{section}{\textbf{ Random\+Point\+Selection} \\*This class can be used to randomly select the next point to use for \doxyref{D\+B\+S\+C\+AN}{p.}{classmlpack_1_1dbscan_1_1DBSCAN} }{\pageref{classmlpack_1_1dbscan_1_1RandomPointSelection}}{}
\item\contentsline{section}{\textbf{ D\+Tree$<$ Mat\+Type, Tag\+Type $>$} \\*A density estimation tree is similar to both a decision tree and a space partitioning tree (like a kd-\/tree) }{\pageref{classmlpack_1_1det_1_1DTree}}{}
\item\contentsline{section}{\textbf{ Path\+Cacher} \\*This class is responsible for caching the path to each node of the tree }{\pageref{classmlpack_1_1det_1_1PathCacher}}{}
\item\contentsline{section}{\textbf{ Diagonal\+Gaussian\+Distribution} \\*A single multivariate Gaussian distribution with diagonal covariance }{\pageref{classmlpack_1_1distribution_1_1DiagonalGaussianDistribution}}{}
\item\contentsline{section}{\textbf{ Discrete\+Distribution} \\*A discrete distribution where the only observations are discrete observations }{\pageref{classmlpack_1_1distribution_1_1DiscreteDistribution}}{}
\item\contentsline{section}{\textbf{ Gamma\+Distribution} \\*This class represents the Gamma distribution }{\pageref{classmlpack_1_1distribution_1_1GammaDistribution}}{}
\item\contentsline{section}{\textbf{ Gaussian\+Distribution} \\*A single multivariate Gaussian distribution }{\pageref{classmlpack_1_1distribution_1_1GaussianDistribution}}{}
\item\contentsline{section}{\textbf{ Laplace\+Distribution} \\*The multivariate Laplace distribution centered at 0 has pdf }{\pageref{classmlpack_1_1distribution_1_1LaplaceDistribution}}{}
\item\contentsline{section}{\textbf{ D\+T\+B\+Rules$<$ Metric\+Type, Tree\+Type $>$} }{\pageref{classmlpack_1_1emst_1_1DTBRules}}{}
\item\contentsline{section}{\textbf{ D\+T\+B\+Stat} \\*A statistic for use with mlpack trees, which stores the upper bound on distance to nearest neighbors and the component which this node belongs to }{\pageref{classmlpack_1_1emst_1_1DTBStat}}{}
\item\contentsline{section}{\textbf{ Dual\+Tree\+Boruvka$<$ Metric\+Type, Mat\+Type, Tree\+Type $>$} \\*Performs the M\+ST calculation using the Dual-\/\+Tree Boruvka algorithm, using any type of tree }{\pageref{classmlpack_1_1emst_1_1DualTreeBoruvka}}{}
\item\contentsline{section}{\textbf{ Edge\+Pair} \\*An edge pair is simply two indices and a distance }{\pageref{classmlpack_1_1emst_1_1EdgePair}}{}
\item\contentsline{section}{\textbf{ Union\+Find} \\*A Union-\/\+Find data structure }{\pageref{classmlpack_1_1emst_1_1UnionFind}}{}
\item\contentsline{section}{\textbf{ Fast\+M\+K\+S$<$ Kernel\+Type, Mat\+Type, Tree\+Type $>$} \\*An implementation of fast exact max-\/kernel search }{\pageref{classmlpack_1_1fastmks_1_1FastMKS}}{}
\item\contentsline{section}{\textbf{ Fast\+M\+K\+S\+Model} \\*A utility struct to contain all the possible \doxyref{Fast\+M\+KS}{p.}{classmlpack_1_1fastmks_1_1FastMKS} models, for use by the mlpack\+\_\+fastmks program }{\pageref{classmlpack_1_1fastmks_1_1FastMKSModel}}{}
\item\contentsline{section}{\textbf{ Fast\+M\+K\+S\+Rules$<$ Kernel\+Type, Tree\+Type $>$} \\*The \doxyref{Fast\+M\+K\+S\+Rules}{p.}{classmlpack_1_1fastmks_1_1FastMKSRules} class is a template helper class used by \doxyref{Fast\+M\+KS}{p.}{classmlpack_1_1fastmks_1_1FastMKS} class when performing exact max-\/kernel search }{\pageref{classmlpack_1_1fastmks_1_1FastMKSRules}}{}
\item\contentsline{section}{\textbf{ Fast\+M\+K\+S\+Stat} \\*The statistic used in trees with \doxyref{Fast\+M\+KS}{p.}{classmlpack_1_1fastmks_1_1FastMKS} }{\pageref{classmlpack_1_1fastmks_1_1FastMKSStat}}{}
\item\contentsline{section}{\textbf{ Diagonal\+Constraint} \\*Force a covariance matrix to be diagonal }{\pageref{classmlpack_1_1gmm_1_1DiagonalConstraint}}{}
\item\contentsline{section}{\textbf{ Diagonal\+G\+MM} \\*A Diagonal Gaussian Mixture Model }{\pageref{classmlpack_1_1gmm_1_1DiagonalGMM}}{}
\item\contentsline{section}{\textbf{ Eigenvalue\+Ratio\+Constraint} \\*Given a vector of eigenvalue ratios, ensure that the covariance matrix always has those eigenvalue ratios }{\pageref{classmlpack_1_1gmm_1_1EigenvalueRatioConstraint}}{}
\item\contentsline{section}{\textbf{ E\+M\+Fit$<$ Initial\+Clustering\+Type, Covariance\+Constraint\+Policy, Distribution $>$} \\*This class contains methods which can fit a \doxyref{G\+MM}{p.}{classmlpack_1_1gmm_1_1GMM} to observations using the EM algorithm }{\pageref{classmlpack_1_1gmm_1_1EMFit}}{}
\item\contentsline{section}{\textbf{ G\+MM} \\*A Gaussian Mixture Model (\doxyref{G\+MM}{p.}{classmlpack_1_1gmm_1_1GMM}) }{\pageref{classmlpack_1_1gmm_1_1GMM}}{}
\item\contentsline{section}{\textbf{ No\+Constraint} \\*This class enforces no constraint on the covariance matrix }{\pageref{classmlpack_1_1gmm_1_1NoConstraint}}{}
\item\contentsline{section}{\textbf{ Positive\+Definite\+Constraint} \\*Given a covariance matrix, force the matrix to be positive definite }{\pageref{classmlpack_1_1gmm_1_1PositiveDefiniteConstraint}}{}
\item\contentsline{section}{\textbf{ H\+M\+M$<$ Distribution $>$} \\*A class that represents a Hidden Markov Model with an arbitrary type of emission distribution }{\pageref{classmlpack_1_1hmm_1_1HMM}}{}
\item\contentsline{section}{\textbf{ H\+M\+M\+Model} \\*A serializable \doxyref{H\+MM}{p.}{classmlpack_1_1hmm_1_1HMM} model that also stores the type }{\pageref{classmlpack_1_1hmm_1_1HMMModel}}{}
\item\contentsline{section}{\textbf{ H\+M\+M\+Regression} \\*A class that represents a Hidden Markov Model Regression (H\+M\+MR) }{\pageref{classmlpack_1_1hmm_1_1HMMRegression}}{}
\item\contentsline{section}{\textbf{ C\+V\+Function$<$ C\+V\+Type, M\+L\+Algorithm, Total\+Args, Bound\+Args $>$} \\*This wrapper serves for adapting the interface of the cross-\/validation classes to the one that can be utilized by the mlpack optimizers }{\pageref{classmlpack_1_1hpt_1_1CVFunction}}{}
\item\contentsline{section}{\textbf{ Deduce\+Hyper\+Parameter\+Types$<$ Args $>$} \\*A type function for deducing types of hyper-\/parameters from types of arguments in the Optimize method in \doxyref{Hyper\+Parameter\+Tuner}{p.}{classmlpack_1_1hpt_1_1HyperParameterTuner} }{\pageref{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes}}{}
\item\contentsline{section}{\textbf{ Deduce\+Hyper\+Parameter\+Types$<$ Args $>$\+::\+Result\+Holder$<$ H\+P\+Types $>$} }{\pageref{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes_1_1ResultHolder}}{}
\item\contentsline{section}{\textbf{ Deduce\+Hyper\+Parameter\+Types$<$ Pre\+Fixed\+Arg$<$ T $>$, Args... $>$} \\*Defining \doxyref{Deduce\+Hyper\+Parameter\+Types}{p.}{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes} for the case when not all argument types have been processed, and the next one is the type of an argument that should be fixed }{\pageref{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes_3_01PreFixedArg_3_01T_01_4_00_01Args_8_8_8_01_4}}{}
\item\contentsline{section}{\textbf{ Deduce\+Hyper\+Parameter\+Types$<$ Pre\+Fixed\+Arg$<$ T $>$, Args... $>$\+::\+Result\+Holder$<$ H\+P\+Types $>$} }{\pageref{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes_3_01PreFixedArg_3_01T_01_4_00_01Args_8_8_8_01_4_1_1ResultHolder}}{}
\item\contentsline{section}{\textbf{ Deduce\+Hyper\+Parameter\+Types$<$ T, Args... $>$} \\*Defining \doxyref{Deduce\+Hyper\+Parameter\+Types}{p.}{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes} for the case when not all argument types have been processed, and the next one (T) is a collection type or an arithmetic type }{\pageref{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes_3_01T_00_01Args_8_8_8_01_4}}{}
\item\contentsline{section}{\textbf{ Deduce\+Hyper\+Parameter\+Types$<$ T, Args... $>$\+::\+Is\+Collection\+Type$<$ Type $>$} \\*A type function to check whether Type is a collection type (for that it should define value\+\_\+type) }{\pageref{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes_3_01T_00_01Args_8_8_8_01_4_1_1IsCollectionType}}{}
\item\contentsline{section}{\textbf{ Deduce\+Hyper\+Parameter\+Types$<$ T, Args... $>$\+::\+Result\+Holder$<$ H\+P\+Types $>$} }{\pageref{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes_3_01T_00_01Args_8_8_8_01_4_1_1ResultHolder}}{}
\item\contentsline{section}{\textbf{ Deduce\+Hyper\+Parameter\+Types$<$ T, Args... $>$\+::\+Result\+H\+P\+Type$<$ Argument\+Type, Is\+Arithmetic $>$} \\*A type function to deduce the result hyper-\/parameter type for Argument\+Type }{\pageref{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes_3_01T_00_01Args_8_8_8_01_4_1_1ResultHPType}}{}
\item\contentsline{section}{\textbf{ Deduce\+Hyper\+Parameter\+Types$<$ T, Args... $>$\+::\+Result\+H\+P\+Type$<$ Arithmetic\+Type, true $>$} }{\pageref{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes_3_01T_00_01Args_8_8_8_01_4_1_1ResultHPType_3_01ArithmeticType_00_01true_01_4}}{}
\item\contentsline{section}{\textbf{ Deduce\+Hyper\+Parameter\+Types$<$ T, Args... $>$\+::\+Result\+H\+P\+Type$<$ Collection\+Type, false $>$} }{\pageref{structmlpack_1_1hpt_1_1DeduceHyperParameterTypes_3_01T_00_01Args_8_8_8_01_4_1_1ResultHPType_3_01CollectionType_00_01false_01_4}}{}
\item\contentsline{section}{\textbf{ Fixed\+Arg$<$ T, I $>$} \\*A struct for storing information about a fixed argument }{\pageref{structmlpack_1_1hpt_1_1FixedArg}}{}
\item\contentsline{section}{\textbf{ Hyper\+Parameter\+Tuner$<$ M\+L\+Algorithm, Metric, C\+V, Optimizer\+Type, Mat\+Type, Predictions\+Type, Weights\+Type $>$} \\*The class \doxyref{Hyper\+Parameter\+Tuner}{p.}{classmlpack_1_1hpt_1_1HyperParameterTuner} for the given M\+L\+Algorithm utilizes the provided Optimizer to find the values of hyper-\/parameters that optimize the value of the given Metric }{\pageref{classmlpack_1_1hpt_1_1HyperParameterTuner}}{}
\item\contentsline{section}{\textbf{ Is\+Pre\+Fixed\+Arg$<$ T $>$} \\*A type function for checking whether the given type is \doxyref{Pre\+Fixed\+Arg}{p.}{structmlpack_1_1hpt_1_1PreFixedArg} }{\pageref{classmlpack_1_1hpt_1_1IsPreFixedArg}}{}
\item\contentsline{section}{\textbf{ Pre\+Fixed\+Arg$<$ T $>$} \\*A struct for marking arguments as ones that should be fixed (it can be useful for the Optimize method of \doxyref{Hyper\+Parameter\+Tuner}{p.}{classmlpack_1_1hpt_1_1HyperParameterTuner}) }{\pageref{structmlpack_1_1hpt_1_1PreFixedArg}}{}
\item\contentsline{section}{\textbf{ Pre\+Fixed\+Arg$<$ T \& $>$} \\*The specialization of the template for references }{\pageref{structmlpack_1_1hpt_1_1PreFixedArg_3_01T_01_6_01_4}}{}
\item\contentsline{section}{\textbf{ IO} \\*Parses the command line for parameters and holds user-\/specified parameters }{\pageref{classmlpack_1_1IO}}{}
\item\contentsline{section}{\textbf{ K\+D\+E$<$ Kernel\+Type, Metric\+Type, Mat\+Type, Tree\+Type, Dual\+Tree\+Traversal\+Type, Single\+Tree\+Traversal\+Type $>$} \\*The \doxyref{K\+DE}{p.}{classmlpack_1_1kde_1_1KDE} class is a template class for performing Kernel Density Estimations }{\pageref{classmlpack_1_1kde_1_1KDE}}{}
\item\contentsline{section}{\textbf{ K\+D\+E\+Clean\+Rules$<$ Tree\+Type $>$} \\*A dual-\/tree traversal Rules class for cleaning used trees before performing kernel density estimation }{\pageref{classmlpack_1_1kde_1_1KDECleanRules}}{}
\item\contentsline{section}{\textbf{ K\+D\+E\+Default\+Params} \\*\doxyref{K\+D\+E\+Default\+Params}{p.}{structmlpack_1_1kde_1_1KDEDefaultParams} contains the default input parameter values for \doxyref{K\+DE}{p.}{classmlpack_1_1kde_1_1KDE} }{\pageref{structmlpack_1_1kde_1_1KDEDefaultParams}}{}
\item\contentsline{section}{\textbf{ K\+D\+E\+Model} \\*The \doxyref{K\+D\+E\+Model}{p.}{classmlpack_1_1kde_1_1KDEModel} provides an abstraction for the \doxyref{K\+DE}{p.}{classmlpack_1_1kde_1_1KDE} class, abstracting away the Kernel\+Type and Tree\+Type parameters and allowing those to be specified at runtime }{\pageref{classmlpack_1_1kde_1_1KDEModel}}{}
\item\contentsline{section}{\textbf{ K\+D\+E\+Rules$<$ Metric\+Type, Kernel\+Type, Tree\+Type $>$} \\*A dual-\/tree traversal Rules class for kernel density estimation }{\pageref{classmlpack_1_1kde_1_1KDERules}}{}
\item\contentsline{section}{\textbf{ K\+D\+E\+Stat} \\*Extra data for each node in the tree for the task of kernel density estimation }{\pageref{classmlpack_1_1kde_1_1KDEStat}}{}
\item\contentsline{section}{\textbf{ K\+D\+E\+Wrapper$<$ Kernel\+Type, Tree\+Type $>$} \\*\doxyref{K\+D\+E\+Wrapper}{p.}{classmlpack_1_1kde_1_1KDEWrapper} is a wrapper class for all \doxyref{K\+DE}{p.}{classmlpack_1_1kde_1_1KDE} types supported by \doxyref{K\+D\+E\+Model}{p.}{classmlpack_1_1kde_1_1KDEModel} }{\pageref{classmlpack_1_1kde_1_1KDEWrapper}}{}
\item\contentsline{section}{\textbf{ K\+D\+E\+Wrapper\+Base} \\*\doxyref{K\+D\+E\+Wrapper\+Base}{p.}{classmlpack_1_1kde_1_1KDEWrapperBase} is a base wrapper class for holding all \doxyref{K\+DE}{p.}{classmlpack_1_1kde_1_1KDE} types supported by \doxyref{K\+D\+E\+Model}{p.}{classmlpack_1_1kde_1_1KDEModel} }{\pageref{classmlpack_1_1kde_1_1KDEWrapperBase}}{}
\item\contentsline{section}{\textbf{ Kernel\+Normalizer} \\*\doxyref{Kernel\+Normalizer}{p.}{classmlpack_1_1kde_1_1KernelNormalizer} holds a set of methods to normalize estimations applying in each case the appropiate kernel normalizer function }{\pageref{classmlpack_1_1kde_1_1KernelNormalizer}}{}
\item\contentsline{section}{\textbf{ Cauchy\+Kernel} \\*The Cauchy kernel }{\pageref{classmlpack_1_1kernel_1_1CauchyKernel}}{}
\item\contentsline{section}{\textbf{ Cosine\+Distance} \\*The cosine distance (or cosine similarity) }{\pageref{classmlpack_1_1kernel_1_1CosineDistance}}{}
\item\contentsline{section}{\textbf{ Epanechnikov\+Kernel} \\*The Epanechnikov kernel, defined as }{\pageref{classmlpack_1_1kernel_1_1EpanechnikovKernel}}{}
\item\contentsline{section}{\textbf{ Example\+Kernel} \\*An example kernel function }{\pageref{classmlpack_1_1kernel_1_1ExampleKernel}}{}
\item\contentsline{section}{\textbf{ Gaussian\+Kernel} \\*The standard Gaussian kernel }{\pageref{classmlpack_1_1kernel_1_1GaussianKernel}}{}
\item\contentsline{section}{\textbf{ Hyperbolic\+Tangent\+Kernel} \\*Hyperbolic tangent kernel }{\pageref{classmlpack_1_1kernel_1_1HyperbolicTangentKernel}}{}
\item\contentsline{section}{\textbf{ Kernel\+Traits$<$ Kernel\+Type $>$} \\*This is a template class that can provide information about various kernels }{\pageref{classmlpack_1_1kernel_1_1KernelTraits}}{}
\item\contentsline{section}{\textbf{ Kernel\+Traits$<$ Cauchy\+Kernel $>$} \\*Kernel traits for the Cauchy kernel }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01CauchyKernel_01_4}}{}
\item\contentsline{section}{\textbf{ Kernel\+Traits$<$ Cosine\+Distance $>$} \\*Kernel traits for the cosine distance }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01CosineDistance_01_4}}{}
\item\contentsline{section}{\textbf{ Kernel\+Traits$<$ Epanechnikov\+Kernel $>$} \\*Kernel traits for the Epanechnikov kernel }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01EpanechnikovKernel_01_4}}{}
\item\contentsline{section}{\textbf{ Kernel\+Traits$<$ Gaussian\+Kernel $>$} \\*Kernel traits for the Gaussian kernel }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01GaussianKernel_01_4}}{}
\item\contentsline{section}{\textbf{ Kernel\+Traits$<$ Laplacian\+Kernel $>$} \\*Kernel traits of the Laplacian kernel }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01LaplacianKernel_01_4}}{}
\item\contentsline{section}{\textbf{ Kernel\+Traits$<$ Spherical\+Kernel $>$} \\*Kernel traits for the spherical kernel }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01SphericalKernel_01_4}}{}
\item\contentsline{section}{\textbf{ Kernel\+Traits$<$ Triangular\+Kernel $>$} \\*Kernel traits for the triangular kernel }{\pageref{classmlpack_1_1kernel_1_1KernelTraits_3_01TriangularKernel_01_4}}{}
\item\contentsline{section}{\textbf{ K\+Means\+Selection$<$ Clustering\+Type, max\+Iterations $>$} \\*Implementation of the kmeans sampling scheme }{\pageref{classmlpack_1_1kernel_1_1KMeansSelection}}{}
\item\contentsline{section}{\textbf{ Laplacian\+Kernel} \\*The standard Laplacian kernel }{\pageref{classmlpack_1_1kernel_1_1LaplacianKernel}}{}
\item\contentsline{section}{\textbf{ Linear\+Kernel} \\*The simple linear kernel (dot product) }{\pageref{classmlpack_1_1kernel_1_1LinearKernel}}{}
\item\contentsline{section}{\textbf{ Nystroem\+Method$<$ Kernel\+Type, Point\+Selection\+Policy $>$} }{\pageref{classmlpack_1_1kernel_1_1NystroemMethod}}{}
\item\contentsline{section}{\textbf{ Ordered\+Selection} }{\pageref{classmlpack_1_1kernel_1_1OrderedSelection}}{}
\item\contentsline{section}{\textbf{ Polynomial\+Kernel} \\*The simple polynomial kernel }{\pageref{classmlpack_1_1kernel_1_1PolynomialKernel}}{}
\item\contentsline{section}{\textbf{ P\+Spectrum\+String\+Kernel} \\*The p-\/spectrum string kernel }{\pageref{classmlpack_1_1kernel_1_1PSpectrumStringKernel}}{}
\item\contentsline{section}{\textbf{ Random\+Selection} }{\pageref{classmlpack_1_1kernel_1_1RandomSelection}}{}
\item\contentsline{section}{\textbf{ Spherical\+Kernel} \\*The spherical kernel, which is 1 when the distance between the two argument points is less than or equal to the bandwidth, or 0 otherwise }{\pageref{classmlpack_1_1kernel_1_1SphericalKernel}}{}
\item\contentsline{section}{\textbf{ Triangular\+Kernel} \\*The trivially simple triangular kernel, defined by }{\pageref{classmlpack_1_1kernel_1_1TriangularKernel}}{}
\item\contentsline{section}{\textbf{ Dual\+Tree\+K\+Means$<$ Metric\+Type, Mat\+Type, Tree\+Type $>$} \\*An algorithm for an exact Lloyd iteration which simply uses dual-\/tree nearest-\/neighbor search to find the nearest centroid for each point in the dataset }{\pageref{classmlpack_1_1kmeans_1_1DualTreeKMeans}}{}
\item\contentsline{section}{\textbf{ Dual\+Tree\+K\+Means\+Rules$<$ Metric\+Type, Tree\+Type $>$} }{\pageref{classmlpack_1_1kmeans_1_1DualTreeKMeansRules}}{}
\item\contentsline{section}{\textbf{ Dual\+Tree\+K\+Means\+Statistic} }{\pageref{classmlpack_1_1kmeans_1_1DualTreeKMeansStatistic}}{}
\item\contentsline{section}{\textbf{ Elkan\+K\+Means$<$ Metric\+Type, Mat\+Type $>$} }{\pageref{classmlpack_1_1kmeans_1_1ElkanKMeans}}{}
\item\contentsline{section}{\textbf{ Hamerly\+K\+Means$<$ Metric\+Type, Mat\+Type $>$} }{\pageref{classmlpack_1_1kmeans_1_1HamerlyKMeans}}{}
\item\contentsline{section}{\textbf{ K\+Means$<$ Metric\+Type, Initial\+Partition\+Policy, Empty\+Cluster\+Policy, Lloyd\+Step\+Type, Mat\+Type $>$} \\*This class implements K-\/\+Means clustering, using a variety of possible implementations of Lloyd\textquotesingle{}s algorithm }{\pageref{classmlpack_1_1kmeans_1_1KMeans}}{}
\item\contentsline{section}{\textbf{ Max\+Variance\+New\+Cluster} \\*When an empty cluster is detected, this class takes the point furthest from the centroid of the cluster with maximum variance as a new cluster }{\pageref{classmlpack_1_1kmeans_1_1MaxVarianceNewCluster}}{}
\item\contentsline{section}{\textbf{ Naive\+K\+Means$<$ Metric\+Type, Mat\+Type $>$} \\*This is an implementation of a single iteration of Lloyd\textquotesingle{}s algorithm for k-\/means }{\pageref{classmlpack_1_1kmeans_1_1NaiveKMeans}}{}
\item\contentsline{section}{\textbf{ Pelleg\+Moore\+K\+Means$<$ Metric\+Type, Mat\+Type $>$} \\*An implementation of Pelleg-\/\+Moore\textquotesingle{}s \textquotesingle{}blacklist\textquotesingle{} algorithm for k-\/means clustering }{\pageref{classmlpack_1_1kmeans_1_1PellegMooreKMeans}}{}
\item\contentsline{section}{\textbf{ Pelleg\+Moore\+K\+Means\+Rules$<$ Metric\+Type, Tree\+Type $>$} \\*The rules class for the single-\/tree Pelleg-\/\+Moore kd-\/tree traversal for k-\/means clustering }{\pageref{classmlpack_1_1kmeans_1_1PellegMooreKMeansRules}}{}
\item\contentsline{section}{\textbf{ Pelleg\+Moore\+K\+Means\+Statistic} \\*A statistic for trees which holds the blacklist for Pelleg-\/\+Moore k-\/means clustering (which represents the clusters that cannot possibly own any points in a node) }{\pageref{classmlpack_1_1kmeans_1_1PellegMooreKMeansStatistic}}{}
\item\contentsline{section}{\textbf{ Random\+Partition} \\*A very simple partitioner which partitions the data randomly into the number of desired clusters }{\pageref{classmlpack_1_1kmeans_1_1RandomPartition}}{}
\item\contentsline{section}{\textbf{ Refined\+Start} \\*A refined approach for choosing initial points for k-\/means clustering }{\pageref{classmlpack_1_1kmeans_1_1RefinedStart}}{}
\item\contentsline{section}{\textbf{ Sample\+Initialization} }{\pageref{classmlpack_1_1kmeans_1_1SampleInitialization}}{}
\item\contentsline{section}{\textbf{ Kernel\+P\+C\+A$<$ Kernel\+Type, Kernel\+Rule $>$} \\*This class performs kernel principal components analysis (Kernel P\+CA), for a given kernel }{\pageref{classmlpack_1_1kpca_1_1KernelPCA}}{}
\item\contentsline{section}{\textbf{ Naive\+Kernel\+Rule$<$ Kernel\+Type $>$} }{\pageref{classmlpack_1_1kpca_1_1NaiveKernelRule}}{}
\item\contentsline{section}{\textbf{ Nystroem\+Kernel\+Rule$<$ Kernel\+Type, Point\+Selection\+Policy $>$} }{\pageref{classmlpack_1_1kpca_1_1NystroemKernelRule}}{}
\item\contentsline{section}{\textbf{ Local\+Coordinate\+Coding} \\*An implementation of Local Coordinate Coding (L\+CC) that codes data which approximately lives on a manifold using a variation of l1-\/norm regularized sparse coding; in L\+CC, the penalty on the absolute value of each point\textquotesingle{}s coefficient for each atom is weighted by the squared distance of that point to that atom }{\pageref{classmlpack_1_1lcc_1_1LocalCoordinateCoding}}{}
\item\contentsline{section}{\textbf{ Constraints$<$ Metric\+Type $>$} \\*Interface for generating distance based constraints on a given dataset, provided corresponding true labels and a quantity parameter (k) are specified }{\pageref{classmlpack_1_1lmnn_1_1Constraints}}{}
\item\contentsline{section}{\textbf{ L\+M\+N\+N$<$ Metric\+Type, Optimizer\+Type $>$} \\*An implementation of Large Margin nearest neighbor metric learning technique }{\pageref{classmlpack_1_1lmnn_1_1LMNN}}{}
\item\contentsline{section}{\textbf{ L\+M\+N\+N\+Function$<$ Metric\+Type $>$} \\*The Large Margin Nearest Neighbors function }{\pageref{classmlpack_1_1lmnn_1_1LMNNFunction}}{}
\item\contentsline{section}{\textbf{ Columns\+To\+Blocks} \\*Transform the columns of the given matrix into a block format }{\pageref{classmlpack_1_1math_1_1ColumnsToBlocks}}{}
\item\contentsline{section}{\textbf{ Matrix\+Completion} \\*This class implements the popular nuclear norm minimization heuristic for matrix completion problems }{\pageref{classmlpack_1_1matrix__completion_1_1MatrixCompletion}}{}
\item\contentsline{section}{\textbf{ Mean\+Shift$<$ Use\+Kernel, Kernel\+Type, Mat\+Type $>$} \\*This class implements mean shift clustering }{\pageref{classmlpack_1_1meanshift_1_1MeanShift}}{}
\item\contentsline{section}{\textbf{ B\+L\+E\+U$<$ Elem\+Type, Precision\+Type $>$} \\*\doxyref{B\+L\+EU}{p.}{classmlpack_1_1metric_1_1BLEU}, or the Bilingual Evaluation Understudy, is an algorithm for evaluating the quality of text which has been machine translated from one natural language to another }{\pageref{classmlpack_1_1metric_1_1BLEU}}{}
\item\contentsline{section}{\textbf{ Io\+U$<$ Use\+Coordinates $>$} \\*Definition of Intersection over Union metric }{\pageref{classmlpack_1_1metric_1_1IoU}}{}
\item\contentsline{section}{\textbf{ I\+P\+Metric$<$ Kernel\+Type $>$} \\*The inner product metric, \doxyref{I\+P\+Metric}{p.}{classmlpack_1_1metric_1_1IPMetric}, takes a given Mercer kernel (Kernel\+Type), and when \doxyref{Evaluate()}{p.}{classmlpack_1_1metric_1_1IPMetric_a55e03560fb8c7923de4a43df9a265437} is called, returns the distance between the two points in kernel space\+: }{\pageref{classmlpack_1_1metric_1_1IPMetric}}{}
\item\contentsline{section}{\textbf{ L\+Metric$<$ T\+Power, T\+Take\+Root $>$} \\*The L\+\_\+p metric for arbitrary integer p, with an option to take the root }{\pageref{classmlpack_1_1metric_1_1LMetric}}{}
\item\contentsline{section}{\textbf{ Mahalanobis\+Distance$<$ Take\+Root $>$} \\*The Mahalanobis distance, which is essentially a stretched Euclidean distance }{\pageref{classmlpack_1_1metric_1_1MahalanobisDistance}}{}
\item\contentsline{section}{\textbf{ N\+M\+S$<$ Use\+Coordinates $>$} \\*Definition of Non Maximal Supression }{\pageref{classmlpack_1_1metric_1_1NMS}}{}
\item\contentsline{section}{\textbf{ M\+VU} \\*Meant to provide a good abstraction for users }{\pageref{classmlpack_1_1mvu_1_1MVU}}{}
\item\contentsline{section}{\textbf{ Naive\+Bayes\+Classifier$<$ Model\+Mat\+Type $>$} \\*The simple Naive Bayes classifier }{\pageref{classmlpack_1_1naive__bayes_1_1NaiveBayesClassifier}}{}
\item\contentsline{section}{\textbf{ N\+C\+A$<$ Metric\+Type, Optimizer\+Type $>$} \\*An implementation of Neighborhood Components Analysis, both a linear dimensionality reduction technique and a distance learning technique }{\pageref{classmlpack_1_1nca_1_1NCA}}{}
\item\contentsline{section}{\textbf{ Softmax\+Error\+Function$<$ Metric\+Type $>$} \\*The \char`\"{}softmax\char`\"{} stochastic neighbor assignment probability function }{\pageref{classmlpack_1_1nca_1_1SoftmaxErrorFunction}}{}
\item\contentsline{section}{\textbf{ Drusilla\+Select$<$ Mat\+Type $>$} }{\pageref{classmlpack_1_1neighbor_1_1DrusillaSelect}}{}
\item\contentsline{section}{\textbf{ Furthest\+NS} \\*This class implements the necessary methods for the Sort\+Policy template parameter of the \doxyref{Neighbor\+Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} class }{\pageref{classmlpack_1_1neighbor_1_1FurthestNS}}{}
\item\contentsline{section}{\textbf{ Leaf\+Size\+N\+S\+Wrapper$<$ Sort\+Policy, Tree\+Type, Dual\+Tree\+Traversal\+Type, Single\+Tree\+Traversal\+Type $>$} \\*\doxyref{Leaf\+Size\+N\+S\+Wrapper}{p.}{classmlpack_1_1neighbor_1_1LeafSizeNSWrapper} wraps any \doxyref{Neighbor\+Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} types that take a leaf size for tree construction }{\pageref{classmlpack_1_1neighbor_1_1LeafSizeNSWrapper}}{}
\item\contentsline{section}{\textbf{ Leaf\+Size\+R\+A\+Wrapper$<$ Tree\+Type $>$} \\*\doxyref{Leaf\+Size\+R\+A\+Wrapper}{p.}{classmlpack_1_1neighbor_1_1LeafSizeRAWrapper} wraps any \doxyref{R\+A\+Search}{p.}{classmlpack_1_1neighbor_1_1RASearch} type that needs to be able to take the leaf size into account when building trees }{\pageref{classmlpack_1_1neighbor_1_1LeafSizeRAWrapper}}{}
\item\contentsline{section}{\textbf{ L\+S\+H\+Search$<$ Sort\+Policy, Mat\+Type $>$} \\*The \doxyref{L\+S\+H\+Search}{p.}{classmlpack_1_1neighbor_1_1LSHSearch} class; this class builds a hash on the reference set and uses this hash to compute the distance-\/approximate nearest-\/neighbors of the given queries }{\pageref{classmlpack_1_1neighbor_1_1LSHSearch}}{}
\item\contentsline{section}{\textbf{ Nearest\+NS} \\*This class implements the necessary methods for the Sort\+Policy template parameter of the \doxyref{Neighbor\+Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} class }{\pageref{classmlpack_1_1neighbor_1_1NearestNS}}{}
\item\contentsline{section}{\textbf{ Neighbor\+Search$<$ Sort\+Policy, Metric\+Type, Mat\+Type, Tree\+Type, Dual\+Tree\+Traversal\+Type, Single\+Tree\+Traversal\+Type $>$} \\*The \doxyref{Neighbor\+Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} class is a template class for performing distance-\/based neighbor searches }{\pageref{classmlpack_1_1neighbor_1_1NeighborSearch}}{}
\item\contentsline{section}{\textbf{ Neighbor\+Search\+Rules$<$ Sort\+Policy, Metric\+Type, Tree\+Type $>$} \\*The \doxyref{Neighbor\+Search\+Rules}{p.}{classmlpack_1_1neighbor_1_1NeighborSearchRules} class is a template helper class used by \doxyref{Neighbor\+Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} class when performing distance-\/based neighbor searches }{\pageref{classmlpack_1_1neighbor_1_1NeighborSearchRules}}{}
\item\contentsline{section}{\textbf{ Neighbor\+Search\+Rules$<$ Sort\+Policy, Metric\+Type, Tree\+Type $>$\+::\+Candidate\+Cmp} \\*Compare two candidates based on the distance }{\pageref{structmlpack_1_1neighbor_1_1NeighborSearchRules_1_1CandidateCmp}}{}
\item\contentsline{section}{\textbf{ Neighbor\+Search\+Stat$<$ Sort\+Policy $>$} \\*Extra data for each node in the tree }{\pageref{classmlpack_1_1neighbor_1_1NeighborSearchStat}}{}
\item\contentsline{section}{\textbf{ N\+S\+Model$<$ Sort\+Policy $>$} \\*The \doxyref{N\+S\+Model}{p.}{classmlpack_1_1neighbor_1_1NSModel} class provides an easy way to serialize a model, abstracts away the different types of trees, and also reflects the \doxyref{Neighbor\+Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} A\+PI }{\pageref{classmlpack_1_1neighbor_1_1NSModel}}{}
\item\contentsline{section}{\textbf{ N\+S\+Wrapper$<$ Sort\+Policy, Tree\+Type, Dual\+Tree\+Traversal\+Type, Single\+Tree\+Traversal\+Type $>$} \\*\doxyref{N\+S\+Wrapper}{p.}{classmlpack_1_1neighbor_1_1NSWrapper} is a wrapper class for most \doxyref{Neighbor\+Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} types }{\pageref{classmlpack_1_1neighbor_1_1NSWrapper}}{}
\item\contentsline{section}{\textbf{ N\+S\+Wrapper\+Base} \\*\doxyref{N\+S\+Wrapper\+Base}{p.}{classmlpack_1_1neighbor_1_1NSWrapperBase} is a base wrapper class for holding all \doxyref{Neighbor\+Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} types supported by \doxyref{N\+S\+Model}{p.}{classmlpack_1_1neighbor_1_1NSModel} }{\pageref{classmlpack_1_1neighbor_1_1NSWrapperBase}}{}
\item\contentsline{section}{\textbf{ Q\+D\+A\+F\+N$<$ Mat\+Type $>$} }{\pageref{classmlpack_1_1neighbor_1_1QDAFN}}{}
\item\contentsline{section}{\textbf{ R\+A\+Model} \\*Abstraction for the \doxyref{R\+A\+Search}{p.}{classmlpack_1_1neighbor_1_1RASearch} class, abstracting away the Tree\+Type parameter and allowing it to be specified at runtime in this class }{\pageref{classmlpack_1_1neighbor_1_1RAModel}}{}
\item\contentsline{section}{\textbf{ R\+A\+Query\+Stat$<$ Sort\+Policy $>$} \\*Extra data for each node in the tree }{\pageref{classmlpack_1_1neighbor_1_1RAQueryStat}}{}
\item\contentsline{section}{\textbf{ R\+A\+Search$<$ Sort\+Policy, Metric\+Type, Mat\+Type, Tree\+Type $>$} \\*The \doxyref{R\+A\+Search}{p.}{classmlpack_1_1neighbor_1_1RASearch} class\+: This class provides a generic manner to perform rank-\/approximate search via random-\/sampling }{\pageref{classmlpack_1_1neighbor_1_1RASearch}}{}
\item\contentsline{section}{\textbf{ R\+A\+Search\+Rules$<$ Sort\+Policy, Metric\+Type, Tree\+Type $>$} \\*The \doxyref{R\+A\+Search\+Rules}{p.}{classmlpack_1_1neighbor_1_1RASearchRules} class is a template helper class used by \doxyref{R\+A\+Search}{p.}{classmlpack_1_1neighbor_1_1RASearch} class when performing rank-\/approximate search via random-\/sampling }{\pageref{classmlpack_1_1neighbor_1_1RASearchRules}}{}
\item\contentsline{section}{\textbf{ R\+A\+Util} }{\pageref{classmlpack_1_1neighbor_1_1RAUtil}}{}
\item\contentsline{section}{\textbf{ R\+A\+Wrapper$<$ Tree\+Type $>$} \\*\doxyref{R\+A\+Wrapper}{p.}{classmlpack_1_1neighbor_1_1RAWrapper} is a wrapper class for most \doxyref{R\+A\+Search}{p.}{classmlpack_1_1neighbor_1_1RASearch} types }{\pageref{classmlpack_1_1neighbor_1_1RAWrapper}}{}
\item\contentsline{section}{\textbf{ R\+A\+Wrapper\+Base} \\*\doxyref{R\+A\+Wrapper\+Base}{p.}{classmlpack_1_1neighbor_1_1RAWrapperBase} is a base wrapper class for holding all \doxyref{R\+A\+Search}{p.}{classmlpack_1_1neighbor_1_1RASearch} types supported by \doxyref{R\+A\+Model}{p.}{classmlpack_1_1neighbor_1_1RAModel} }{\pageref{classmlpack_1_1neighbor_1_1RAWrapperBase}}{}
\item\contentsline{section}{\textbf{ Spill\+N\+S\+Wrapper$<$ Sort\+Policy $>$} \\*The \doxyref{Spill\+N\+S\+Wrapper}{p.}{classmlpack_1_1neighbor_1_1SpillNSWrapper} class wraps the \doxyref{Neighbor\+Search}{p.}{classmlpack_1_1neighbor_1_1NeighborSearch} class when the spill tree is used }{\pageref{classmlpack_1_1neighbor_1_1SpillNSWrapper}}{}
\item\contentsline{section}{\textbf{ Sparse\+Autoencoder} \\*A sparse autoencoder is a neural network whose aim to learn compressed representations of the data, typically for dimensionality reduction, with a constraint on the activity of the neurons in the network }{\pageref{classmlpack_1_1nn_1_1SparseAutoencoder}}{}
\item\contentsline{section}{\textbf{ Sparse\+Autoencoder\+Function} \\*This is a class for the sparse autoencoder objective function }{\pageref{classmlpack_1_1nn_1_1SparseAutoencoderFunction}}{}
\item\contentsline{section}{\textbf{ Exact\+S\+V\+D\+Policy} \\*Implementation of the exact S\+VD policy }{\pageref{classmlpack_1_1pca_1_1ExactSVDPolicy}}{}
\item\contentsline{section}{\textbf{ P\+C\+A$<$ Decomposition\+Policy $>$} \\*This class implements principal components analysis (\doxyref{P\+CA}{p.}{classmlpack_1_1pca_1_1PCA}) }{\pageref{classmlpack_1_1pca_1_1PCA}}{}
\item\contentsline{section}{\textbf{ Q\+U\+I\+C\+S\+V\+D\+Policy} \\*Implementation of the Q\+U\+I\+C-\/\+S\+VD policy }{\pageref{classmlpack_1_1pca_1_1QUICSVDPolicy}}{}
\item\contentsline{section}{\textbf{ Randomized\+Block\+Krylov\+S\+V\+D\+Policy} \\*Implementation of the randomized block krylov S\+VD policy }{\pageref{classmlpack_1_1pca_1_1RandomizedBlockKrylovSVDPolicy}}{}
\item\contentsline{section}{\textbf{ Randomized\+S\+V\+D\+Policy} \\*Implementation of the randomized S\+VD policy }{\pageref{classmlpack_1_1pca_1_1RandomizedSVDPolicy}}{}
\item\contentsline{section}{\textbf{ Perceptron$<$ Learn\+Policy, Weight\+Initialization\+Policy, Mat\+Type $>$} \\*This class implements a simple perceptron (i.\+e., a single layer neural network) }{\pageref{classmlpack_1_1perceptron_1_1Perceptron}}{}
\item\contentsline{section}{\textbf{ Random\+Initialization} \\*This class is used to initialize weights for the weight\+Vectors matrix in a random manner }{\pageref{classmlpack_1_1perceptron_1_1RandomInitialization}}{}
\item\contentsline{section}{\textbf{ Simple\+Weight\+Update} }{\pageref{classmlpack_1_1perceptron_1_1SimpleWeightUpdate}}{}
\item\contentsline{section}{\textbf{ Zero\+Initialization} \\*This class is used to initialize the matrix weight\+Vectors to zero }{\pageref{classmlpack_1_1perceptron_1_1ZeroInitialization}}{}
\item\contentsline{section}{\textbf{ Radical} \\*An implementation of R\+A\+D\+I\+C\+AL, an algorithm for independent component analysis (I\+CA) }{\pageref{classmlpack_1_1radical_1_1Radical}}{}
\item\contentsline{section}{\textbf{ Leaf\+Size\+R\+S\+Wrapper$<$ Tree\+Type $>$} \\*Forward declaration }{\pageref{classmlpack_1_1range_1_1LeafSizeRSWrapper}}{}
\item\contentsline{section}{\textbf{ Range\+Search$<$ Metric\+Type, Mat\+Type, Tree\+Type $>$} \\*The \doxyref{Range\+Search}{p.}{classmlpack_1_1range_1_1RangeSearch} class is a template class for performing range searches }{\pageref{classmlpack_1_1range_1_1RangeSearch}}{}
\item\contentsline{section}{\textbf{ Range\+Search\+Rules$<$ Metric\+Type, Tree\+Type $>$} \\*The \doxyref{Range\+Search\+Rules}{p.}{classmlpack_1_1range_1_1RangeSearchRules} class is a template helper class used by \doxyref{Range\+Search}{p.}{classmlpack_1_1range_1_1RangeSearch} class when performing range searches }{\pageref{classmlpack_1_1range_1_1RangeSearchRules}}{}
\item\contentsline{section}{\textbf{ Range\+Search\+Stat} \\*Statistic class for \doxyref{Range\+Search}{p.}{classmlpack_1_1range_1_1RangeSearch}, to be set to the Statistic\+Type of the tree type that range search is being performed with }{\pageref{classmlpack_1_1range_1_1RangeSearchStat}}{}
\item\contentsline{section}{\textbf{ R\+S\+Model} \\*Abstraction for the \doxyref{Range\+Search}{p.}{classmlpack_1_1range_1_1RangeSearch} class, abstracting away the Tree\+Type parameter and allowing it to be specified at runtime }{\pageref{classmlpack_1_1range_1_1RSModel}}{}
\item\contentsline{section}{\textbf{ R\+S\+Wrapper$<$ Tree\+Type $>$} \\*\doxyref{R\+S\+Wrapper}{p.}{classmlpack_1_1range_1_1RSWrapper} is a wrapper class for most \doxyref{Range\+Search}{p.}{classmlpack_1_1range_1_1RangeSearch} types }{\pageref{classmlpack_1_1range_1_1RSWrapper}}{}
\item\contentsline{section}{\textbf{ R\+S\+Wrapper\+Base} \\*\doxyref{R\+S\+Wrapper\+Base}{p.}{classmlpack_1_1range_1_1RSWrapperBase} is a base wrapper class for holding all \doxyref{Range\+Search}{p.}{classmlpack_1_1range_1_1RangeSearch} types supported by \doxyref{R\+S\+Model}{p.}{classmlpack_1_1range_1_1RSModel} }{\pageref{classmlpack_1_1range_1_1RSWrapperBase}}{}
\item\contentsline{section}{\textbf{ Bayesian\+Linear\+Regression} \\*A Bayesian approach to the maximum likelihood estimation of the parameters $ \omega $ of the linear regression model }{\pageref{classmlpack_1_1regression_1_1BayesianLinearRegression}}{}
\item\contentsline{section}{\textbf{ L\+A\+RS} \\*An implementation of \doxyref{L\+A\+RS}{p.}{classmlpack_1_1regression_1_1LARS}, a stage-\/wise homotopy-\/based algorithm for l1-\/regularized linear regression (L\+A\+S\+SO) and l1+l2 regularized linear regression (Elastic Net) }{\pageref{classmlpack_1_1regression_1_1LARS}}{}
\item\contentsline{section}{\textbf{ Linear\+Regression} \\*A simple linear regression algorithm using ordinary least squares }{\pageref{classmlpack_1_1regression_1_1LinearRegression}}{}
\item\contentsline{section}{\textbf{ Logistic\+Regression$<$ Mat\+Type $>$} \\*The \doxyref{Logistic\+Regression}{p.}{classmlpack_1_1regression_1_1LogisticRegression} class implements an L2-\/regularized logistic regression model, and supports training with multiple optimizers and classification }{\pageref{classmlpack_1_1regression_1_1LogisticRegression}}{}
\item\contentsline{section}{\textbf{ Logistic\+Regression\+Function$<$ Mat\+Type $>$} \\*The log-\/likelihood function for the logistic regression objective function }{\pageref{classmlpack_1_1regression_1_1LogisticRegressionFunction}}{}
\item\contentsline{section}{\textbf{ Softmax\+Regression} \\*Softmax Regression is a classifier which can be used for classification when the data available can take two or more class values }{\pageref{classmlpack_1_1regression_1_1SoftmaxRegression}}{}
\item\contentsline{section}{\textbf{ Softmax\+Regression\+Function} }{\pageref{classmlpack_1_1regression_1_1SoftmaxRegressionFunction}}{}
\item\contentsline{section}{\textbf{ Acrobot} \\*Implementation of \doxyref{Acrobot}{p.}{classmlpack_1_1rl_1_1Acrobot} game }{\pageref{classmlpack_1_1rl_1_1Acrobot}}{}
\item\contentsline{section}{\textbf{ Acrobot\+::\+Action} }{\pageref{classmlpack_1_1rl_1_1Acrobot_1_1Action}}{}
\item\contentsline{section}{\textbf{ Acrobot\+::\+State} }{\pageref{classmlpack_1_1rl_1_1Acrobot_1_1State}}{}
\item\contentsline{section}{\textbf{ Aggregated\+Policy$<$ Policy\+Type $>$} }{\pageref{classmlpack_1_1rl_1_1AggregatedPolicy}}{}
\item\contentsline{section}{\textbf{ Async\+Learning$<$ Worker\+Type, Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$} \\*Wrapper of various asynchronous learning algorithms, e.\+g }{\pageref{classmlpack_1_1rl_1_1AsyncLearning}}{}
\item\contentsline{section}{\textbf{ Cart\+Pole} \\*Implementation of Cart Pole task }{\pageref{classmlpack_1_1rl_1_1CartPole}}{}
\item\contentsline{section}{\textbf{ Cart\+Pole\+::\+Action} \\*Implementation of action of Cart Pole }{\pageref{classmlpack_1_1rl_1_1CartPole_1_1Action}}{}
\item\contentsline{section}{\textbf{ Cart\+Pole\+::\+State} \\*Implementation of the state of Cart Pole }{\pageref{classmlpack_1_1rl_1_1CartPole_1_1State}}{}
\item\contentsline{section}{\textbf{ Categorical\+D\+Q\+N$<$ Output\+Layer\+Type, Init\+Type, Network\+Type $>$} \\*Implementation of the Categorical Deep Q-\/\+Learning network }{\pageref{classmlpack_1_1rl_1_1CategoricalDQN}}{}
\item\contentsline{section}{\textbf{ Continuous\+Action\+Env} \\*To use the dummy environment, one may start by specifying the state and action dimensions }{\pageref{classmlpack_1_1rl_1_1ContinuousActionEnv}}{}
\item\contentsline{section}{\textbf{ Continuous\+Action\+Env\+::\+Action} \\*Implementation of continuous action }{\pageref{classmlpack_1_1rl_1_1ContinuousActionEnv_1_1Action}}{}
\item\contentsline{section}{\textbf{ Continuous\+Action\+Env\+::\+State} \\*Implementation of state of the dummy environment }{\pageref{classmlpack_1_1rl_1_1ContinuousActionEnv_1_1State}}{}
\item\contentsline{section}{\textbf{ Continuous\+Double\+Pole\+Cart} \\*Implementation of Continuous Double Pole Cart Balancing task }{\pageref{classmlpack_1_1rl_1_1ContinuousDoublePoleCart}}{}
\item\contentsline{section}{\textbf{ Continuous\+Double\+Pole\+Cart\+::\+Action} \\*Implementation of action of Continuous Double Pole Cart }{\pageref{structmlpack_1_1rl_1_1ContinuousDoublePoleCart_1_1Action}}{}
\item\contentsline{section}{\textbf{ Continuous\+Double\+Pole\+Cart\+::\+State} \\*Implementation of the state of Continuous Double Pole Cart }{\pageref{classmlpack_1_1rl_1_1ContinuousDoublePoleCart_1_1State}}{}
\item\contentsline{section}{\textbf{ Continuous\+Mountain\+Car} \\*Implementation of Continuous Mountain Car task }{\pageref{classmlpack_1_1rl_1_1ContinuousMountainCar}}{}
\item\contentsline{section}{\textbf{ Continuous\+Mountain\+Car\+::\+Action} \\*Implementation of action of Continuous Mountain Car }{\pageref{structmlpack_1_1rl_1_1ContinuousMountainCar_1_1Action}}{}
\item\contentsline{section}{\textbf{ Continuous\+Mountain\+Car\+::\+State} \\*Implementation of state of Continuous Mountain Car }{\pageref{classmlpack_1_1rl_1_1ContinuousMountainCar_1_1State}}{}
\item\contentsline{section}{\textbf{ Discrete\+Action\+Env} \\*To use the dummy environment, one may start by specifying the state and action dimensions }{\pageref{classmlpack_1_1rl_1_1DiscreteActionEnv}}{}
\item\contentsline{section}{\textbf{ Discrete\+Action\+Env\+::\+Action} \\*Implementation of discrete action }{\pageref{classmlpack_1_1rl_1_1DiscreteActionEnv_1_1Action}}{}
\item\contentsline{section}{\textbf{ Discrete\+Action\+Env\+::\+State} \\*Implementation of state of the dummy environment }{\pageref{classmlpack_1_1rl_1_1DiscreteActionEnv_1_1State}}{}
\item\contentsline{section}{\textbf{ Double\+Pole\+Cart} \\*Implementation of Double Pole Cart Balancing task }{\pageref{classmlpack_1_1rl_1_1DoublePoleCart}}{}
\item\contentsline{section}{\textbf{ Double\+Pole\+Cart\+::\+Action} \\*Implementation of action of Double Pole Cart }{\pageref{classmlpack_1_1rl_1_1DoublePoleCart_1_1Action}}{}
\item\contentsline{section}{\textbf{ Double\+Pole\+Cart\+::\+State} \\*Implementation of the state of Double Pole Cart }{\pageref{classmlpack_1_1rl_1_1DoublePoleCart_1_1State}}{}
\item\contentsline{section}{\textbf{ Dueling\+D\+Q\+N$<$ Output\+Layer\+Type, Init\+Type, Complete\+Network\+Type, Feature\+Network\+Type, Advantage\+Network\+Type, Value\+Network\+Type $>$} \\*Implementation of the Dueling Deep Q-\/\+Learning network }{\pageref{classmlpack_1_1rl_1_1DuelingDQN}}{}
\item\contentsline{section}{\textbf{ Greedy\+Policy$<$ Environment\+Type $>$} \\*Implementation for epsilon greedy policy }{\pageref{classmlpack_1_1rl_1_1GreedyPolicy}}{}
\item\contentsline{section}{\textbf{ Mountain\+Car} \\*Implementation of Mountain Car task }{\pageref{classmlpack_1_1rl_1_1MountainCar}}{}
\item\contentsline{section}{\textbf{ Mountain\+Car\+::\+Action} \\*Implementation of action of Mountain Car }{\pageref{classmlpack_1_1rl_1_1MountainCar_1_1Action}}{}
\item\contentsline{section}{\textbf{ Mountain\+Car\+::\+State} \\*Implementation of state of Mountain Car }{\pageref{classmlpack_1_1rl_1_1MountainCar_1_1State}}{}
\item\contentsline{section}{\textbf{ N\+Step\+Q\+Learning\+Worker$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$} \\*Forward declaration of \doxyref{N\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1NStepQLearningWorker} }{\pageref{classmlpack_1_1rl_1_1NStepQLearningWorker}}{}
\item\contentsline{section}{\textbf{ One\+Step\+Q\+Learning\+Worker$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$} \\*Forward declaration of \doxyref{One\+Step\+Q\+Learning\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepQLearningWorker} }{\pageref{classmlpack_1_1rl_1_1OneStepQLearningWorker}}{}
\item\contentsline{section}{\textbf{ One\+Step\+Sarsa\+Worker$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type $>$} \\*Forward declaration of \doxyref{One\+Step\+Sarsa\+Worker}{p.}{classmlpack_1_1rl_1_1OneStepSarsaWorker} }{\pageref{classmlpack_1_1rl_1_1OneStepSarsaWorker}}{}
\item\contentsline{section}{\textbf{ Pendulum} \\*Implementation of \doxyref{Pendulum}{p.}{classmlpack_1_1rl_1_1Pendulum} task }{\pageref{classmlpack_1_1rl_1_1Pendulum}}{}
\item\contentsline{section}{\textbf{ Pendulum\+::\+Action} \\*Implementation of action of \doxyref{Pendulum}{p.}{classmlpack_1_1rl_1_1Pendulum} }{\pageref{classmlpack_1_1rl_1_1Pendulum_1_1Action}}{}
\item\contentsline{section}{\textbf{ Pendulum\+::\+State} \\*Implementation of state of \doxyref{Pendulum}{p.}{classmlpack_1_1rl_1_1Pendulum} }{\pageref{classmlpack_1_1rl_1_1Pendulum_1_1State}}{}
\item\contentsline{section}{\textbf{ Prioritized\+Replay$<$ Environment\+Type $>$} \\*Implementation of prioritized experience replay }{\pageref{classmlpack_1_1rl_1_1PrioritizedReplay}}{}
\item\contentsline{section}{\textbf{ Prioritized\+Replay$<$ Environment\+Type $>$\+::\+Transition} }{\pageref{structmlpack_1_1rl_1_1PrioritizedReplay_1_1Transition}}{}
\item\contentsline{section}{\textbf{ Q\+Learning$<$ Environment\+Type, Network\+Type, Updater\+Type, Policy\+Type, Replay\+Type $>$} \\*Implementation of various Q-\/\+Learning algorithms, such as D\+QN, double D\+QN }{\pageref{classmlpack_1_1rl_1_1QLearning}}{}
\item\contentsline{section}{\textbf{ Random\+Replay$<$ Environment\+Type $>$} \\*Implementation of random experience replay }{\pageref{classmlpack_1_1rl_1_1RandomReplay}}{}
\item\contentsline{section}{\textbf{ Random\+Replay$<$ Environment\+Type $>$\+::\+Transition} }{\pageref{structmlpack_1_1rl_1_1RandomReplay_1_1Transition}}{}
\item\contentsline{section}{\textbf{ Reward\+Clipping$<$ Environment\+Type $>$} \\*Interface for clipping the reward to some value between the specified maximum and minimum value (Clipping here is implemented as $ g_{\text{clipped}} = \max(g_{\text{min}}, \min(g_{\text{min}}, g))) $.) }{\pageref{classmlpack_1_1rl_1_1RewardClipping}}{}
\item\contentsline{section}{\textbf{ S\+A\+C$<$ Environment\+Type, Q\+Network\+Type, Policy\+Network\+Type, Updater\+Type, Replay\+Type $>$} \\*Implementation of Soft Actor-\/\+Critic, a model-\/free off-\/policy actor-\/critic based deep reinforcement learning algorithm }{\pageref{classmlpack_1_1rl_1_1SAC}}{}
\item\contentsline{section}{\textbf{ Simple\+D\+Q\+N$<$ Output\+Layer\+Type, Init\+Type, Network\+Type $>$} }{\pageref{classmlpack_1_1rl_1_1SimpleDQN}}{}
\item\contentsline{section}{\textbf{ Sum\+Tree$<$ T $>$} \\*Implementation of \doxyref{Sum\+Tree}{p.}{classmlpack_1_1rl_1_1SumTree} }{\pageref{classmlpack_1_1rl_1_1SumTree}}{}
\item\contentsline{section}{\textbf{ Training\+Config} }{\pageref{classmlpack_1_1rl_1_1TrainingConfig}}{}
\item\contentsline{section}{\textbf{ Method\+Form\+Detector$<$ Class, Method\+Form, Additional\+Args\+Count $>$} }{\pageref{structmlpack_1_1sfinae_1_1MethodFormDetector}}{}
\item\contentsline{section}{\textbf{ Method\+Form\+Detector$<$ Class, Method\+Form, 0 $>$} }{\pageref{structmlpack_1_1sfinae_1_1MethodFormDetector_3_01Class_00_01MethodForm_00_010_01_4}}{}
\item\contentsline{section}{\textbf{ Method\+Form\+Detector$<$ Class, Method\+Form, 1 $>$} }{\pageref{structmlpack_1_1sfinae_1_1MethodFormDetector_3_01Class_00_01MethodForm_00_011_01_4}}{}
\item\contentsline{section}{\textbf{ Method\+Form\+Detector$<$ Class, Method\+Form, 2 $>$} }{\pageref{structmlpack_1_1sfinae_1_1MethodFormDetector_3_01Class_00_01MethodForm_00_012_01_4}}{}
\item\contentsline{section}{\textbf{ Method\+Form\+Detector$<$ Class, Method\+Form, 3 $>$} }{\pageref{structmlpack_1_1sfinae_1_1MethodFormDetector_3_01Class_00_01MethodForm_00_013_01_4}}{}
\item\contentsline{section}{\textbf{ Method\+Form\+Detector$<$ Class, Method\+Form, 4 $>$} }{\pageref{structmlpack_1_1sfinae_1_1MethodFormDetector_3_01Class_00_01MethodForm_00_014_01_4}}{}
\item\contentsline{section}{\textbf{ Method\+Form\+Detector$<$ Class, Method\+Form, 5 $>$} }{\pageref{structmlpack_1_1sfinae_1_1MethodFormDetector_3_01Class_00_01MethodForm_00_015_01_4}}{}
\item\contentsline{section}{\textbf{ Method\+Form\+Detector$<$ Class, Method\+Form, 6 $>$} }{\pageref{structmlpack_1_1sfinae_1_1MethodFormDetector_3_01Class_00_01MethodForm_00_016_01_4}}{}
\item\contentsline{section}{\textbf{ Method\+Form\+Detector$<$ Class, Method\+Form, 7 $>$} }{\pageref{structmlpack_1_1sfinae_1_1MethodFormDetector_3_01Class_00_01MethodForm_00_017_01_4}}{}
\item\contentsline{section}{\textbf{ Sig\+Check$<$ U, U $>$} \\*Utility struct for checking signatures }{\pageref{structmlpack_1_1sfinae_1_1SigCheck}}{}
\item\contentsline{section}{\textbf{ Data\+Dependent\+Random\+Initializer} \\*A data-\/dependent random dictionary initializer for \doxyref{Sparse\+Coding}{p.}{classmlpack_1_1sparse__coding_1_1SparseCoding} }{\pageref{classmlpack_1_1sparse__coding_1_1DataDependentRandomInitializer}}{}
\item\contentsline{section}{\textbf{ Nothing\+Initializer} \\*A Dictionary\+Initializer for \doxyref{Sparse\+Coding}{p.}{classmlpack_1_1sparse__coding_1_1SparseCoding} which does not initialize anything; it is useful for when the dictionary is already known and will be set with \doxyref{Sparse\+Coding\+::\+Dictionary()}{p.}{classmlpack_1_1sparse__coding_1_1SparseCoding_a3146526cfc85ff339121972d67c73f62} }{\pageref{classmlpack_1_1sparse__coding_1_1NothingInitializer}}{}
\item\contentsline{section}{\textbf{ Random\+Initializer} \\*A Dictionary\+Initializer for use with the \doxyref{Sparse\+Coding}{p.}{classmlpack_1_1sparse__coding_1_1SparseCoding} class }{\pageref{classmlpack_1_1sparse__coding_1_1RandomInitializer}}{}
\item\contentsline{section}{\textbf{ Sparse\+Coding} \\*An implementation of Sparse Coding with Dictionary Learning that achieves sparsity via an l1-\/norm regularizer on the codes (L\+A\+S\+SO) or an (l1+l2)-\/norm regularizer on the codes (the Elastic Net) }{\pageref{classmlpack_1_1sparse__coding_1_1SparseCoding}}{}
\item\contentsline{section}{\textbf{ Bias\+S\+V\+D$<$ Optimizer\+Type $>$} \\*Bias S\+VD is an improvement on Regularized S\+VD which is a matrix factorization techniques }{\pageref{classmlpack_1_1svd_1_1BiasSVD}}{}
\item\contentsline{section}{\textbf{ Bias\+S\+V\+D\+Function$<$ Mat\+Type $>$} \\*This class contains methods which are used to calculate the cost of \doxyref{Bias\+S\+VD}{p.}{classmlpack_1_1svd_1_1BiasSVD}\textquotesingle{}s objective function, to calculate gradient of parameters with respect to the objective function, etc }{\pageref{classmlpack_1_1svd_1_1BiasSVDFunction}}{}
\item\contentsline{section}{\textbf{ Q\+U\+I\+C\+\_\+\+S\+VD} \\*Q\+U\+I\+C-\/\+S\+VD is a matrix factorization technique, which operates in a subspace such that A\textquotesingle{}s approximation in that subspace has minimum error(A being the data matrix) }{\pageref{classmlpack_1_1svd_1_1QUIC__SVD}}{}
\item\contentsline{section}{\textbf{ Randomized\+Block\+Krylov\+S\+VD} \\*Randomized block krylov S\+VD is a matrix factorization that is based on randomized matrix approximation techniques, developed in in \char`\"{}\+Randomized Block Krylov Methods for Stronger and Faster Approximate
\+Singular Value Decomposition\char`\"{} }{\pageref{classmlpack_1_1svd_1_1RandomizedBlockKrylovSVD}}{}
\item\contentsline{section}{\textbf{ Randomized\+S\+VD} \\*Randomized S\+VD is a matrix factorization that is based on randomized matrix approximation techniques, developed in in \char`\"{}\+Finding structure with randomness\+:
\+Probabilistic algorithms for constructing approximate matrix decompositions\char`\"{} }{\pageref{classmlpack_1_1svd_1_1RandomizedSVD}}{}
\item\contentsline{section}{\textbf{ Regularized\+S\+V\+D$<$ Optimizer\+Type $>$} \\*Regularized S\+VD is a matrix factorization technique that seeks to reduce the error on the training set, that is on the examples for which the ratings have been provided by the users }{\pageref{classmlpack_1_1svd_1_1RegularizedSVD}}{}
\item\contentsline{section}{\textbf{ Regularized\+S\+V\+D\+Function$<$ Mat\+Type $>$} \\*The data is stored in a matrix of type Mat\+Type, so that this class can be used with both dense and sparse matrix types }{\pageref{classmlpack_1_1svd_1_1RegularizedSVDFunction}}{}
\item\contentsline{section}{\textbf{ S\+V\+D\+Plus\+Plus$<$ Optimizer\+Type $>$} \\*S\+V\+D++ is a matrix decomposition tenique used in collaborative filtering }{\pageref{classmlpack_1_1svd_1_1SVDPlusPlus}}{}
\item\contentsline{section}{\textbf{ S\+V\+D\+Plus\+Plus\+Function$<$ Mat\+Type $>$} \\*This class contains methods which are used to calculate the cost of S\+V\+D++\textquotesingle{}s objective function, to calculate gradient of parameters with respect to the objective function, etc }{\pageref{classmlpack_1_1svd_1_1SVDPlusPlusFunction}}{}
\item\contentsline{section}{\textbf{ Linear\+S\+V\+M$<$ Mat\+Type $>$} \\*The \doxyref{Linear\+S\+VM}{p.}{classmlpack_1_1svm_1_1LinearSVM} class implements an L2-\/regularized support vector machine model, and supports training with multiple optimizers and classification }{\pageref{classmlpack_1_1svm_1_1LinearSVM}}{}
\item\contentsline{section}{\textbf{ Linear\+S\+V\+M\+Function$<$ Mat\+Type $>$} \\*The hinge loss function for the linear S\+VM objective function }{\pageref{classmlpack_1_1svm_1_1LinearSVMFunction}}{}
\item\contentsline{section}{\textbf{ Timer} \\*The timer class provides a way for mlpack methods to be timed }{\pageref{classmlpack_1_1Timer}}{}
\item\contentsline{section}{\textbf{ Timers} }{\pageref{classmlpack_1_1Timers}}{}
\item\contentsline{section}{\textbf{ All\+Categorical\+Split$<$ Fitness\+Function $>$} \\*The \doxyref{All\+Categorical\+Split}{p.}{classmlpack_1_1tree_1_1AllCategoricalSplit} is a splitting function that will split categorical features into many children\+: one child for each category }{\pageref{classmlpack_1_1tree_1_1AllCategoricalSplit}}{}
\item\contentsline{section}{\textbf{ All\+Categorical\+Split$<$ Fitness\+Function $>$\+::\+Auxiliary\+Split\+Info} }{\pageref{classmlpack_1_1tree_1_1AllCategoricalSplit_1_1AuxiliarySplitInfo}}{}
\item\contentsline{section}{\textbf{ All\+Dimension\+Select} \\*This dimension selection policy allows any dimension to be selected for splitting }{\pageref{classmlpack_1_1tree_1_1AllDimensionSelect}}{}
\item\contentsline{section}{\textbf{ Axis\+Parallel\+Proj\+Vector} \\*\doxyref{Axis\+Parallel\+Proj\+Vector}{p.}{classmlpack_1_1tree_1_1AxisParallelProjVector} defines an axis-\/parallel projection vector }{\pageref{classmlpack_1_1tree_1_1AxisParallelProjVector}}{}
\item\contentsline{section}{\textbf{ Best\+Binary\+Numeric\+Split$<$ Fitness\+Function $>$} \\*The \doxyref{Best\+Binary\+Numeric\+Split}{p.}{classmlpack_1_1tree_1_1BestBinaryNumericSplit} is a splitting function for decision trees that will exhaustively search a numeric dimension for the best binary split }{\pageref{classmlpack_1_1tree_1_1BestBinaryNumericSplit}}{}
\item\contentsline{section}{\textbf{ Best\+Binary\+Numeric\+Split$<$ Fitness\+Function $>$\+::\+Auxiliary\+Split\+Info} }{\pageref{classmlpack_1_1tree_1_1BestBinaryNumericSplit_1_1AuxiliarySplitInfo}}{}
\item\contentsline{section}{\textbf{ Binary\+Numeric\+Split$<$ Fitness\+Function, Observation\+Type $>$} \\*The \doxyref{Binary\+Numeric\+Split}{p.}{classmlpack_1_1tree_1_1BinaryNumericSplit} class implements the numeric feature splitting strategy devised by Gama, Rocha, and Medas in the following paper\+: }{\pageref{classmlpack_1_1tree_1_1BinaryNumericSplit}}{}
\item\contentsline{section}{\textbf{ Binary\+Numeric\+Split\+Info$<$ Observation\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1BinaryNumericSplitInfo}}{}
\item\contentsline{section}{\textbf{ Binary\+Space\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Bound\+Type, Split\+Type $>$} \\*A binary space partitioning tree, such as a K\+D-\/tree or a ball tree }{\pageref{classmlpack_1_1tree_1_1BinarySpaceTree}}{}
\item\contentsline{section}{\textbf{ Binary\+Space\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Bound\+Type, Split\+Type $>$\+::\+Breadth\+First\+Dual\+Tree\+Traverser$<$ Rule\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1BinarySpaceTree_1_1BreadthFirstDualTreeTraverser}}{}
\item\contentsline{section}{\textbf{ Binary\+Space\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Bound\+Type, Split\+Type $>$\+::\+Dual\+Tree\+Traverser$<$ Rule\+Type $>$} \\*A dual-\/tree traverser for binary space trees; see dual\+\_\+tree\+\_\+traverser.\+hpp }{\pageref{classmlpack_1_1tree_1_1BinarySpaceTree_1_1DualTreeTraverser}}{}
\item\contentsline{section}{\textbf{ Binary\+Space\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Bound\+Type, Split\+Type $>$\+::\+Single\+Tree\+Traverser$<$ Rule\+Type $>$} \\*A single-\/tree traverser for binary space trees; see single\+\_\+tree\+\_\+traverser.\+hpp for implementation }{\pageref{classmlpack_1_1tree_1_1BinarySpaceTree_1_1SingleTreeTraverser}}{}
\item\contentsline{section}{\textbf{ Categorical\+Split\+Info} }{\pageref{classmlpack_1_1tree_1_1CategoricalSplitInfo}}{}
\item\contentsline{section}{\textbf{ Compare\+Cosine\+Node} }{\pageref{classmlpack_1_1tree_1_1CompareCosineNode}}{}
\item\contentsline{section}{\textbf{ Cosine\+Tree} }{\pageref{classmlpack_1_1tree_1_1CosineTree}}{}
\item\contentsline{section}{\textbf{ Cover\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Root\+Point\+Policy $>$} \\*A cover tree is a tree specifically designed to speed up nearest-\/neighbor computation in high-\/dimensional spaces }{\pageref{classmlpack_1_1tree_1_1CoverTree}}{}
\item\contentsline{section}{\textbf{ Cover\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Root\+Point\+Policy $>$\+::\+Dual\+Tree\+Traverser$<$ Rule\+Type $>$} \\*A dual-\/tree cover tree traverser; see dual\+\_\+tree\+\_\+traverser.\+hpp }{\pageref{classmlpack_1_1tree_1_1CoverTree_1_1DualTreeTraverser}}{}
\item\contentsline{section}{\textbf{ Cover\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Root\+Point\+Policy $>$\+::\+Single\+Tree\+Traverser$<$ Rule\+Type $>$} \\*A single-\/tree cover tree traverser; see single\+\_\+tree\+\_\+traverser.\+hpp for implementation }{\pageref{classmlpack_1_1tree_1_1CoverTree_1_1SingleTreeTraverser}}{}
\item\contentsline{section}{\textbf{ Decision\+Tree$<$ Fitness\+Function, Numeric\+Split\+Type, Categorical\+Split\+Type, Dimension\+Selection\+Type, No\+Recursion $>$} \\*This class implements a generic decision tree learner }{\pageref{classmlpack_1_1tree_1_1DecisionTree}}{}
\item\contentsline{section}{\textbf{ Discrete\+Hilbert\+Value$<$ Tree\+Elem\+Type $>$} \\*The \doxyref{Discrete\+Hilbert\+Value}{p.}{classmlpack_1_1tree_1_1DiscreteHilbertValue} class stores Hilbert values for all of the points in a \doxyref{Rectangle\+Tree}{p.}{namespacemlpack_1_1tree_1_1RectangleTree} node, and calculates Hilbert values for new points }{\pageref{classmlpack_1_1tree_1_1DiscreteHilbertValue}}{}
\item\contentsline{section}{\textbf{ Empty\+Statistic} \\*Empty statistic if you are not interested in storing statistics in your tree }{\pageref{classmlpack_1_1tree_1_1EmptyStatistic}}{}
\item\contentsline{section}{\textbf{ Example\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type $>$} \\*This is not an actual space tree but instead an example tree that exists to show and document all the functions that mlpack trees must implement }{\pageref{classmlpack_1_1tree_1_1ExampleTree}}{}
\item\contentsline{section}{\textbf{ First\+Point\+Is\+Root} \\*This class is meant to be used as a choice for the policy class Root\+Point\+Policy of the \doxyref{Cover\+Tree}{p.}{classmlpack_1_1tree_1_1CoverTree} class }{\pageref{classmlpack_1_1tree_1_1FirstPointIsRoot}}{}
\item\contentsline{section}{\textbf{ Gini\+Gain} \\*The Gini gain, a measure of set purity usable as a fitness function (Fitness\+Function) for decision trees }{\pageref{classmlpack_1_1tree_1_1GiniGain}}{}
\item\contentsline{section}{\textbf{ Gini\+Impurity} }{\pageref{classmlpack_1_1tree_1_1GiniImpurity}}{}
\item\contentsline{section}{\textbf{ Greedy\+Single\+Tree\+Traverser$<$ Tree\+Type, Rule\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1GreedySingleTreeTraverser}}{}
\item\contentsline{section}{\textbf{ Hilbert\+R\+Tree\+Auxiliary\+Information$<$ Tree\+Type, Hilbert\+Value\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1HilbertRTreeAuxiliaryInformation}}{}
\item\contentsline{section}{\textbf{ Hilbert\+R\+Tree\+Descent\+Heuristic} \\*This class chooses the best child of a node in a Hilbert R tree when inserting a new point }{\pageref{classmlpack_1_1tree_1_1HilbertRTreeDescentHeuristic}}{}
\item\contentsline{section}{\textbf{ Hilbert\+R\+Tree\+Split$<$ split\+Order $>$} \\*The splitting procedure for the Hilbert R tree }{\pageref{classmlpack_1_1tree_1_1HilbertRTreeSplit}}{}
\item\contentsline{section}{\textbf{ Hoeffding\+Categorical\+Split$<$ Fitness\+Function $>$} \\*This is the standard Hoeffding-\/bound categorical feature proposed in the paper below\+: }{\pageref{classmlpack_1_1tree_1_1HoeffdingCategoricalSplit}}{}
\item\contentsline{section}{\textbf{ Hoeffding\+Information\+Gain} }{\pageref{classmlpack_1_1tree_1_1HoeffdingInformationGain}}{}
\item\contentsline{section}{\textbf{ Hoeffding\+Numeric\+Split$<$ Fitness\+Function, Observation\+Type $>$} \\*The \doxyref{Hoeffding\+Numeric\+Split}{p.}{classmlpack_1_1tree_1_1HoeffdingNumericSplit} class implements the numeric feature splitting strategy alluded to by Domingos and Hulten in the following paper\+: }{\pageref{classmlpack_1_1tree_1_1HoeffdingNumericSplit}}{}
\item\contentsline{section}{\textbf{ Hoeffding\+Tree$<$ Fitness\+Function, Numeric\+Split\+Type, Categorical\+Split\+Type $>$} \\*The \doxyref{Hoeffding\+Tree}{p.}{classmlpack_1_1tree_1_1HoeffdingTree} object represents all of the necessary information for a Hoeffding-\/bound-\/based decision tree }{\pageref{classmlpack_1_1tree_1_1HoeffdingTree}}{}
\item\contentsline{section}{\textbf{ Hoeffding\+Tree\+Model} \\*This class is a serializable Hoeffding tree model that can hold four different types of Hoeffding trees }{\pageref{classmlpack_1_1tree_1_1HoeffdingTreeModel}}{}
\item\contentsline{section}{\textbf{ Hyperplane\+Base$<$ Bound\+T, Proj\+Vector\+T $>$} \\*\doxyref{Hyperplane\+Base}{p.}{classmlpack_1_1tree_1_1HyperplaneBase} defines a splitting hyperplane based on a projection vector and projection value }{\pageref{classmlpack_1_1tree_1_1HyperplaneBase}}{}
\item\contentsline{section}{\textbf{ Information\+Gain} \\*The standard information gain criterion, used for calculating gain in decision trees }{\pageref{classmlpack_1_1tree_1_1InformationGain}}{}
\item\contentsline{section}{\textbf{ Is\+Spill\+Tree$<$ Tree\+Type $>$} }{\pageref{structmlpack_1_1tree_1_1IsSpillTree}}{}
\item\contentsline{section}{\textbf{ Is\+Spill\+Tree$<$ tree\+::\+Spill\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Hyperplane\+Type, Split\+Type $>$ $>$} }{\pageref{structmlpack_1_1tree_1_1IsSpillTree_3_01tree_1_1SpillTree_3_01MetricType_00_01StatisticType_00_0d41f2b10e451850b8eb14d3156c51340}}{}
\item\contentsline{section}{\textbf{ Mean\+Space\+Split$<$ Metric\+Type, Mat\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1MeanSpaceSplit}}{}
\item\contentsline{section}{\textbf{ Mean\+Split$<$ Bound\+Type, Mat\+Type $>$} \\*A binary space partitioning tree node is split into its left and right child }{\pageref{classmlpack_1_1tree_1_1MeanSplit}}{}
\item\contentsline{section}{\textbf{ Mean\+Split$<$ Bound\+Type, Mat\+Type $>$\+::\+Split\+Info} \\*An information about the partition }{\pageref{structmlpack_1_1tree_1_1MeanSplit_1_1SplitInfo}}{}
\item\contentsline{section}{\textbf{ Midpoint\+Space\+Split$<$ Metric\+Type, Mat\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1MidpointSpaceSplit}}{}
\item\contentsline{section}{\textbf{ Midpoint\+Split$<$ Bound\+Type, Mat\+Type $>$} \\*A binary space partitioning tree node is split into its left and right child }{\pageref{classmlpack_1_1tree_1_1MidpointSplit}}{}
\item\contentsline{section}{\textbf{ Midpoint\+Split$<$ Bound\+Type, Mat\+Type $>$\+::\+Split\+Info} \\*A struct that contains an information about the split }{\pageref{structmlpack_1_1tree_1_1MidpointSplit_1_1SplitInfo}}{}
\item\contentsline{section}{\textbf{ Minimal\+Coverage\+Sweep$<$ Split\+Policy $>$} \\*The \doxyref{Minimal\+Coverage\+Sweep}{p.}{classmlpack_1_1tree_1_1MinimalCoverageSweep} class finds a partition along which we can split a node according to the coverage of two resulting nodes }{\pageref{classmlpack_1_1tree_1_1MinimalCoverageSweep}}{}
\item\contentsline{section}{\textbf{ Minimal\+Coverage\+Sweep$<$ Split\+Policy $>$\+::\+Sweep\+Cost$<$ Tree\+Type $>$} \\*A struct that provides the type of the sweep cost }{\pageref{structmlpack_1_1tree_1_1MinimalCoverageSweep_1_1SweepCost}}{}
\item\contentsline{section}{\textbf{ Minimal\+Splits\+Number\+Sweep$<$ Split\+Policy $>$} \\*The \doxyref{Minimal\+Splits\+Number\+Sweep}{p.}{classmlpack_1_1tree_1_1MinimalSplitsNumberSweep} class finds a partition along which we can split a node according to the number of required splits of the node }{\pageref{classmlpack_1_1tree_1_1MinimalSplitsNumberSweep}}{}
\item\contentsline{section}{\textbf{ Minimal\+Splits\+Number\+Sweep$<$ Split\+Policy $>$\+::\+Sweep\+Cost$<$ typename $>$} \\*A struct that provides the type of the sweep cost }{\pageref{structmlpack_1_1tree_1_1MinimalSplitsNumberSweep_1_1SweepCost}}{}
\item\contentsline{section}{\textbf{ Multiple\+Random\+Dimension\+Select} \\*This dimension selection policy allows the selection from a few random dimensions }{\pageref{classmlpack_1_1tree_1_1MultipleRandomDimensionSelect}}{}
\item\contentsline{section}{\textbf{ No\+Auxiliary\+Information$<$ Tree\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1NoAuxiliaryInformation}}{}
\item\contentsline{section}{\textbf{ Numeric\+Split\+Info$<$ Observation\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1NumericSplitInfo}}{}
\item\contentsline{section}{\textbf{ Octree$<$ Metric\+Type, Statistic\+Type, Mat\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1Octree}}{}
\item\contentsline{section}{\textbf{ Octree$<$ Metric\+Type, Statistic\+Type, Mat\+Type $>$\+::\+Dual\+Tree\+Traverser$<$ Metric\+Type, Statistic\+Type, Mat\+Type $>$} \\*A dual-\/tree traverser; see dual\+\_\+tree\+\_\+traverser.\+hpp }{\pageref{classmlpack_1_1tree_1_1Octree_1_1DualTreeTraverser}}{}
\item\contentsline{section}{\textbf{ Octree$<$ Metric\+Type, Statistic\+Type, Mat\+Type $>$\+::\+Single\+Tree\+Traverser$<$ Rule\+Type $>$} \\*A single-\/tree traverser; see single\+\_\+tree\+\_\+traverser.\+hpp }{\pageref{classmlpack_1_1tree_1_1Octree_1_1SingleTreeTraverser}}{}
\item\contentsline{section}{\textbf{ Octree$<$ Metric\+Type, Statistic\+Type, Mat\+Type $>$\+::\+Split\+Type\+::\+Split\+Info} }{\pageref{structmlpack_1_1tree_1_1Octree_1_1SplitType_1_1SplitInfo}}{}
\item\contentsline{section}{\textbf{ Proj\+Vector} \\*\doxyref{Proj\+Vector}{p.}{classmlpack_1_1tree_1_1ProjVector} defines a general projection vector (not necessarily axis-\/parallel) }{\pageref{classmlpack_1_1tree_1_1ProjVector}}{}
\item\contentsline{section}{\textbf{ Queue\+Frame$<$ Tree\+Type, Traversal\+Info\+Type $>$} }{\pageref{structmlpack_1_1tree_1_1QueueFrame}}{}
\item\contentsline{section}{\textbf{ Random\+Binary\+Numeric\+Split$<$ Fitness\+Function $>$} \\*The \doxyref{Random\+Binary\+Numeric\+Split}{p.}{classmlpack_1_1tree_1_1RandomBinaryNumericSplit} is a splitting function for decision trees that will split based on a randomly selected point between the minimum and maximum value of the numerical dimension }{\pageref{classmlpack_1_1tree_1_1RandomBinaryNumericSplit}}{}
\item\contentsline{section}{\textbf{ Random\+Binary\+Numeric\+Split$<$ Fitness\+Function $>$\+::\+Auxiliary\+Split\+Info} }{\pageref{classmlpack_1_1tree_1_1RandomBinaryNumericSplit_1_1AuxiliarySplitInfo}}{}
\item\contentsline{section}{\textbf{ Random\+Dimension\+Select} \\*This dimension selection policy only selects one single random dimension }{\pageref{classmlpack_1_1tree_1_1RandomDimensionSelect}}{}
\item\contentsline{section}{\textbf{ Random\+Forest$<$ Fitness\+Function, Dimension\+Selection\+Type, Numeric\+Split\+Type, Categorical\+Split\+Type, Use\+Bootstrap $>$} \\*The \doxyref{Random\+Forest}{p.}{classmlpack_1_1tree_1_1RandomForest} class provides an implementation of random forests, described in Breiman\textquotesingle{}s seminal paper\+: }{\pageref{classmlpack_1_1tree_1_1RandomForest}}{}
\item\contentsline{section}{\textbf{ Dual\+Tree\+Traverser$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Split\+Type, Descent\+Type, Auxiliary\+Information\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1RectangleTree_1_1DualTreeTraverser}}{}
\item\contentsline{section}{\textbf{ Single\+Tree\+Traverser$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Split\+Type, Descent\+Type, Auxiliary\+Information\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1RectangleTree_1_1SingleTreeTraverser}}{}
\item\contentsline{section}{\textbf{ R\+Plus\+Plus\+Tree\+Auxiliary\+Information$<$ Tree\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1RPlusPlusTreeAuxiliaryInformation}}{}
\item\contentsline{section}{\textbf{ R\+Plus\+Plus\+Tree\+Descent\+Heuristic} }{\pageref{classmlpack_1_1tree_1_1RPlusPlusTreeDescentHeuristic}}{}
\item\contentsline{section}{\textbf{ R\+Plus\+Plus\+Tree\+Split\+Policy} \\*The \doxyref{R\+Plus\+Plus\+Tree\+Split\+Policy}{p.}{classmlpack_1_1tree_1_1RPlusPlusTreeSplitPolicy} helps to determine the subtree into which we should insert a child of an intermediate node that is being split }{\pageref{classmlpack_1_1tree_1_1RPlusPlusTreeSplitPolicy}}{}
\item\contentsline{section}{\textbf{ R\+Plus\+Tree\+Descent\+Heuristic} }{\pageref{classmlpack_1_1tree_1_1RPlusTreeDescentHeuristic}}{}
\item\contentsline{section}{\textbf{ R\+Plus\+Tree\+Split$<$ Split\+Policy\+Type, Sweep\+Type $>$} \\*The \doxyref{R\+Plus\+Tree\+Split}{p.}{classmlpack_1_1tree_1_1RPlusTreeSplit} class performs the split process of a node on overflow }{\pageref{classmlpack_1_1tree_1_1RPlusTreeSplit}}{}
\item\contentsline{section}{\textbf{ R\+Plus\+Tree\+Split\+Policy} \\*The \doxyref{R\+Plus\+Plus\+Tree\+Split\+Policy}{p.}{classmlpack_1_1tree_1_1RPlusPlusTreeSplitPolicy} helps to determine the subtree into which we should insert a child of an intermediate node that is being split }{\pageref{classmlpack_1_1tree_1_1RPlusTreeSplitPolicy}}{}
\item\contentsline{section}{\textbf{ R\+P\+Tree\+Max\+Split$<$ Bound\+Type, Mat\+Type $>$} \\*This class splits a node by a random hyperplane }{\pageref{classmlpack_1_1tree_1_1RPTreeMaxSplit}}{}
\item\contentsline{section}{\textbf{ R\+P\+Tree\+Max\+Split$<$ Bound\+Type, Mat\+Type $>$\+::\+Split\+Info} \\*An information about the partition }{\pageref{structmlpack_1_1tree_1_1RPTreeMaxSplit_1_1SplitInfo}}{}
\item\contentsline{section}{\textbf{ R\+P\+Tree\+Mean\+Split$<$ Bound\+Type, Mat\+Type $>$} \\*This class splits a binary space tree }{\pageref{classmlpack_1_1tree_1_1RPTreeMeanSplit}}{}
\item\contentsline{section}{\textbf{ R\+P\+Tree\+Mean\+Split$<$ Bound\+Type, Mat\+Type $>$\+::\+Split\+Info} \\*An information about the partition }{\pageref{structmlpack_1_1tree_1_1RPTreeMeanSplit_1_1SplitInfo}}{}
\item\contentsline{section}{\textbf{ R\+Star\+Tree\+Descent\+Heuristic} \\*When descending a \doxyref{Rectangle\+Tree}{p.}{namespacemlpack_1_1tree_1_1RectangleTree} to insert a point, we need to have a way to choose a child node when the point isn\textquotesingle{}t enclosed by any of them }{\pageref{classmlpack_1_1tree_1_1RStarTreeDescentHeuristic}}{}
\item\contentsline{section}{\textbf{ R\+Star\+Tree\+Split} \\*A Rectangle Tree has new points inserted at the bottom }{\pageref{classmlpack_1_1tree_1_1RStarTreeSplit}}{}
\item\contentsline{section}{\textbf{ R\+Tree\+Descent\+Heuristic} \\*When descending a \doxyref{Rectangle\+Tree}{p.}{namespacemlpack_1_1tree_1_1RectangleTree} to insert a point, we need to have a way to choose a child node when the point isn\textquotesingle{}t enclosed by any of them }{\pageref{classmlpack_1_1tree_1_1RTreeDescentHeuristic}}{}
\item\contentsline{section}{\textbf{ R\+Tree\+Split} \\*A Rectangle Tree has new points inserted at the bottom }{\pageref{classmlpack_1_1tree_1_1RTreeSplit}}{}
\item\contentsline{section}{\textbf{ Space\+Split$<$ Metric\+Type, Mat\+Type $>$} }{\pageref{classmlpack_1_1tree_1_1SpaceSplit}}{}
\item\contentsline{section}{\textbf{ Spill\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Hyperplane\+Type, Split\+Type $>$} \\*A hybrid spill tree is a variant of binary space trees in which the children of a node can \char`\"{}spill over\char`\"{} each other, and contain shared datapoints }{\pageref{classmlpack_1_1tree_1_1SpillTree}}{}
\item\contentsline{section}{\textbf{ Spill\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Hyperplane\+Type, Split\+Type $>$\+::\+Spill\+Dual\+Tree\+Traverser$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Hyperplane\+Type, Split\+Type $>$} \\*A generic dual-\/tree traverser for hybrid spill trees; see \doxyref{spill\+\_\+dual\+\_\+tree\+\_\+traverser.\+hpp}{p.}{spill__dual__tree__traverser_8hpp} for implementation }{\pageref{classmlpack_1_1tree_1_1SpillTree_1_1SpillDualTreeTraverser}}{}
\item\contentsline{section}{\textbf{ Spill\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Hyperplane\+Type, Split\+Type $>$\+::\+Spill\+Single\+Tree\+Traverser$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Hyperplane\+Type, Split\+Type $>$} \\*A generic single-\/tree traverser for hybrid spill trees; see \doxyref{spill\+\_\+single\+\_\+tree\+\_\+traverser.\+hpp}{p.}{spill__single__tree__traverser_8hpp} for implementation }{\pageref{classmlpack_1_1tree_1_1SpillTree_1_1SpillSingleTreeTraverser}}{}
\item\contentsline{section}{\textbf{ Traversal\+Info$<$ Tree\+Type $>$} \\*The \doxyref{Traversal\+Info}{p.}{classmlpack_1_1tree_1_1TraversalInfo} class holds traversal information which is used in dual-\/tree (and single-\/tree) traversals }{\pageref{classmlpack_1_1tree_1_1TraversalInfo}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Tree\+Type $>$} \\*The \doxyref{Tree\+Traits}{p.}{classmlpack_1_1tree_1_1TreeTraits} class provides compile-\/time information on the characteristics of a given tree type }{\pageref{classmlpack_1_1tree_1_1TreeTraits}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Binary\+Space\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, bound\+::\+Ball\+Bound, Split\+Type $>$ $>$} \\*This is a specialization of the Tree\+Type class to the Ball\+Tree tree type }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01BinarySpaceTree_3_01MetricType_00_01StatisticType_00_01Mat267d3b8606ae92840ddcba6834055254}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Binary\+Space\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, bound\+::\+Cell\+Bound, Split\+Type $>$ $>$} \\*This is a specialization of the Tree\+Type class to the U\+B\+Tree tree type }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01BinarySpaceTree_3_01MetricType_00_01StatisticType_00_01Mat224e09bac64c8e2ee29120d72866c234}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Binary\+Space\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, bound\+::\+Hollow\+Ball\+Bound, Split\+Type $>$ $>$} \\*This is a specialization of the Tree\+Type class to an arbitrary tree with Hollow\+Ball\+Bound (currently only the vantage point tree is supported) }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01BinarySpaceTree_3_01MetricType_00_01StatisticType_00_01Mat5e47ac61d347b64f5768de253cdf2773}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Binary\+Space\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Bound\+Type, R\+P\+Tree\+Max\+Split $>$ $>$} \\*This is a specialization of the Tree\+Type class to the max-\/split random projection tree }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01BinarySpaceTree_3_01MetricType_00_01StatisticType_00_01Mat455d0165b2c85743977ec4c0a5dd95ca}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Binary\+Space\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Bound\+Type, R\+P\+Tree\+Mean\+Split $>$ $>$} \\*This is a specialization of the Tree\+Type class to the mean-\/split random projection tree }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01BinarySpaceTree_3_01MetricType_00_01StatisticType_00_01Mat83fa92e671856c0b52f8456f1beaf6c5}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Binary\+Space\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Bound\+Type, Split\+Type $>$ $>$} \\*This is a specialization of the \doxyref{Tree\+Traits}{p.}{classmlpack_1_1tree_1_1TreeTraits} class to the \doxyref{Binary\+Space\+Tree}{p.}{classmlpack_1_1tree_1_1BinarySpaceTree} tree type }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01BinarySpaceTree_3_01MetricType_00_01StatisticType_00_01Matc82955fcc5e17376c7ac825c22d34930}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Cover\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Root\+Point\+Policy $>$ $>$} \\*The specialization of the \doxyref{Tree\+Traits}{p.}{classmlpack_1_1tree_1_1TreeTraits} class for the \doxyref{Cover\+Tree}{p.}{classmlpack_1_1tree_1_1CoverTree} tree type }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01CoverTree_3_01MetricType_00_01StatisticType_00_01MatType_00_01RootPointPolicy_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Octree$<$ Metric\+Type, Statistic\+Type, Mat\+Type $>$ $>$} \\*This is a specialization of the \doxyref{Tree\+Traits}{p.}{classmlpack_1_1tree_1_1TreeTraits} class to the \doxyref{Octree}{p.}{classmlpack_1_1tree_1_1Octree} tree type }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01Octree_3_01MetricType_00_01StatisticType_00_01MatType_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Rectangle\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, R\+Plus\+Tree\+Split$<$ Split\+Policy\+Type, Sweep\+Type $>$, Descent\+Type, Auxiliary\+Information\+Type $>$ $>$} \\*Since the R+/\+R++ tree can not have overlapping children, we should define traits for the R+/\+R++ tree }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01RectangleTree_3_01MetricType_00_01StatisticType_00_01MatTyd3300c6b7e2f56d4c1027298545eb7bf}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Rectangle\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Split\+Type, Descent\+Type, Auxiliary\+Information\+Type $>$ $>$} \\*This is a specialization of the Tree\+Type class to the \doxyref{Rectangle\+Tree}{p.}{namespacemlpack_1_1tree_1_1RectangleTree} tree type }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01RectangleTree_3_01MetricType_00_01StatisticType_00_01MatTy0686cbbcde9440cadacd80904499ea50}}{}
\item\contentsline{section}{\textbf{ Tree\+Traits$<$ Spill\+Tree$<$ Metric\+Type, Statistic\+Type, Mat\+Type, Hyperplane\+Type, Split\+Type $>$ $>$} \\*This is a specialization of the Tree\+Type class to the \doxyref{Spill\+Tree}{p.}{classmlpack_1_1tree_1_1SpillTree} tree type }{\pageref{classmlpack_1_1tree_1_1TreeTraits_3_01SpillTree_3_01MetricType_00_01StatisticType_00_01MatType_03c639ada9e7ec3c7879b4d5a2cf50982}}{}
\item\contentsline{section}{\textbf{ U\+B\+Tree\+Split$<$ Bound\+Type, Mat\+Type $>$} \\*Split a node into two parts according to the median address of points contained in the node }{\pageref{classmlpack_1_1tree_1_1UBTreeSplit}}{}
\item\contentsline{section}{\textbf{ Vantage\+Point\+Split$<$ Bound\+Type, Mat\+Type, Max\+Num\+Samples $>$} \\*The class splits a binary space partitioning tree node according to the median distance to the vantage point }{\pageref{classmlpack_1_1tree_1_1VantagePointSplit}}{}
\item\contentsline{section}{\textbf{ Vantage\+Point\+Split$<$ Bound\+Type, Mat\+Type, Max\+Num\+Samples $>$\+::\+Split\+Info} \\*A struct that contains an information about the split }{\pageref{structmlpack_1_1tree_1_1VantagePointSplit_1_1SplitInfo}}{}
\item\contentsline{section}{\textbf{ X\+Tree\+Auxiliary\+Information$<$ Tree\+Type $>$} \\*The \doxyref{X\+Tree\+Auxiliary\+Information}{p.}{classmlpack_1_1tree_1_1XTreeAuxiliaryInformation} class provides information specific to X trees for each node in a \doxyref{Rectangle\+Tree}{p.}{namespacemlpack_1_1tree_1_1RectangleTree} }{\pageref{classmlpack_1_1tree_1_1XTreeAuxiliaryInformation}}{}
\item\contentsline{section}{\textbf{ X\+Tree\+Auxiliary\+Information$<$ Tree\+Type $>$\+::\+Split\+History\+Struct} \\*The X tree requires that the tree records it\textquotesingle{}s \char`\"{}split history\char`\"{} }{\pageref{structmlpack_1_1tree_1_1XTreeAuxiliaryInformation_1_1SplitHistoryStruct}}{}
\item\contentsline{section}{\textbf{ X\+Tree\+Split} \\*A Rectangle Tree has new points inserted at the bottom }{\pageref{classmlpack_1_1tree_1_1XTreeSplit}}{}
\item\contentsline{section}{\textbf{ Binding\+Details} \\*This structure holds all of the information about bindings documentation }{\pageref{structmlpack_1_1util_1_1BindingDetails}}{}
\item\contentsline{section}{\textbf{ Example} }{\pageref{classmlpack_1_1util_1_1Example}}{}
\item\contentsline{section}{\textbf{ Is\+Std\+Vector$<$ T $>$} \\*Metaprogramming structure for vector detection }{\pageref{structmlpack_1_1util_1_1IsStdVector}}{}
\item\contentsline{section}{\textbf{ Is\+Std\+Vector$<$ std\+::vector$<$ T, A $>$ $>$} \\*Metaprogramming structure for vector detection }{\pageref{structmlpack_1_1util_1_1IsStdVector_3_01std_1_1vector_3_01T_00_01A_01_4_01_4}}{}
\item\contentsline{section}{\textbf{ Long\+Description} }{\pageref{classmlpack_1_1util_1_1LongDescription}}{}
\item\contentsline{section}{\textbf{ Null\+Out\+Stream} \\*Used for Log\+::\+Debug when not compiled with debugging symbols }{\pageref{classmlpack_1_1util_1_1NullOutStream}}{}
\item\contentsline{section}{\textbf{ Param\+Data} \\*This structure holds all of the information about a single parameter, including its value (which is set when Parse\+Command\+Line() is called) }{\pageref{structmlpack_1_1util_1_1ParamData}}{}
\item\contentsline{section}{\textbf{ Prefixed\+Out\+Stream} \\*Allows us to output to an ostream with a prefix at the beginning of each line, in the same way we would output to cout or cerr }{\pageref{classmlpack_1_1util_1_1PrefixedOutStream}}{}
\item\contentsline{section}{\textbf{ Program\+Name} }{\pageref{classmlpack_1_1util_1_1ProgramName}}{}
\item\contentsline{section}{\textbf{ See\+Also} }{\pageref{classmlpack_1_1util_1_1SeeAlso}}{}
\item\contentsline{section}{\textbf{ Short\+Description} }{\pageref{classmlpack_1_1util_1_1ShortDescription}}{}
\item\contentsline{section}{\textbf{ Train\+H\+M\+M\+Model} }{\pageref{structTrainHMMModel}}{}
\end{DoxyCompactList}

\section{Log\+Soft\+Max$<$ Input\+Data\+Type, Output\+Data\+Type $>$ Class Template Reference}
\label{classmlpack_1_1ann_1_1LogSoftMax}\index{Log\+Soft\+Max$<$ Input\+Data\+Type, Output\+Data\+Type $>$@{Log\+Soft\+Max$<$ Input\+Data\+Type, Output\+Data\+Type $>$}}


Implementation of the log softmax layer.  


\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
\textbf{ Log\+Soft\+Max} ()
\begin{DoxyCompactList}\small\item\em Create the Log\+Softmax object. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename eT $>$ }\\void \textbf{ Backward} (const arma\+::\+Mat$<$ eT $>$ \&input, const arma\+::\+Mat$<$ eT $>$ \&gy, arma\+::\+Mat$<$ eT $>$ \&g)
\begin{DoxyCompactList}\small\item\em Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f. \end{DoxyCompactList}\item 
Input\+Data\+Type \& \textbf{ Delta} () const
\begin{DoxyCompactList}\small\item\em Get the delta. \end{DoxyCompactList}\item 
Input\+Data\+Type \& \textbf{ Delta} ()
\begin{DoxyCompactList}\small\item\em Modify the delta. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Input\+Type , typename Output\+Type $>$ }\\void \textbf{ Forward} (const Input\+Type \&input, Output\+Type \&output)
\begin{DoxyCompactList}\small\item\em Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Output\+Parameter} () const
\begin{DoxyCompactList}\small\item\em Get the output parameter. \end{DoxyCompactList}\item 
Output\+Data\+Type \& \textbf{ Output\+Parameter} ()
\begin{DoxyCompactList}\small\item\em Modify the output parameter. \end{DoxyCompactList}\item 
{\footnotesize template$<$typename Archive $>$ }\\void \textbf{ serialize} (Archive \&, const uint32\+\_\+t)
\begin{DoxyCompactList}\small\item\em Serialize the layer. \end{DoxyCompactList}\end{DoxyCompactItemize}


\subsection{Detailed Description}
\subsubsection*{template$<$typename Input\+Data\+Type = arma\+::mat, typename Output\+Data\+Type = arma\+::mat$>$\newline
class mlpack\+::ann\+::\+Log\+Soft\+Max$<$ Input\+Data\+Type, Output\+Data\+Type $>$}

Implementation of the log softmax layer. 

The log softmax loss layer computes the multinomial logistic loss of the softmax of its inputs. This layer is meant to be used in combination with the negative log likelihood layer (Negative\+Log\+Likelihood\+Layer), which expects that the input contains log-\/probabilities for each class.


\begin{DoxyTemplParams}{Template Parameters}
{\em Input\+Data\+Type} & Type of the input data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
{\em Output\+Data\+Type} & Type of the output data (arma\+::colvec, arma\+::mat, arma\+::sp\+\_\+mat or arma\+::cube). \\
\hline
\end{DoxyTemplParams}


Definition at line 36 of file log\+\_\+softmax.\+hpp.



\subsection{Constructor \& Destructor Documentation}
\mbox{\label{classmlpack_1_1ann_1_1LogSoftMax_a1298935d4b0e2a0751dc55081c9515bf}} 
\index{mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}!Log\+Soft\+Max@{Log\+Soft\+Max}}
\index{Log\+Soft\+Max@{Log\+Soft\+Max}!mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}}
\subsubsection{Log\+Soft\+Max()}
{\footnotesize\ttfamily \textbf{ Log\+Soft\+Max} (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})}



Create the Log\+Softmax object. 



\subsection{Member Function Documentation}
\mbox{\label{classmlpack_1_1ann_1_1LogSoftMax_a78dbad83871f43db1975e45a9a69c376}} 
\index{mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}!Backward@{Backward}}
\index{Backward@{Backward}!mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}}
\subsubsection{Backward()}
{\footnotesize\ttfamily void Backward (\begin{DoxyParamCaption}\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{input,  }\item[{const arma\+::\+Mat$<$ eT $>$ \&}]{gy,  }\item[{arma\+::\+Mat$<$ eT $>$ \&}]{g }\end{DoxyParamCaption})}



Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f. 

Using the results from the feed forward pass.


\begin{DoxyParams}{Parameters}
{\em input} & The propagated input activation. \\
\hline
{\em gy} & The backpropagated error. \\
\hline
{\em g} & The calculated gradient. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1LogSoftMax_aa02f12f0f3e5fea14c9fecf889b3103a}} 
\index{mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}!Delta@{Delta}}
\index{Delta@{Delta}!mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}}
\subsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Input\+Data\+Type\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the delta. 



Definition at line 74 of file log\+\_\+softmax.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1LogSoftMax_a6201406598916738050e1b6caedea03b}} 
\index{mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}!Delta@{Delta}}
\index{Delta@{Delta}!mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}}
\subsubsection{Delta()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Input\+Data\+Type\& Delta (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the delta. 



Definition at line 76 of file log\+\_\+softmax.\+hpp.



References Log\+Soft\+Max$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::serialize().

\mbox{\label{classmlpack_1_1ann_1_1LogSoftMax_a09440df0a90bdcc766e56e097d91205b}} 
\index{mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}!Forward@{Forward}}
\index{Forward@{Forward}!mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}}
\subsubsection{Forward()}
{\footnotesize\ttfamily void Forward (\begin{DoxyParamCaption}\item[{const Input\+Type \&}]{input,  }\item[{Output\+Type \&}]{output }\end{DoxyParamCaption})}



Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f. 


\begin{DoxyParams}{Parameters}
{\em input} & Input data used for evaluating the specified function. \\
\hline
{\em output} & Resulting output activation. \\
\hline
\end{DoxyParams}
\mbox{\label{classmlpack_1_1ann_1_1LogSoftMax_a8bae962cc603d1cab8d80ec78f8d505d}} 
\index{mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}}
\subsubsection{Output\+Parameter()\hspace{0.1cm}{\footnotesize\ttfamily [1/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption}) const\hspace{0.3cm}{\ttfamily [inline]}}



Get the output parameter. 



Definition at line 69 of file log\+\_\+softmax.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1LogSoftMax_a21d5f745f02c709625a4ee0907f004a5}} 
\index{mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}!Output\+Parameter@{Output\+Parameter}}
\index{Output\+Parameter@{Output\+Parameter}!mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}}
\subsubsection{Output\+Parameter()\hspace{0.1cm}{\footnotesize\ttfamily [2/2]}}
{\footnotesize\ttfamily Output\+Data\+Type\& Output\+Parameter (\begin{DoxyParamCaption}{ }\end{DoxyParamCaption})\hspace{0.3cm}{\ttfamily [inline]}}



Modify the output parameter. 



Definition at line 71 of file log\+\_\+softmax.\+hpp.

\mbox{\label{classmlpack_1_1ann_1_1LogSoftMax_aa2ccb5a0533a6ba0abe6dfc1f98fbafb}} 
\index{mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}!serialize@{serialize}}
\index{serialize@{serialize}!mlpack\+::ann\+::\+Log\+Soft\+Max@{mlpack\+::ann\+::\+Log\+Soft\+Max}}
\subsubsection{serialize()}
{\footnotesize\ttfamily void serialize (\begin{DoxyParamCaption}\item[{Archive \&}]{,  }\item[{const uint32\+\_\+t}]{ }\end{DoxyParamCaption})}



Serialize the layer. 



Referenced by Log\+Soft\+Max$<$ Input\+Data\+Type, Output\+Data\+Type $>$\+::\+Delta().



The documentation for this class was generated from the following file\+:\begin{DoxyCompactItemize}
\item 
/home/aakash/mlpack/src/mlpack/methods/ann/layer/\textbf{ log\+\_\+softmax.\+hpp}\end{DoxyCompactItemize}

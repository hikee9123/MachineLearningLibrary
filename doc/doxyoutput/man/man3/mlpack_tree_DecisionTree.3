.TH "DecisionTree< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion >" 3 "Sun Aug 22 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
DecisionTree< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion > \- This class implements a generic decision tree learner\&.  

.SH SYNOPSIS
.br
.PP
.PP
Inherits AuxiliarySplitInfo< FitnessFunction >, and AuxiliarySplitInfo< FitnessFunction >\&.
.SS "Public Types"

.in +1c
.ti -1c
.RI "typedef CategoricalSplitType< FitnessFunction > \fBCategoricalSplit\fP"
.br
.RI "Allow access to the categorical split type\&. "
.ti -1c
.RI "typedef DimensionSelectionType \fBDimensionSelection\fP"
.br
.RI "Allow access to the dimension selection type\&. "
.ti -1c
.RI "typedef NumericSplitType< FitnessFunction > \fBNumericSplit\fP"
.br
.RI "Allow access to the numeric split type\&. "
.in -1c
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "template<typename MatType , typename LabelsType > \fBDecisionTree\fP (MatType data, const \fBdata::DatasetInfo\fP &datasetInfo, LabelsType labels, const size_t numClasses, const size_t minimumLeafSize=10, const double minimumGainSplit=1e\-7, const size_t maximumDepth=0, DimensionSelectionType dimensionSelector=DimensionSelectionType())"
.br
.RI "Construct the decision tree on the given data and labels, where the data can be both numeric and categorical\&. "
.ti -1c
.RI "template<typename MatType , typename LabelsType > \fBDecisionTree\fP (MatType data, LabelsType labels, const size_t numClasses, const size_t minimumLeafSize=10, const double minimumGainSplit=1e\-7, const size_t maximumDepth=0, DimensionSelectionType dimensionSelector=DimensionSelectionType())"
.br
.RI "Construct the decision tree on the given data and labels, assuming that the data is all of the numeric type\&. "
.ti -1c
.RI "template<typename MatType , typename LabelsType , typename WeightsType > \fBDecisionTree\fP (MatType data, const \fBdata::DatasetInfo\fP &datasetInfo, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize=10, const double minimumGainSplit=1e\-7, const size_t maximumDepth=0, DimensionSelectionType dimensionSelector=DimensionSelectionType(), const std::enable_if_t< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > *=0)"
.br
.RI "Construct the decision tree on the given data and labels with weights, where the data can be both numeric and categorical\&. "
.ti -1c
.RI "template<typename MatType , typename LabelsType , typename WeightsType > \fBDecisionTree\fP (const \fBDecisionTree\fP &other, MatType data, const \fBdata::DatasetInfo\fP &datasetInfo, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize=10, const double minimumGainSplit=1e\-7, const std::enable_if_t< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > *=0)"
.br
.RI "Using the hyperparameters of another decision tree, train on the given data and labels with weights, where the data can be both numeric and categorical\&. "
.ti -1c
.RI "template<typename MatType , typename LabelsType , typename WeightsType > \fBDecisionTree\fP (MatType data, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize=10, const double minimumGainSplit=1e\-7, const size_t maximumDepth=0, DimensionSelectionType dimensionSelector=DimensionSelectionType(), const std::enable_if_t< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > *=0)"
.br
.RI "Construct the decision tree on the given data and labels with weights, assuming that the data is all of the numeric type\&. "
.ti -1c
.RI "template<typename MatType , typename LabelsType , typename WeightsType > \fBDecisionTree\fP (const \fBDecisionTree\fP &other, MatType data, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize=10, const double minimumGainSplit=1e\-7, const size_t maximumDepth=0, DimensionSelectionType dimensionSelector=DimensionSelectionType(), const std::enable_if_t< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > *=0)"
.br
.RI "Take ownership of another decision tree and train on the given data and labels with weights, assuming that the data is all of the numeric type\&. "
.ti -1c
.RI "\fBDecisionTree\fP (const size_t numClasses=1)"
.br
.RI "Construct a decision tree without training it\&. "
.ti -1c
.RI "\fBDecisionTree\fP (const \fBDecisionTree\fP &other)"
.br
.RI "Copy another tree\&. "
.ti -1c
.RI "\fBDecisionTree\fP (\fBDecisionTree\fP &&other)"
.br
.RI "Take ownership of another tree\&. "
.ti -1c
.RI "\fB~DecisionTree\fP ()"
.br
.RI "Clean up memory\&. "
.ti -1c
.RI "template<typename VecType > size_t \fBCalculateDirection\fP (const VecType &point) const"
.br
.RI "Given a point and that this node is not a leaf, calculate the index of the child node this point would go towards\&. "
.ti -1c
.RI "const \fBDecisionTree\fP & \fBChild\fP (const size_t i) const"
.br
.RI "Get the child of the given index\&. "
.ti -1c
.RI "\fBDecisionTree\fP & \fBChild\fP (const size_t i)"
.br
.RI "Modify the child of the given index (be careful!)\&. "
.ti -1c
.RI "template<typename VecType > size_t \fBClassify\fP (const VecType &point) const"
.br
.RI "Classify the given point, using the entire tree\&. "
.ti -1c
.RI "template<typename VecType > void \fBClassify\fP (const VecType &point, size_t &prediction, arma::vec &probabilities) const"
.br
.RI "Classify the given point and also return estimates of the probability for each class in the given vector\&. "
.ti -1c
.RI "template<typename MatType > void \fBClassify\fP (const MatType &data, arma::Row< size_t > &predictions) const"
.br
.RI "Classify the given points, using the entire tree\&. "
.ti -1c
.RI "template<typename MatType > void \fBClassify\fP (const MatType &data, arma::Row< size_t > &predictions, arma::mat &probabilities) const"
.br
.RI "Classify the given points and also return estimates of the probabilities for each class in the given matrix\&. "
.ti -1c
.RI "size_t \fBNumChildren\fP () const"
.br
.RI "Get the number of children\&. "
.ti -1c
.RI "size_t \fBNumClasses\fP () const"
.br
.RI "Get the number of classes in the tree\&. "
.ti -1c
.RI "\fBDecisionTree\fP & \fBoperator=\fP (const \fBDecisionTree\fP &other)"
.br
.RI "Copy another tree\&. "
.ti -1c
.RI "\fBDecisionTree\fP & \fBoperator=\fP (\fBDecisionTree\fP &&other)"
.br
.RI "Take ownership of another tree\&. "
.ti -1c
.RI "template<typename Archive > void \fBserialize\fP (Archive &ar, const uint32_t)"
.br
.RI "Serialize the tree\&. "
.ti -1c
.RI "size_t \fBSplitDimension\fP () const"
.br
.RI "Get the split dimension (only meaningful if this is a non-leaf in a trained tree)\&. "
.ti -1c
.RI "template<typename MatType , typename LabelsType > double \fBTrain\fP (MatType data, const \fBdata::DatasetInfo\fP &datasetInfo, LabelsType labels, const size_t numClasses, const size_t minimumLeafSize=10, const double minimumGainSplit=1e\-7, const size_t maximumDepth=0, DimensionSelectionType dimensionSelector=DimensionSelectionType())"
.br
.RI "Train the decision tree on the given data\&. "
.ti -1c
.RI "template<typename MatType , typename LabelsType > double \fBTrain\fP (MatType data, LabelsType labels, const size_t numClasses, const size_t minimumLeafSize=10, const double minimumGainSplit=1e\-7, const size_t maximumDepth=0, DimensionSelectionType dimensionSelector=DimensionSelectionType())"
.br
.RI "Train the decision tree on the given data, assuming that all dimensions are numeric\&. "
.ti -1c
.RI "template<typename MatType , typename LabelsType , typename WeightsType > double \fBTrain\fP (MatType data, const \fBdata::DatasetInfo\fP &datasetInfo, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize=10, const double minimumGainSplit=1e\-7, const size_t maximumDepth=0, DimensionSelectionType dimensionSelector=DimensionSelectionType(), const std::enable_if_t< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > *=0)"
.br
.RI "Train the decision tree on the given weighted data\&. "
.ti -1c
.RI "template<typename MatType , typename LabelsType , typename WeightsType > double \fBTrain\fP (MatType data, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize=10, const double minimumGainSplit=1e\-7, const size_t maximumDepth=0, DimensionSelectionType dimensionSelector=DimensionSelectionType(), const std::enable_if_t< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > *=0)"
.br
.RI "Train the decision tree on the given weighted data, assuming that all dimensions are numeric\&. "
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<typename FitnessFunction = GiniGain, template< typename > class NumericSplitType = BestBinaryNumericSplit, template< typename > class CategoricalSplitType = AllCategoricalSplit, typename DimensionSelectionType = AllDimensionSelect, bool NoRecursion = false>
.br
class mlpack::tree::DecisionTree< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion >"
This class implements a generic decision tree learner\&. 

Its behavior can be controlled via its template arguments\&.
.PP
The class inherits from the auxiliary split information in order to prevent an empty auxiliary split information struct from taking any extra size\&. 
.PP
Definition at line 40 of file decision_tree\&.hpp\&.
.SH "Member Typedef Documentation"
.PP 
.SS "typedef CategoricalSplitType<FitnessFunction> \fBCategoricalSplit\fP"

.PP
Allow access to the categorical split type\&. 
.PP
Definition at line 48 of file decision_tree\&.hpp\&.
.SS "typedef DimensionSelectionType \fBDimensionSelection\fP"

.PP
Allow access to the dimension selection type\&. 
.PP
Definition at line 50 of file decision_tree\&.hpp\&.
.SS "typedef NumericSplitType<FitnessFunction> \fBNumericSplit\fP"

.PP
Allow access to the numeric split type\&. 
.PP
Definition at line 46 of file decision_tree\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBDecisionTree\fP (MatType data, const \fBdata::DatasetInfo\fP & datasetInfo, LabelsType labels, const size_t numClasses, const size_t minimumLeafSize = \fC10\fP, const double minimumGainSplit = \fC1e\-7\fP, const size_t maximumDepth = \fC0\fP, DimensionSelectionType dimensionSelector = \fCDimensionSelectionType()\fP)"

.PP
Construct the decision tree on the given data and labels, where the data can be both numeric and categorical\&. Setting minimumLeafSize and minimumGainSplit too small may cause the tree to overfit, but setting them too large may cause it to underfit\&.
.PP
Use std::move if data or labels are no longer needed to avoid copies\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Dataset to train on\&. 
.br
\fIdatasetInfo\fP Type information for each dimension of the dataset\&. 
.br
\fIlabels\fP Labels for each training point\&. 
.br
\fInumClasses\fP Number of classes in the dataset\&. 
.br
\fIminimumLeafSize\fP Minimum number of points in each leaf node\&. 
.br
\fIminimumGainSplit\fP Minimum gain for the node to split\&. 
.br
\fImaximumDepth\fP Maximum depth for the tree\&. 
.br
\fIdimensionSelector\fP Instantiated dimension selection policy\&. 
.RE
.PP

.SS "\fBDecisionTree\fP (MatType data, LabelsType labels, const size_t numClasses, const size_t minimumLeafSize = \fC10\fP, const double minimumGainSplit = \fC1e\-7\fP, const size_t maximumDepth = \fC0\fP, DimensionSelectionType dimensionSelector = \fCDimensionSelectionType()\fP)"

.PP
Construct the decision tree on the given data and labels, assuming that the data is all of the numeric type\&. Setting minimumLeafSize and minimumGainSplit too small may cause the tree to overfit, but setting them too large may cause it to underfit\&.
.PP
Use std::move if data or labels are no longer needed to avoid copies\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Dataset to train on\&. 
.br
\fIlabels\fP Labels for each training point\&. 
.br
\fInumClasses\fP Number of classes in the dataset\&. 
.br
\fIminimumLeafSize\fP Minimum number of points in each leaf node\&. 
.br
\fIminimumGainSplit\fP Minimum gain for the node to split\&. 
.br
\fImaximumDepth\fP Maximum depth for the tree\&. 
.br
\fIdimensionSelector\fP Instantiated dimension selection policy\&. 
.RE
.PP

.SS "\fBDecisionTree\fP (MatType data, const \fBdata::DatasetInfo\fP & datasetInfo, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize = \fC10\fP, const double minimumGainSplit = \fC1e\-7\fP, const size_t maximumDepth = \fC0\fP, DimensionSelectionType dimensionSelector = \fCDimensionSelectionType()\fP, const \fBstd::enable_if_t\fP< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > * = \fC0\fP)"

.PP
Construct the decision tree on the given data and labels with weights, where the data can be both numeric and categorical\&. Setting minimumLeafSize and minimumGainSplit too small may cause the tree to overfit, but setting them too large may cause it to underfit\&.
.PP
Use std::move if data, labels or weights are no longer needed to avoid copies\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Dataset to train on\&. 
.br
\fIdatasetInfo\fP Type information for each dimension of the dataset\&. 
.br
\fIlabels\fP Labels for each training point\&. 
.br
\fInumClasses\fP Number of classes in the dataset\&. 
.br
\fIweights\fP The weight list of given label\&. 
.br
\fIminimumLeafSize\fP Minimum number of points in each leaf node\&. 
.br
\fIminimumGainSplit\fP Minimum gain for the node to split\&. 
.br
\fImaximumDepth\fP Maximum depth for the tree\&. 
.br
\fIdimensionSelector\fP Instantiated dimension selection policy\&. 
.RE
.PP

.SS "\fBDecisionTree\fP (const \fBDecisionTree\fP< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion > & other, MatType data, const \fBdata::DatasetInfo\fP & datasetInfo, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize = \fC10\fP, const double minimumGainSplit = \fC1e\-7\fP, const \fBstd::enable_if_t\fP< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > * = \fC0\fP)"

.PP
Using the hyperparameters of another decision tree, train on the given data and labels with weights, where the data can be both numeric and categorical\&. Setting minimumLeafSize and minimumGainSplit too small may cause the tree to overfit, but setting them too large may cause it to underfit\&.
.PP
Use std::move if data, labels or weights are no longer needed to avoid copies\&.
.PP
\fBParameters:\fP
.RS 4
\fIother\fP Tree to take ownership of\&. 
.br
\fIdata\fP Dataset to train on\&. 
.br
\fIdatasetInfo\fP Type information for each dimension of the dataset\&. 
.br
\fIlabels\fP Labels for each training point\&. 
.br
\fInumClasses\fP Number of classes in the dataset\&. 
.br
\fIweights\fP The weight list of given label\&. 
.br
\fIminimumLeafSize\fP Minimum number of points in each leaf node\&. 
.br
\fIminimumGainSplit\fP Minimum gain for the node to split\&. 
.RE
.PP

.SS "\fBDecisionTree\fP (MatType data, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize = \fC10\fP, const double minimumGainSplit = \fC1e\-7\fP, const size_t maximumDepth = \fC0\fP, DimensionSelectionType dimensionSelector = \fCDimensionSelectionType()\fP, const \fBstd::enable_if_t\fP< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > * = \fC0\fP)"

.PP
Construct the decision tree on the given data and labels with weights, assuming that the data is all of the numeric type\&. Setting minimumLeafSize and minimumGainSplit too small may cause the tree to overfit, but setting them too large may cause it to underfit\&.
.PP
Use std::move if data, labels or weights are no longer needed to avoid copies\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Dataset to train on\&. 
.br
\fIlabels\fP Labels for each training point\&. 
.br
\fInumClasses\fP Number of classes in the dataset\&. 
.br
\fIweights\fP The Weight list of given labels\&. 
.br
\fIminimumLeafSize\fP Minimum number of points in each leaf node\&. 
.br
\fIminimumGainSplit\fP Minimum gain for the node to split\&. 
.br
\fImaximumDepth\fP Maximum depth for the tree\&. 
.br
\fIdimensionSelector\fP Instantiated dimension selection policy\&. 
.RE
.PP

.SS "\fBDecisionTree\fP (const \fBDecisionTree\fP< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion > & other, MatType data, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize = \fC10\fP, const double minimumGainSplit = \fC1e\-7\fP, const size_t maximumDepth = \fC0\fP, DimensionSelectionType dimensionSelector = \fCDimensionSelectionType()\fP, const \fBstd::enable_if_t\fP< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > * = \fC0\fP)"

.PP
Take ownership of another decision tree and train on the given data and labels with weights, assuming that the data is all of the numeric type\&. Setting minimumLeafSize and minimumGainSplit too small may cause the tree to overfit, but setting them too large may cause it to underfit\&.
.PP
Use std::move if data, labels or weights are no longer needed to avoid copies\&. 
.PP
\fBParameters:\fP
.RS 4
\fIother\fP Tree to take ownership of\&. 
.br
\fIdata\fP Dataset to train on\&. 
.br
\fIlabels\fP Labels for each training point\&. 
.br
\fInumClasses\fP Number of classes in the dataset\&. 
.br
\fIweights\fP The Weight list of given labels\&. 
.br
\fIminimumLeafSize\fP Minimum number of points in each leaf node\&. 
.br
\fIminimumGainSplit\fP Minimum gain for the node to split\&. 
.br
\fImaximumDepth\fP Maximum depth for the tree\&. 
.br
\fIdimensionSelector\fP Instantiated dimension selection policy\&. 
.RE
.PP

.SS "\fBDecisionTree\fP (const size_t numClasses = \fC1\fP)"

.PP
Construct a decision tree without training it\&. It will be a leaf node with equal probabilities for each class\&.
.PP
\fBParameters:\fP
.RS 4
\fInumClasses\fP Number of classes in the dataset\&. 
.RE
.PP

.SS "\fBDecisionTree\fP (const \fBDecisionTree\fP< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion > & other)"

.PP
Copy another tree\&. This may use a lot of memory---be sure that it's what you want to do\&.
.PP
\fBParameters:\fP
.RS 4
\fIother\fP Tree to copy\&. 
.RE
.PP

.SS "\fBDecisionTree\fP (\fBDecisionTree\fP< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion > && other)"

.PP
Take ownership of another tree\&. 
.PP
\fBParameters:\fP
.RS 4
\fIother\fP Tree to take ownership of\&. 
.RE
.PP

.SS "~\fBDecisionTree\fP ()"

.PP
Clean up memory\&. 
.SH "Member Function Documentation"
.PP 
.SS "size_t CalculateDirection (const VecType & point) const"

.PP
Given a point and that this node is not a leaf, calculate the index of the child node this point would go towards\&. This method is primarily used by the \fBClassify()\fP function, but it can be used in a standalone sense too\&.
.PP
\fBParameters:\fP
.RS 4
\fIpoint\fP Point to classify\&. 
.RE
.PP

.PP
Referenced by DecisionTree< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion >::SplitDimension()\&.
.SS "const \fBDecisionTree\fP& Child (const size_t i) const\fC [inline]\fP"

.PP
Get the child of the given index\&. 
.PP
Definition at line 459 of file decision_tree\&.hpp\&.
.SS "\fBDecisionTree\fP& Child (const size_t i)\fC [inline]\fP"

.PP
Modify the child of the given index (be careful!)\&. 
.PP
Definition at line 461 of file decision_tree\&.hpp\&.
.SS "size_t Classify (const VecType & point) const"

.PP
Classify the given point, using the entire tree\&. The predicted label is returned\&.
.PP
\fBParameters:\fP
.RS 4
\fIpoint\fP Point to classify\&. 
.RE
.PP

.SS "void Classify (const VecType & point, size_t & prediction, arma::vec & probabilities) const"

.PP
Classify the given point and also return estimates of the probability for each class in the given vector\&. 
.PP
\fBParameters:\fP
.RS 4
\fIpoint\fP Point to classify\&. 
.br
\fIprediction\fP This will be set to the predicted class of the point\&. 
.br
\fIprobabilities\fP This will be filled with class probabilities for the point\&. 
.RE
.PP

.SS "void Classify (const MatType & data, arma::Row< size_t > & predictions) const"

.PP
Classify the given points, using the entire tree\&. The predicted labels for each point are stored in the given vector\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Set of points to classify\&. 
.br
\fIpredictions\fP This will be filled with predictions for each point\&. 
.RE
.PP

.SS "void Classify (const MatType & data, arma::Row< size_t > & predictions, arma::mat & probabilities) const"

.PP
Classify the given points and also return estimates of the probabilities for each class in the given matrix\&. The predicted labels for each point are stored in the given vector\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Set of points to classify\&. 
.br
\fIpredictions\fP This will be filled with predictions for each point\&. 
.br
\fIprobabilities\fP This will be filled with class probabilities for each point\&. 
.RE
.PP

.SS "size_t NumChildren () const\fC [inline]\fP"

.PP
Get the number of children\&. 
.PP
Definition at line 456 of file decision_tree\&.hpp\&.
.SS "size_t NumClasses () const"

.PP
Get the number of classes in the tree\&. 
.PP
Referenced by DecisionTree< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion >::SplitDimension()\&.
.SS "\fBDecisionTree\fP& operator= (const \fBDecisionTree\fP< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion > & other)"

.PP
Copy another tree\&. This may use a lot of memory---be sure that it's what you want to do\&.
.PP
\fBParameters:\fP
.RS 4
\fIother\fP Tree to copy\&. 
.RE
.PP

.SS "\fBDecisionTree\fP& operator= (\fBDecisionTree\fP< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion > && other)"

.PP
Take ownership of another tree\&. 
.PP
\fBParameters:\fP
.RS 4
\fIother\fP Tree to take ownership of\&. 
.RE
.PP

.SS "void serialize (Archive & ar, const uint32_t)"

.PP
Serialize the tree\&. 
.SS "size_t SplitDimension () const\fC [inline]\fP"

.PP
Get the split dimension (only meaningful if this is a non-leaf in a trained tree)\&. 
.PP
Definition at line 465 of file decision_tree\&.hpp\&.
.PP
References DecisionTree< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion >::CalculateDirection(), DecisionTree< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion >::NumClasses(), and DecisionTree< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion >::Train()\&.
.SS "double Train (MatType data, const \fBdata::DatasetInfo\fP & datasetInfo, LabelsType labels, const size_t numClasses, const size_t minimumLeafSize = \fC10\fP, const double minimumGainSplit = \fC1e\-7\fP, const size_t maximumDepth = \fC0\fP, DimensionSelectionType dimensionSelector = \fCDimensionSelectionType()\fP)"

.PP
Train the decision tree on the given data\&. This will overwrite the existing model\&. The data may have numeric and categorical types, specified by the datasetInfo parameter\&. Setting minimumLeafSize and minimumGainSplit too small may cause the tree to overfit, but setting them too large may cause it to underfit\&.
.PP
Use std::move if data or labels are no longer needed to avoid copies\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Dataset to train on\&. 
.br
\fIdatasetInfo\fP Type information for each dimension\&. 
.br
\fIlabels\fP Labels for each training point\&. 
.br
\fInumClasses\fP Number of classes in the dataset\&. 
.br
\fIminimumLeafSize\fP Minimum number of points in each leaf node\&. 
.br
\fIminimumGainSplit\fP Minimum gain for the node to split\&. 
.br
\fImaximumDepth\fP Maximum depth for the tree\&. 
.br
\fIdimensionSelector\fP Instantiated dimension selection policy\&. 
.RE
.PP
\fBReturns:\fP
.RS 4
The final entropy of decision tree\&. 
.RE
.PP

.PP
Referenced by DecisionTree< FitnessFunction, NumericSplitType, CategoricalSplitType, DimensionSelectionType, NoRecursion >::SplitDimension()\&.
.SS "double Train (MatType data, LabelsType labels, const size_t numClasses, const size_t minimumLeafSize = \fC10\fP, const double minimumGainSplit = \fC1e\-7\fP, const size_t maximumDepth = \fC0\fP, DimensionSelectionType dimensionSelector = \fCDimensionSelectionType()\fP)"

.PP
Train the decision tree on the given data, assuming that all dimensions are numeric\&. This will overwrite the given model\&. Setting minimumLeafSize and minimumGainSplit too small may cause the tree to overfit, but setting them too large may cause it to underfit\&.
.PP
Use std::move if data or labels are no longer needed to avoid copies\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Dataset to train on\&. 
.br
\fIlabels\fP Labels for each training point\&. 
.br
\fInumClasses\fP Number of classes in the dataset\&. 
.br
\fIminimumLeafSize\fP Minimum number of points in each leaf node\&. 
.br
\fIminimumGainSplit\fP Minimum gain for the node to split\&. 
.br
\fImaximumDepth\fP Maximum depth for the tree\&. 
.br
\fIdimensionSelector\fP Instantiated dimension selection policy\&. 
.RE
.PP
\fBReturns:\fP
.RS 4
The final entropy of decision tree\&. 
.RE
.PP

.SS "double Train (MatType data, const \fBdata::DatasetInfo\fP & datasetInfo, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize = \fC10\fP, const double minimumGainSplit = \fC1e\-7\fP, const size_t maximumDepth = \fC0\fP, DimensionSelectionType dimensionSelector = \fCDimensionSelectionType()\fP, const \fBstd::enable_if_t\fP< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > * = \fC0\fP)"

.PP
Train the decision tree on the given weighted data\&. This will overwrite the existing model\&. The data may have numeric and categorical types, specified by the datasetInfo parameter\&. Setting minimumLeafSize and minimumGainSplit too small may cause the tree to overfit, but setting them too large may cause it to underfit\&.
.PP
Use std::move if data, labels or weights are no longer needed to avoid copies\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Dataset to train on\&. 
.br
\fIdatasetInfo\fP Type information for each dimension\&. 
.br
\fIlabels\fP Labels for each training point\&. 
.br
\fInumClasses\fP Number of classes in the dataset\&. 
.br
\fIweights\fP Weights of all the labels 
.br
\fIminimumLeafSize\fP Minimum number of points in each leaf node\&. 
.br
\fIminimumGainSplit\fP Minimum gain for the node to split\&. 
.br
\fImaximumDepth\fP Maximum depth for the tree\&. 
.br
\fIdimensionSelector\fP Instantiated dimension selection policy\&. 
.RE
.PP
\fBReturns:\fP
.RS 4
The final entropy of decision tree\&. 
.RE
.PP

.SS "double Train (MatType data, LabelsType labels, const size_t numClasses, WeightsType weights, const size_t minimumLeafSize = \fC10\fP, const double minimumGainSplit = \fC1e\-7\fP, const size_t maximumDepth = \fC0\fP, DimensionSelectionType dimensionSelector = \fCDimensionSelectionType()\fP, const \fBstd::enable_if_t\fP< arma::is_arma_type< typename std::remove_reference< WeightsType >::type >::value > * = \fC0\fP)"

.PP
Train the decision tree on the given weighted data, assuming that all dimensions are numeric\&. This will overwrite the given model\&. Setting minimumLeafSize and minimumGainSplit too small may cause the tree to overfit, but setting them too large may cause it to underfit\&.
.PP
Use std::move if data, labels or weights are no longer needed to avoid copies\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Dataset to train on\&. 
.br
\fIlabels\fP Labels for each training point\&. 
.br
\fInumClasses\fP Number of classes in the dataset\&. 
.br
\fIweights\fP Weights of all the labels 
.br
\fIminimumLeafSize\fP Minimum number of points in each leaf node\&. 
.br
\fIminimumGainSplit\fP Minimum gain for the node to split\&. 
.br
\fImaximumDepth\fP Maximum depth for the tree\&. 
.br
\fIdimensionSelector\fP Instantiated dimension selection policy\&. 
.RE
.PP
\fBReturns:\fP
.RS 4
The final entropy of decision tree\&. 
.RE
.PP


.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.

.TH "SparseCoding" 3 "Thu Jun 24 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
SparseCoding \- An implementation of Sparse Coding with Dictionary Learning that achieves sparsity via an l1-norm regularizer on the codes (LASSO) or an (l1+l2)-norm regularizer on the codes (the Elastic Net)\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "template<typename DictionaryInitializer  = DataDependentRandomInitializer> \fBSparseCoding\fP (const arma::mat &data, const size_t atoms, const double lambda1, const double lambda2=0, const size_t maxIterations=0, const double objTolerance=0\&.01, const double newtonTolerance=1e\-6, const DictionaryInitializer &initializer=DictionaryInitializer())"
.br
.RI "Set the parameters to \fBSparseCoding\fP\&. "
.ti -1c
.RI "\fBSparseCoding\fP (const size_t atoms=0, const double lambda1=0, const double lambda2=0, const size_t maxIterations=0, const double objTolerance=0\&.01, const double newtonTolerance=1e\-6)"
.br
.RI "Set the parameters to \fBSparseCoding\fP\&. "
.ti -1c
.RI "size_t \fBAtoms\fP () const"
.br
.RI "Access the number of atoms\&. "
.ti -1c
.RI "size_t & \fBAtoms\fP ()"
.br
.RI "Modify the number of atoms\&. "
.ti -1c
.RI "const arma::mat & \fBDictionary\fP () const"
.br
.RI "Access the dictionary\&. "
.ti -1c
.RI "arma::mat & \fBDictionary\fP ()"
.br
.RI "Modify the dictionary\&. "
.ti -1c
.RI "void \fBEncode\fP (const arma::mat &data, arma::mat &codes)"
.br
.RI "Sparse code each point in the given dataset via LARS, using the current dictionary and store the encoded data in the codes matrix\&. "
.ti -1c
.RI "double \fBLambda1\fP () const"
.br
.RI "Access the L1 regularization term\&. "
.ti -1c
.RI "double & \fBLambda1\fP ()"
.br
.RI "Modify the L1 regularization term\&. "
.ti -1c
.RI "double \fBLambda2\fP () const"
.br
.RI "Access the L2 regularization term\&. "
.ti -1c
.RI "double & \fBLambda2\fP ()"
.br
.RI "Modify the L2 regularization term\&. "
.ti -1c
.RI "size_t \fBMaxIterations\fP () const"
.br
.RI "Get the maximum number of iterations\&. "
.ti -1c
.RI "size_t & \fBMaxIterations\fP ()"
.br
.RI "Modify the maximum number of iterations\&. "
.ti -1c
.RI "double \fBNewtonTolerance\fP () const"
.br
.RI "Get the tolerance for Newton's method (dictionary optimization step)\&. "
.ti -1c
.RI "double & \fBNewtonTolerance\fP ()"
.br
.RI "Modify the tolerance for Newton's method (dictionary optimization step)\&. "
.ti -1c
.RI "double \fBObjective\fP (const arma::mat &data, const arma::mat &codes) const"
.br
.RI "Compute the objective function\&. "
.ti -1c
.RI "double \fBObjTolerance\fP () const"
.br
.RI "Get the objective tolerance\&. "
.ti -1c
.RI "double & \fBObjTolerance\fP ()"
.br
.RI "Modify the objective tolerance\&. "
.ti -1c
.RI "double \fBOptimizeDictionary\fP (const arma::mat &data, const arma::mat &codes, const arma::uvec &adjacencies)"
.br
.RI "Learn dictionary via Newton method based on Lagrange dual\&. "
.ti -1c
.RI "void \fBProjectDictionary\fP ()"
.br
.RI "Project each atom of the dictionary back onto the unit ball, if necessary\&. "
.ti -1c
.RI "template<typename Archive > void \fBserialize\fP (Archive &ar, const uint32_t)"
.br
.RI "Serialize the sparse coding model\&. "
.ti -1c
.RI "template<typename DictionaryInitializer  = DataDependentRandomInitializer> double \fBTrain\fP (const arma::mat &data, const DictionaryInitializer &initializer=DictionaryInitializer())"
.br
.RI "Train the sparse coding model on the given dataset\&. "
.in -1c
.SH "Detailed Description"
.PP 
An implementation of Sparse Coding with Dictionary Learning that achieves sparsity via an l1-norm regularizer on the codes (LASSO) or an (l1+l2)-norm regularizer on the codes (the Elastic Net)\&. 

Let d be the number of dimensions in the original space, m the number of training points, and k the number of atoms in the dictionary (the dimension of the learned feature space)\&. The training data X is a d-by-m matrix where each column is a point and each row is a dimension\&. The dictionary D is a d-by-k matrix, and the sparse codes matrix Z is a k-by-m matrix\&. This program seeks to minimize the objective:
.PP
\[ \min_{D,Z} 0.5 ||X - D Z||_{F}^2\ + \lambda_1 \sum_{i=1}^m ||Z_i||_1 + 0.5 \lambda_2 \sum_{i=1}^m ||Z_i||_2^2 \].PP
subject to $ ||D_j||_2 <= 1 $ for $ 1 <= j <= k $ where typically $ lambda_1 > 0 $ and $ lambda_2 = 0 $\&.
.PP
This problem is solved by an algorithm that alternates between a dictionary learning step and a sparse coding step\&. The dictionary learning step updates the dictionary D using a Newton method based on the Lagrange dual (see the paper below for details)\&. The sparse coding step involves solving a large number of sparse linear regression problems; this can be done efficiently using LARS, an algorithm that can solve the LASSO or the Elastic Net (papers below)\&.
.PP
Here are those papers:
.PP
.PP
.nf
@incollection{lee2007efficient,
  title = {Efficient sparse coding algorithms},
  author = {Honglak Lee and Alexis Battle and Rajat Raina and Andrew Y\&. Ng},
  booktitle = {Advances in Neural Information Processing Systems 19},
  editor = {B\&. Sch\"{o}lkopf and J\&. Platt and T\&. Hoffman},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  pages = {801--808},
  year = {2007}
}
.fi
.PP
.PP
.PP
.nf
@article{efron2004least,
  title={Least angle regression},
  author={Efron, B\&. and Hastie, T\&. and Johnstone, I\&. and Tibshirani, R\&.},
  journal={The Annals of statistics},
  volume={32},
  number={2},
  pages={407--499},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}
.fi
.PP
.PP
.PP
.nf
@article{zou2005regularization,
  title={Regularization and variable selection via the elastic net},
  author={Zou, H\&. and Hastie, T\&.},
  journal={Journal of the Royal Statistical Society Series B},
  volume={67},
  number={2},
  pages={301--320},
  year={2005},
  publisher={Royal Statistical Society}
}
.fi
.PP
.PP
Note that the implementation here does not use the feature-sign search algorithm from Honglak Lee's paper, but instead the LARS algorithm suggested in that paper\&.
.PP
When \fBTrain()\fP is called, the dictionary is initialized using the DictionaryInitializationPolicy class\&. Possible choices include the \fBRandomInitializer\fP, which provides an entirely random dictionary, the \fBDataDependentRandomInitializer\fP, which provides a random dictionary based loosely on characteristics of the dataset, and the \fBNothingInitializer\fP, which does not initialize the dictionary -- instead, the user should set the dictionary using the \fBDictionary()\fP mutator method\&.
.PP
Once a dictionary is trained with \fBTrain()\fP, another matrix may be encoded with the \fBEncode()\fP function\&.
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIDictionaryInitializationPolicy\fP The class to use to initialize the dictionary; must have 'void Initialize(const arma::mat& data, arma::mat& dictionary)' function\&. 
.RE
.PP

.PP
Definition at line 115 of file sparse_coding\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBSparseCoding\fP (const arma::mat & data, const size_t atoms, const double lambda1, const double lambda2 = \fC0\fP, const size_t maxIterations = \fC0\fP, const double objTolerance = \fC0\&.01\fP, const double newtonTolerance = \fC1e\-6\fP, const DictionaryInitializer & initializer = \fCDictionaryInitializer()\fP)"

.PP
Set the parameters to \fBSparseCoding\fP\&. lambda2 defaults to 0\&. This constructor will train the model\&. If that is not desired, call the other constructor that does not take a data matrix\&. This constructor will also initialize the dictionary using the given DictionaryInitializer before training\&.
.PP
If you want to initialize the dictionary to a custom matrix, consider either writing your own DictionaryInitializer class (with void Initialize(const arma::mat& data, arma::mat& dictionary) function), or call the constructor that does not take a data matrix, then call \fBDictionary()\fP to set the dictionary matrix to a matrix of your choosing, and then call \fBTrain()\fP with \fBNothingInitializer\fP (i\&.e\&. Train<NothingInitializer>(data))\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Data matrix\&. 
.br
\fIatoms\fP Number of atoms in dictionary\&. 
.br
\fIlambda1\fP Regularization parameter for l1-norm penalty\&. 
.br
\fIlambda2\fP Regularization parameter for l2-norm penalty\&. 
.br
\fImaxIterations\fP Maximum number of iterations to run algorithm\&. If 0, the algorithm will run until convergence (or forever)\&. 
.br
\fIobjTolerance\fP Tolerance for objective function\&. When an iteration of the algorithm produces an improvement smaller than this, the algorithm will terminate\&. 
.br
\fInewtonTolerance\fP Tolerance for the Newton's method dictionary optimization step\&. 
.br
\fIinitializer\fP The initializer to use\&. 
.RE
.PP

.SS "\fBSparseCoding\fP (const size_t atoms = \fC0\fP, const double lambda1 = \fC0\fP, const double lambda2 = \fC0\fP, const size_t maxIterations = \fC0\fP, const double objTolerance = \fC0\&.01\fP, const double newtonTolerance = \fC1e\-6\fP)"

.PP
Set the parameters to \fBSparseCoding\fP\&. lambda2 defaults to 0\&. This constructor will not train the model, and a subsequent call to \fBTrain()\fP will be required before the model can encode points with \fBEncode()\fP\&.
.PP
\fBParameters:\fP
.RS 4
\fIatoms\fP Number of atoms in dictionary\&. 
.br
\fIlambda1\fP Regularization parameter for l1-norm penalty\&. 
.br
\fIlambda2\fP Regularization parameter for l2-norm penalty\&. 
.br
\fImaxIterations\fP Maximum number of iterations to run algorithm\&. If 0, the algorithm will run until convergence (or forever)\&. 
.br
\fIobjTolerance\fP Tolerance for objective function\&. When an iteration of the algorithm produces an improvement smaller than this, the algorithm will terminate\&. 
.br
\fInewtonTolerance\fP Tolerance for the Newton's method dictionary optimization step\&. 
.RE
.PP

.SH "Member Function Documentation"
.PP 
.SS "size_t Atoms () const\fC [inline]\fP"

.PP
Access the number of atoms\&. 
.PP
Definition at line 228 of file sparse_coding\&.hpp\&.
.SS "size_t& Atoms ()\fC [inline]\fP"

.PP
Modify the number of atoms\&. 
.PP
Definition at line 230 of file sparse_coding\&.hpp\&.
.SS "const arma::mat& Dictionary () const\fC [inline]\fP"

.PP
Access the dictionary\&. 
.PP
Definition at line 223 of file sparse_coding\&.hpp\&.
.SS "arma::mat& Dictionary ()\fC [inline]\fP"

.PP
Modify the dictionary\&. 
.PP
Definition at line 225 of file sparse_coding\&.hpp\&.
.SS "void Encode (const arma::mat & data, arma::mat & codes)"

.PP
Sparse code each point in the given dataset via LARS, using the current dictionary and store the encoded data in the codes matrix\&. 
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Input data matrix to be encoded\&. 
.br
\fIcodes\fP Output codes matrix\&. 
.RE
.PP

.SS "double Lambda1 () const\fC [inline]\fP"

.PP
Access the L1 regularization term\&. 
.PP
Definition at line 233 of file sparse_coding\&.hpp\&.
.SS "double& Lambda1 ()\fC [inline]\fP"

.PP
Modify the L1 regularization term\&. 
.PP
Definition at line 235 of file sparse_coding\&.hpp\&.
.SS "double Lambda2 () const\fC [inline]\fP"

.PP
Access the L2 regularization term\&. 
.PP
Definition at line 238 of file sparse_coding\&.hpp\&.
.SS "double& Lambda2 ()\fC [inline]\fP"

.PP
Modify the L2 regularization term\&. 
.PP
Definition at line 240 of file sparse_coding\&.hpp\&.
.SS "size_t MaxIterations () const\fC [inline]\fP"

.PP
Get the maximum number of iterations\&. 
.PP
Definition at line 243 of file sparse_coding\&.hpp\&.
.SS "size_t& MaxIterations ()\fC [inline]\fP"

.PP
Modify the maximum number of iterations\&. 
.PP
Definition at line 245 of file sparse_coding\&.hpp\&.
.SS "double NewtonTolerance () const\fC [inline]\fP"

.PP
Get the tolerance for Newton's method (dictionary optimization step)\&. 
.PP
Definition at line 253 of file sparse_coding\&.hpp\&.
.SS "double& NewtonTolerance ()\fC [inline]\fP"

.PP
Modify the tolerance for Newton's method (dictionary optimization step)\&. 
.PP
Definition at line 255 of file sparse_coding\&.hpp\&.
.PP
References SparseCoding::serialize()\&.
.SS "double Objective (const arma::mat & data, const arma::mat & codes) const"

.PP
Compute the objective function\&. 
.SS "double ObjTolerance () const\fC [inline]\fP"

.PP
Get the objective tolerance\&. 
.PP
Definition at line 248 of file sparse_coding\&.hpp\&.
.SS "double& ObjTolerance ()\fC [inline]\fP"

.PP
Modify the objective tolerance\&. 
.PP
Definition at line 250 of file sparse_coding\&.hpp\&.
.SS "double OptimizeDictionary (const arma::mat & data, const arma::mat & codes, const arma::uvec & adjacencies)"

.PP
Learn dictionary via Newton method based on Lagrange dual\&. 
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Data matrix\&. 
.br
\fIcodes\fP Matrix of codes\&. 
.br
\fIadjacencies\fP Indices of entries (unrolled column by column) of the coding matrix Z that are non-zero (the adjacency matrix for the bipartite graph of points and atoms)\&. 
.RE
.PP
\fBReturns:\fP
.RS 4
the norm of the gradient of the Lagrange dual with respect to the dual variables 
.RE
.PP

.SS "void ProjectDictionary ()"

.PP
Project each atom of the dictionary back onto the unit ball, if necessary\&. 
.SS "void serialize (Archive & ar, const uint32_t)"

.PP
Serialize the sparse coding model\&. 
.PP
Referenced by SparseCoding::NewtonTolerance()\&.
.SS "double Train (const arma::mat & data, const DictionaryInitializer & initializer = \fCDictionaryInitializer()\fP)"

.PP
Train the sparse coding model on the given dataset\&. 
.PP
\fBReturns:\fP
.RS 4
The final objective value\&. 
.RE
.PP


.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.

.TH "RecurrentAttention< InputDataType, OutputDataType >" 3 "Thu Jun 24 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
RecurrentAttention< InputDataType, OutputDataType > \- This class implements the \fBRecurrent\fP Model for Visual Attention, using a variety of possible layer implementations\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBRecurrentAttention\fP ()"
.br
.RI "Default constructor: this will not give a usable \fBRecurrentAttention\fP object, so be sure to set all the parameters before use\&. "
.ti -1c
.RI "template<typename RNNModuleType , typename ActionModuleType > \fBRecurrentAttention\fP (const size_t outSize, const RNNModuleType &rnn, const ActionModuleType &action, const size_t rho)"
.br
.RI "Create the \fBRecurrentAttention\fP object using the specified modules\&. "
.ti -1c
.RI "template<typename eT > void \fBBackward\fP (const arma::Mat< eT > &, const arma::Mat< eT > &gy, arma::Mat< eT > &g)"
.br
.RI "Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f\&. "
.ti -1c
.RI "OutputDataType const  & \fBDelta\fP () const"
.br
.RI "Get the delta\&. "
.ti -1c
.RI "OutputDataType & \fBDelta\fP ()"
.br
.RI "Modify the delta\&. "
.ti -1c
.RI "bool \fBDeterministic\fP () const"
.br
.RI "The value of the deterministic parameter\&. "
.ti -1c
.RI "bool & \fBDeterministic\fP ()"
.br
.RI "Modify the value of the deterministic parameter\&. "
.ti -1c
.RI "template<typename eT > void \fBForward\fP (const arma::Mat< eT > &input, arma::Mat< eT > &output)"
.br
.RI "Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f\&. "
.ti -1c
.RI "template<typename eT > void \fBGradient\fP (const arma::Mat< eT > &, const arma::Mat< eT > &, arma::Mat< eT > &)"
.br
.ti -1c
.RI "OutputDataType const  & \fBGradient\fP () const"
.br
.RI "Get the gradient\&. "
.ti -1c
.RI "OutputDataType & \fBGradient\fP ()"
.br
.RI "Modify the gradient\&. "
.ti -1c
.RI "std::vector< \fBLayerTypes\fP<> > & \fBModel\fP ()"
.br
.RI "Get the model modules\&. "
.ti -1c
.RI "OutputDataType const  & \fBOutputParameter\fP () const"
.br
.RI "Get the output parameter\&. "
.ti -1c
.RI "OutputDataType & \fBOutputParameter\fP ()"
.br
.RI "Modify the output parameter\&. "
.ti -1c
.RI "size_t \fBOutSize\fP () const"
.br
.RI "Get the module output size\&. "
.ti -1c
.RI "OutputDataType const  & \fBParameters\fP () const"
.br
.RI "Get the parameters\&. "
.ti -1c
.RI "OutputDataType & \fBParameters\fP ()"
.br
.RI "Modify the parameters\&. "
.ti -1c
.RI "size_t const  & \fBRho\fP () const"
.br
.RI "Get the number of steps to backpropagate through time\&. "
.ti -1c
.RI "template<typename Archive > void \fBserialize\fP (Archive &ar, const uint32_t)"
.br
.RI "Serialize the layer\&. "
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<typename InputDataType = arma::mat, typename OutputDataType = arma::mat>
.br
class mlpack::ann::RecurrentAttention< InputDataType, OutputDataType >"
This class implements the \fBRecurrent\fP Model for Visual Attention, using a variety of possible layer implementations\&. 

For more information, see the following paper\&.
.PP
.PP
.nf
@article{MnihHGK14,
  title   = {Recurrent Models of Visual Attention},
  author  = {Volodymyr Mnih, Nicolas Heess, Alex Graves, Koray Kavukcuoglu},
  journal = {CoRR},
  volume  = {abs/1406\&.6247},
  year    = {2014},
  url     = {https://arxiv\&.org/abs/1406\&.6247}
}
.fi
.PP
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIInputDataType\fP Type of the input data (arma::colvec, arma::mat, arma::sp_mat or arma::cube)\&. 
.br
\fIOutputDataType\fP Type of the output data (arma::colvec, arma::mat, arma::sp_mat or arma::cube)\&. 
.RE
.PP

.PP
Definition at line 199 of file layer_types\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBRecurrentAttention\fP ()"

.PP
Default constructor: this will not give a usable \fBRecurrentAttention\fP object, so be sure to set all the parameters before use\&. 
.SS "\fBRecurrentAttention\fP (const size_t outSize, const RNNModuleType & rnn, const ActionModuleType & action, const size_t rho)"

.PP
Create the \fBRecurrentAttention\fP object using the specified modules\&. 
.PP
\fBParameters:\fP
.RS 4
\fIoutSize\fP The module output size\&. 
.br
\fIrnn\fP The recurrent neural network module\&. 
.br
\fIaction\fP The action module\&. 
.br
\fIrho\fP Maximum number of steps to backpropagate through time (BPTT)\&. 
.RE
.PP

.SH "Member Function Documentation"
.PP 
.SS "void Backward (const arma::Mat< eT > &, const arma::Mat< eT > & gy, arma::Mat< eT > & g)"

.PP
Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards trough f\&. Using the results from the feed forward pass\&.
.PP
\fBParameters:\fP
.RS 4
\fI*\fP (input) The propagated input activation\&. 
.br
\fIgy\fP The backpropagated error\&. 
.br
\fIg\fP The calculated gradient\&. 
.RE
.PP

.SS "OutputDataType const& Delta () const\fC [inline]\fP"

.PP
Get the delta\&. 
.PP
Definition at line 134 of file recurrent_attention\&.hpp\&.
.SS "OutputDataType& Delta ()\fC [inline]\fP"

.PP
Modify the delta\&. 
.PP
Definition at line 136 of file recurrent_attention\&.hpp\&.
.SS "bool Deterministic () const\fC [inline]\fP"

.PP
The value of the deterministic parameter\&. 
.PP
Definition at line 119 of file recurrent_attention\&.hpp\&.
.SS "bool& Deterministic ()\fC [inline]\fP"

.PP
Modify the value of the deterministic parameter\&. 
.PP
Definition at line 121 of file recurrent_attention\&.hpp\&.
.SS "void Forward (const arma::Mat< eT > & input, arma::Mat< eT > & output)"

.PP
Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f\&. 
.PP
\fBParameters:\fP
.RS 4
\fIinput\fP Input data used for evaluating the specified function\&. 
.br
\fIoutput\fP Resulting output activation\&. 
.RE
.PP

.SS "void Gradient (const arma::Mat< eT > &, const arma::Mat< eT > &, arma::Mat< eT > &)"

.SS "OutputDataType const& Gradient () const\fC [inline]\fP"

.PP
Get the gradient\&. 
.PP
Definition at line 139 of file recurrent_attention\&.hpp\&.
.SS "OutputDataType& Gradient ()\fC [inline]\fP"

.PP
Modify the gradient\&. 
.PP
Definition at line 141 of file recurrent_attention\&.hpp\&.
.SS "std::vector<\fBLayerTypes\fP<> >& Model ()\fC [inline]\fP"

.PP
Get the model modules\&. 
.PP
Definition at line 116 of file recurrent_attention\&.hpp\&.
.SS "OutputDataType const& OutputParameter () const\fC [inline]\fP"

.PP
Get the output parameter\&. 
.PP
Definition at line 129 of file recurrent_attention\&.hpp\&.
.SS "OutputDataType& OutputParameter ()\fC [inline]\fP"

.PP
Modify the output parameter\&. 
.PP
Definition at line 131 of file recurrent_attention\&.hpp\&.
.SS "size_t OutSize () const\fC [inline]\fP"

.PP
Get the module output size\&. 
.PP
Definition at line 144 of file recurrent_attention\&.hpp\&.
.SS "OutputDataType const& Parameters () const\fC [inline]\fP"

.PP
Get the parameters\&. 
.PP
Definition at line 124 of file recurrent_attention\&.hpp\&.
.SS "OutputDataType& Parameters ()\fC [inline]\fP"

.PP
Modify the parameters\&. 
.PP
Definition at line 126 of file recurrent_attention\&.hpp\&.
.SS "size_t const& Rho () const\fC [inline]\fP"

.PP
Get the number of steps to backpropagate through time\&. 
.PP
Definition at line 147 of file recurrent_attention\&.hpp\&.
.PP
References RecurrentAttention< InputDataType, OutputDataType >::serialize()\&.
.SS "void serialize (Archive & ar, const uint32_t)"

.PP
Serialize the layer\&. 
.PP
Referenced by RecurrentAttention< InputDataType, OutputDataType >::Rho()\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.

.TH "OneStepQLearningWorker< EnvironmentType, NetworkType, UpdaterType, PolicyType >" 3 "Thu Jun 24 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
OneStepQLearningWorker< EnvironmentType, NetworkType, UpdaterType, PolicyType > \- Forward declaration of \fBOneStepQLearningWorker\fP\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Types"

.in +1c
.ti -1c
.RI "using \fBActionType\fP = typename EnvironmentType::Action"
.br
.ti -1c
.RI "using \fBStateType\fP = typename EnvironmentType::State"
.br
.ti -1c
.RI "using \fBTransitionType\fP = std::tuple< \fBStateType\fP, \fBActionType\fP, double, \fBStateType\fP >"
.br
.in -1c
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBOneStepQLearningWorker\fP (const UpdaterType &updater, const EnvironmentType &environment, const \fBTrainingConfig\fP &config, bool deterministic)"
.br
.RI "Construct one step Q-Learning worker with the given parameters and environment\&. "
.ti -1c
.RI "\fBOneStepQLearningWorker\fP (const \fBOneStepQLearningWorker\fP &other)"
.br
.RI "Copy another \fBOneStepQLearningWorker\fP\&. "
.ti -1c
.RI "\fBOneStepQLearningWorker\fP (\fBOneStepQLearningWorker\fP &&other)"
.br
.RI "Take ownership of another \fBOneStepQLearningWorker\fP\&. "
.ti -1c
.RI "\fB~OneStepQLearningWorker\fP ()"
.br
.RI "Clean memory\&. "
.ti -1c
.RI "void \fBInitialize\fP (NetworkType &learningNetwork)"
.br
.RI "Initialize the worker\&. "
.ti -1c
.RI "\fBOneStepQLearningWorker\fP & \fBoperator=\fP (const \fBOneStepQLearningWorker\fP &other)"
.br
.RI "Copy another \fBOneStepQLearningWorker\fP\&. "
.ti -1c
.RI "\fBOneStepQLearningWorker\fP & \fBoperator=\fP (\fBOneStepQLearningWorker\fP &&other)"
.br
.RI "Take ownership of another \fBOneStepQLearningWorker\fP\&. "
.ti -1c
.RI "bool \fBStep\fP (NetworkType &learningNetwork, NetworkType &targetNetwork, size_t &totalSteps, PolicyType &policy, double &totalReward)"
.br
.RI "The agent will execute one step\&. "
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<typename EnvironmentType, typename NetworkType, typename UpdaterType, typename PolicyType>
.br
class mlpack::rl::OneStepQLearningWorker< EnvironmentType, NetworkType, UpdaterType, PolicyType >"
Forward declaration of \fBOneStepQLearningWorker\fP\&. 

One step Q-Learning worker\&.
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIEnvironmentType\fP The type of the reinforcement learning task\&. 
.br
\fINetworkType\fP The type of the network model\&. 
.br
\fIUpdaterType\fP The type of the optimizer\&. 
.br
\fIPolicyType\fP The type of the behavior policy\&.
.br
\fIEnvironmentType\fP The type of the reinforcement learning task\&. 
.br
\fINetworkType\fP The type of the network model\&. 
.br
\fIUpdaterType\fP The type of the optimizer\&. 
.br
\fIPolicyType\fP The type of the behavior policy\&. * 
.RE
.PP

.PP
Definition at line 147 of file async_learning\&.hpp\&.
.SH "Member Typedef Documentation"
.PP 
.SS "using \fBActionType\fP =  typename EnvironmentType::Action"

.PP
Definition at line 39 of file one_step_q_learning_worker\&.hpp\&.
.SS "using \fBStateType\fP =  typename EnvironmentType::State"

.PP
Definition at line 38 of file one_step_q_learning_worker\&.hpp\&.
.SS "using \fBTransitionType\fP =  std::tuple<\fBStateType\fP, \fBActionType\fP, double, \fBStateType\fP>"

.PP
Definition at line 40 of file one_step_q_learning_worker\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBOneStepQLearningWorker\fP (const UpdaterType & updater, const EnvironmentType & environment, const \fBTrainingConfig\fP & config, bool deterministic)\fC [inline]\fP"

.PP
Construct one step Q-Learning worker with the given parameters and environment\&. 
.PP
\fBParameters:\fP
.RS 4
\fIupdater\fP The optimizer\&. 
.br
\fIenvironment\fP The reinforcement learning task\&. 
.br
\fIconfig\fP Hyper-parameters\&. 
.br
\fIdeterministic\fP Whether it should be deterministic\&. 
.RE
.PP

.PP
Definition at line 51 of file one_step_q_learning_worker\&.hpp\&.
.SS "\fBOneStepQLearningWorker\fP (const \fBOneStepQLearningWorker\fP< EnvironmentType, NetworkType, UpdaterType, PolicyType > & other)\fC [inline]\fP"

.PP
Copy another \fBOneStepQLearningWorker\fP\&. 
.PP
\fBParameters:\fP
.RS 4
\fIother\fP \fBOneStepQLearningWorker\fP to copy\&. 
.RE
.PP

.PP
Definition at line 71 of file one_step_q_learning_worker\&.hpp\&.
.SS "\fBOneStepQLearningWorker\fP (\fBOneStepQLearningWorker\fP< EnvironmentType, NetworkType, UpdaterType, PolicyType > && other)\fC [inline]\fP"

.PP
Take ownership of another \fBOneStepQLearningWorker\fP\&. 
.PP
\fBParameters:\fP
.RS 4
\fIother\fP \fBOneStepQLearningWorker\fP to take ownership of\&. 
.RE
.PP

.PP
Definition at line 101 of file one_step_q_learning_worker\&.hpp\&.
.SS "~\fBOneStepQLearningWorker\fP ()\fC [inline]\fP"

.PP
Clean memory\&. 
.PP
Definition at line 203 of file one_step_q_learning_worker\&.hpp\&.
.SH "Member Function Documentation"
.PP 
.SS "void Initialize (NetworkType & learningNetwork)\fC [inline]\fP"

.PP
Initialize the worker\&. 
.PP
\fBParameters:\fP
.RS 4
\fIlearningNetwork\fP The shared network\&. 
.RE
.PP

.PP
Definition at line 214 of file one_step_q_learning_worker\&.hpp\&.
.SS "\fBOneStepQLearningWorker\fP& operator= (const \fBOneStepQLearningWorker\fP< EnvironmentType, NetworkType, UpdaterType, PolicyType > & other)\fC [inline]\fP"

.PP
Copy another \fBOneStepQLearningWorker\fP\&. 
.PP
\fBParameters:\fP
.RS 4
\fIother\fP \fBOneStepQLearningWorker\fP to copy\&. 
.RE
.PP

.PP
Definition at line 131 of file one_step_q_learning_worker\&.hpp\&.
.SS "\fBOneStepQLearningWorker\fP& operator= (\fBOneStepQLearningWorker\fP< EnvironmentType, NetworkType, UpdaterType, PolicyType > && other)\fC [inline]\fP"

.PP
Take ownership of another \fBOneStepQLearningWorker\fP\&. 
.PP
\fBParameters:\fP
.RS 4
\fIother\fP \fBOneStepQLearningWorker\fP to take ownership of\&. 
.RE
.PP

.PP
Definition at line 168 of file one_step_q_learning_worker\&.hpp\&.
.SS "bool Step (NetworkType & learningNetwork, NetworkType & targetNetwork, size_t & totalSteps, PolicyType & policy, double & totalReward)\fC [inline]\fP"

.PP
The agent will execute one step\&. 
.PP
\fBParameters:\fP
.RS 4
\fIlearningNetwork\fP The shared learning network\&. 
.br
\fItargetNetwork\fP The shared target network\&. 
.br
\fItotalSteps\fP The shared counter for total steps\&. 
.br
\fIpolicy\fP The shared behavior policy\&. 
.br
\fItotalReward\fP This will be the episode return if the episode ends after this step\&. Otherwise this is invalid\&. 
.RE
.PP
\fBReturns:\fP
.RS 4
Indicate whether current episode ends after this step\&. 
.RE
.PP

.PP
Definition at line 243 of file one_step_q_learning_worker\&.hpp\&.
.PP
References TrainingConfig::Discount(), TrainingConfig::GradientLimit(), TrainingConfig::StepLimit(), TrainingConfig::StepSize(), TrainingConfig::TargetNetworkSyncInterval(), and TrainingConfig::UpdateInterval()\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.

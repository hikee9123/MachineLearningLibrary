.TH "AsyncLearning< WorkerType, EnvironmentType, NetworkType, UpdaterType, PolicyType >" 3 "Thu Jun 24 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
AsyncLearning< WorkerType, EnvironmentType, NetworkType, UpdaterType, PolicyType > \- Wrapper of various asynchronous learning algorithms, e\&.g\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBAsyncLearning\fP (\fBTrainingConfig\fP config, NetworkType network, PolicyType policy, UpdaterType updater=UpdaterType(), EnvironmentType environment=EnvironmentType())"
.br
.RI "Construct an instance of the given async learning algorithm\&. "
.ti -1c
.RI "\fBTrainingConfig\fP & \fBConfig\fP ()"
.br
.RI "Get training config\&. "
.ti -1c
.RI "const \fBTrainingConfig\fP & \fBConfig\fP () const"
.br
.RI "Modify training config\&. "
.ti -1c
.RI "EnvironmentType & \fBEnvironment\fP ()"
.br
.RI "Get the environment\&. "
.ti -1c
.RI "const EnvironmentType & \fBEnvironment\fP () const"
.br
.RI "Modify the environment\&. "
.ti -1c
.RI "NetworkType & \fBNetwork\fP ()"
.br
.RI "Get learning network\&. "
.ti -1c
.RI "const NetworkType & \fBNetwork\fP () const"
.br
.RI "Modify learning network\&. "
.ti -1c
.RI "PolicyType & \fBPolicy\fP ()"
.br
.RI "Get behavior policy\&. "
.ti -1c
.RI "const PolicyType & \fBPolicy\fP () const"
.br
.RI "Modify behavior policy\&. "
.ti -1c
.RI "template<typename Measure > void \fBTrain\fP (Measure &measure)"
.br
.RI "Starting async training\&. "
.ti -1c
.RI "UpdaterType & \fBUpdater\fP ()"
.br
.RI "Get optimizer\&. "
.ti -1c
.RI "const UpdaterType & \fBUpdater\fP () const"
.br
.RI "Modify optimizer\&. "
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<typename WorkerType, typename EnvironmentType, typename NetworkType, typename UpdaterType, typename PolicyType>
.br
class mlpack::rl::AsyncLearning< WorkerType, EnvironmentType, NetworkType, UpdaterType, PolicyType >"
Wrapper of various asynchronous learning algorithms, e\&.g\&. 

async one-step Q-learning, async one-step Sarsa, async n-step Q-learning and async advantage actor-critic\&.
.PP
For more details, see the following: 
.PP
.nf
@inproceedings{mnih2016asynchronous,
  title     = {Asynchronous methods for deep reinforcement learning},
  author    = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza,
               Mehdi and Graves, Alex and Lillicrap, Timothy and Harley,
               Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle = {International Conference on Machine Learning},
  pages     = {1928--1937},
  year      = {2016}
}

.fi
.PP
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIWorkerType\fP The type of the worker\&. 
.br
\fIEnvironmentType\fP The type of reinforcement learning task\&. 
.br
\fINetworkType\fP The type of the network model\&. 
.br
\fIUpdaterType\fP The type of the optimizer\&. 
.br
\fIPolicyType\fP The type of the behavior policy\&. 
.RE
.PP

.PP
Definition at line 57 of file async_learning\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBAsyncLearning\fP (\fBTrainingConfig\fP config, NetworkType network, PolicyType policy, UpdaterType updater = \fCUpdaterType()\fP, EnvironmentType environment = \fCEnvironmentType()\fP)"

.PP
Construct an instance of the given async learning algorithm\&. 
.PP
\fBParameters:\fP
.RS 4
\fIconfig\fP Hyper-parameters for training\&. 
.br
\fInetwork\fP The network model\&. 
.br
\fIpolicy\fP The behavior policy\&. 
.br
\fIupdater\fP The optimizer\&. 
.br
\fIenvironment\fP The reinforcement learning task\&. 
.RE
.PP

.SH "Member Function Documentation"
.PP 
.SS "\fBTrainingConfig\fP& Config ()\fC [inline]\fP"

.PP
Get training config\&. 
.PP
Definition at line 92 of file async_learning\&.hpp\&.
.SS "const \fBTrainingConfig\fP& Config () const\fC [inline]\fP"

.PP
Modify training config\&. 
.PP
Definition at line 94 of file async_learning\&.hpp\&.
.SS "EnvironmentType& Environment ()\fC [inline]\fP"

.PP
Get the environment\&. 
.PP
Definition at line 112 of file async_learning\&.hpp\&.
.SS "const EnvironmentType& Environment () const\fC [inline]\fP"

.PP
Modify the environment\&. 
.PP
Definition at line 114 of file async_learning\&.hpp\&.
.SS "NetworkType& Network ()\fC [inline]\fP"

.PP
Get learning network\&. 
.PP
Definition at line 97 of file async_learning\&.hpp\&.
.SS "const NetworkType& Network () const\fC [inline]\fP"

.PP
Modify learning network\&. 
.PP
Definition at line 99 of file async_learning\&.hpp\&.
.SS "PolicyType& Policy ()\fC [inline]\fP"

.PP
Get behavior policy\&. 
.PP
Definition at line 102 of file async_learning\&.hpp\&.
.SS "const PolicyType& Policy () const\fC [inline]\fP"

.PP
Modify behavior policy\&. 
.PP
Definition at line 104 of file async_learning\&.hpp\&.
.SS "void Train (Measure & measure)"

.PP
Starting async training\&. 
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIMeasure\fP The type of the measurement\&. It should be a callable object like 
.PP
.nf
bool foo(double reward);

.fi
.PP
 where reward is the total reward of a deterministic test episode, and the return value should indicate whether the training process is completed\&. 
.RE
.PP
\fBParameters:\fP
.RS 4
\fImeasure\fP The measurement instance\&. 
.RE
.PP

.SS "UpdaterType& Updater ()\fC [inline]\fP"

.PP
Get optimizer\&. 
.PP
Definition at line 107 of file async_learning\&.hpp\&.
.SS "const UpdaterType& Updater () const\fC [inline]\fP"

.PP
Modify optimizer\&. 
.PP
Definition at line 109 of file async_learning\&.hpp\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.

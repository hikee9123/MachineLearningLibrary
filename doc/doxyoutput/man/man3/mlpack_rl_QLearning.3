.TH "QLearning< EnvironmentType, NetworkType, UpdaterType, PolicyType, ReplayType >" 3 "Thu Jun 24 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
QLearning< EnvironmentType, NetworkType, UpdaterType, PolicyType, ReplayType > \- Implementation of various Q-Learning algorithms, such as DQN, double DQN\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Types"

.in +1c
.ti -1c
.RI "using \fBActionType\fP = typename EnvironmentType::Action"
.br
.RI "Convenient typedef for action\&. "
.ti -1c
.RI "using \fBStateType\fP = typename EnvironmentType::State"
.br
.RI "Convenient typedef for state\&. "
.in -1c
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBQLearning\fP (\fBTrainingConfig\fP &config, NetworkType &network, PolicyType &policy, ReplayType &replayMethod, UpdaterType updater=UpdaterType(), EnvironmentType environment=EnvironmentType())"
.br
.RI "Create the \fBQLearning\fP object with given settings\&. "
.ti -1c
.RI "\fB~QLearning\fP ()"
.br
.RI "Clean memory\&. "
.ti -1c
.RI "const \fBActionType\fP & \fBAction\fP () const"
.br
.RI "Get the action of the agent\&. "
.ti -1c
.RI "bool & \fBDeterministic\fP ()"
.br
.RI "Modify the training mode / test mode indicator\&. "
.ti -1c
.RI "const bool & \fBDeterministic\fP () const"
.br
.RI "Get the indicator of training mode / test mode\&. "
.ti -1c
.RI "EnvironmentType & \fBEnvironment\fP ()"
.br
.RI "Modify the environment in which the agent is\&. "
.ti -1c
.RI "const EnvironmentType & \fBEnvironment\fP () const"
.br
.RI "Get the environment in which the agent is\&. "
.ti -1c
.RI "double \fBEpisode\fP ()"
.br
.RI "Execute an episode\&. "
.ti -1c
.RI "const NetworkType & \fBNetwork\fP () const"
.br
.RI "Return the learning network\&. "
.ti -1c
.RI "NetworkType & \fBNetwork\fP ()"
.br
.RI "Modify the learning network\&. "
.ti -1c
.RI "void \fBSelectAction\fP ()"
.br
.RI "Select an action, given an agent\&. "
.ti -1c
.RI "\fBStateType\fP & \fBState\fP ()"
.br
.RI "Modify the state of the agent\&. "
.ti -1c
.RI "const \fBStateType\fP & \fBState\fP () const"
.br
.RI "Get the state of the agent\&. "
.ti -1c
.RI "size_t & \fBTotalSteps\fP ()"
.br
.RI "Modify total steps from beginning\&. "
.ti -1c
.RI "const size_t & \fBTotalSteps\fP () const"
.br
.RI "Get total steps from beginning\&. "
.ti -1c
.RI "void \fBTrainAgent\fP ()"
.br
.RI "Trains the DQN agent(non-categorical)\&. "
.ti -1c
.RI "void \fBTrainCategoricalAgent\fP ()"
.br
.RI "Trains the DQN agent of categorical type\&. "
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<typename EnvironmentType, typename NetworkType, typename UpdaterType, typename PolicyType, typename ReplayType = RandomReplay<EnvironmentType>>
.br
class mlpack::rl::QLearning< EnvironmentType, NetworkType, UpdaterType, PolicyType, ReplayType >"
Implementation of various Q-Learning algorithms, such as DQN, double DQN\&. 

For more details, see the following: 
.PP
.nf
@article{Mnih2013,
 author    = {Volodymyr Mnih and
              Koray Kavukcuoglu and
              David Silver and
              Alex Graves and
              Ioannis Antonoglou and
              Daan Wierstra and
              Martin A\&. Riedmiller},
 title     = {Playing Atari with Deep Reinforcement Learning},
 journal   = {CoRR},
 year      = {2013},
 url       = {http://arxiv\&.org/abs/1312\&.5602}
}

.fi
.PP
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIEnvironmentType\fP The environment of the reinforcement learning task\&. 
.br
\fINetworkType\fP The network to compute action value\&. 
.br
\fIUpdaterType\fP How to apply gradients when training\&. 
.br
\fIPolicyType\fP Behavior policy of the agent\&. 
.br
\fIReplayType\fP Experience replay method\&. 
.RE
.PP

.PP
Definition at line 58 of file q_learning\&.hpp\&.
.SH "Member Typedef Documentation"
.PP 
.SS "using \fBActionType\fP =  typename EnvironmentType::Action"

.PP
Convenient typedef for action\&. 
.PP
Definition at line 65 of file q_learning\&.hpp\&.
.SS "using \fBStateType\fP =  typename EnvironmentType::State"

.PP
Convenient typedef for state\&. 
.PP
Definition at line 62 of file q_learning\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBQLearning\fP (\fBTrainingConfig\fP & config, NetworkType & network, PolicyType & policy, ReplayType & replayMethod, UpdaterType updater = \fCUpdaterType()\fP, EnvironmentType environment = \fCEnvironmentType()\fP)"

.PP
Create the \fBQLearning\fP object with given settings\&. If you want to pass in a parameter and discard the original parameter object, be sure to use std::move to avoid unnecessary copy\&.
.PP
\fBParameters:\fP
.RS 4
\fIconfig\fP Hyper-parameters for training\&. 
.br
\fInetwork\fP The network to compute action value\&. 
.br
\fIpolicy\fP Behavior policy of the agent\&. 
.br
\fIreplayMethod\fP Experience replay method\&. 
.br
\fIupdater\fP How to apply gradients when training\&. 
.br
\fIenvironment\fP Reinforcement learning task\&. 
.RE
.PP

.SS "~\fBQLearning\fP ()"

.PP
Clean memory\&. 
.SH "Member Function Documentation"
.PP 
.SS "const \fBActionType\fP& Action () const\fC [inline]\fP"

.PP
Get the action of the agent\&. 
.PP
Definition at line 124 of file q_learning\&.hpp\&.
.SS "bool& Deterministic ()\fC [inline]\fP"

.PP
Modify the training mode / test mode indicator\&. 
.PP
Definition at line 132 of file q_learning\&.hpp\&.
.SS "const bool& Deterministic () const\fC [inline]\fP"

.PP
Get the indicator of training mode / test mode\&. 
.PP
Definition at line 134 of file q_learning\&.hpp\&.
.SS "EnvironmentType& Environment ()\fC [inline]\fP"

.PP
Modify the environment in which the agent is\&. 
.PP
Definition at line 127 of file q_learning\&.hpp\&.
.SS "const EnvironmentType& Environment () const\fC [inline]\fP"

.PP
Get the environment in which the agent is\&. 
.PP
Definition at line 129 of file q_learning\&.hpp\&.
.SS "double Episode ()"

.PP
Execute an episode\&. 
.PP
\fBReturns:\fP
.RS 4
Return of the episode\&. 
.RE
.PP

.SS "const NetworkType& Network () const\fC [inline]\fP"

.PP
Return the learning network\&. 
.PP
Definition at line 137 of file q_learning\&.hpp\&.
.SS "NetworkType& Network ()\fC [inline]\fP"

.PP
Modify the learning network\&. 
.PP
Definition at line 139 of file q_learning\&.hpp\&.
.SS "void SelectAction ()"

.PP
Select an action, given an agent\&. 
.SS "\fBStateType\fP& State ()\fC [inline]\fP"

.PP
Modify the state of the agent\&. 
.PP
Definition at line 119 of file q_learning\&.hpp\&.
.SS "const \fBStateType\fP& State () const\fC [inline]\fP"

.PP
Get the state of the agent\&. 
.PP
Definition at line 121 of file q_learning\&.hpp\&.
.SS "size_t& TotalSteps ()\fC [inline]\fP"

.PP
Modify total steps from beginning\&. 
.PP
Definition at line 114 of file q_learning\&.hpp\&.
.SS "const size_t& TotalSteps () const\fC [inline]\fP"

.PP
Get total steps from beginning\&. 
.PP
Definition at line 116 of file q_learning\&.hpp\&.
.SS "void TrainAgent ()"

.PP
Trains the DQN agent(non-categorical)\&. 
.SS "void TrainCategoricalAgent ()"

.PP
Trains the DQN agent of categorical type\&. 

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.

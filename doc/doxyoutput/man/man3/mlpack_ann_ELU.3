.TH "ELU< InputDataType, OutputDataType >" 3 "Thu Jun 24 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
ELU< InputDataType, OutputDataType > \- The \fBELU\fP activation function, defined by\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBELU\fP ()"
.br
.RI "Create the \fBELU\fP object\&. "
.ti -1c
.RI "\fBELU\fP (const double alpha)"
.br
.RI "Create the \fBELU\fP object using the specified parameter\&. "
.ti -1c
.RI "double const  & \fBAlpha\fP () const"
.br
.RI "Get the non zero gradient\&. "
.ti -1c
.RI "double & \fBAlpha\fP ()"
.br
.RI "Modify the non zero gradient\&. "
.ti -1c
.RI "template<typename DataType > void \fBBackward\fP (const DataType &input, const DataType &gy, DataType &g)"
.br
.RI "Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards through f\&. "
.ti -1c
.RI "OutputDataType const  & \fBDelta\fP () const"
.br
.RI "Get the delta\&. "
.ti -1c
.RI "OutputDataType & \fBDelta\fP ()"
.br
.RI "Modify the delta\&. "
.ti -1c
.RI "bool \fBDeterministic\fP () const"
.br
.RI "Get the value of deterministic parameter\&. "
.ti -1c
.RI "bool & \fBDeterministic\fP ()"
.br
.RI "Modify the value of deterministic parameter\&. "
.ti -1c
.RI "template<typename InputType , typename OutputType > void \fBForward\fP (const InputType &input, OutputType &output)"
.br
.RI "Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f\&. "
.ti -1c
.RI "double const  & \fBLambda\fP () const"
.br
.RI "Get the lambda parameter\&. "
.ti -1c
.RI "OutputDataType const  & \fBOutputParameter\fP () const"
.br
.RI "Get the output parameter\&. "
.ti -1c
.RI "OutputDataType & \fBOutputParameter\fP ()"
.br
.RI "Modify the output parameter\&. "
.ti -1c
.RI "template<typename Archive > void \fBserialize\fP (Archive &ar, const uint32_t)"
.br
.RI "Serialize the layer\&. "
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<typename InputDataType = arma::mat, typename OutputDataType = arma::mat>
.br
class mlpack::ann::ELU< InputDataType, OutputDataType >"
The \fBELU\fP activation function, defined by\&. 

\begin{eqnarray*} f(x) &=& \left\{ \begin{array}{lr} x & : x > 0 \\ \alpha(e^x - 1) & : x \le 0 \end{array} \right. \\ f'(x) &=& \left\{ \begin{array}{lr} 1 & : x > 0 \\ f(x) + \alpha & : x \le 0 \end{array} \right. \end{eqnarray*}
.PP
For more information, read the following paper:
.PP
.PP
.nf
@article{Clevert2015,
  author  = {Djork{-}Arn{\'{e}} Clevert and Thomas Unterthiner and
             Sepp Hochreiter},
  title   = {Fast and Accurate Deep Network Learning by Exponential Linear
             Units (ELUs)},
  journal = {CoRR},
  year    = {2015},
  url     = {https://arxiv\&.org/abs/1511\&.07289}
}
.fi
.PP
.PP
The SELU activation function is defined by
.PP
\begin{eqnarray*} f(x) &=& \left\{ \begin{array}{lr} \lambda * x & : x > 0 \\ \lambda * \alpha(e^x - 1) & : x \le 0 \end{array} \right. \\ f'(x) &=& \left\{ \begin{array}{lr} \lambda & : x > 0 \\ f(x) + \lambda * \alpha & : x \le 0 \end{array} \right. \end{eqnarray*}.PP
For more information, read the following paper:
.PP
.PP
.nf
@article{Klambauer2017,
  author  = {Gunter Klambauer and Thomas Unterthiner and
             Andreas Mayr},
  title   = {Self-Normalizing Neural Networks},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2017},
  url = {https://arxiv\&.org/abs/1706\&.02515}
}
.fi
.PP
.PP
In the deterministic mode, there is no computation of the derivative\&.
.PP
\fBNote:\fP
.RS 4
During training deterministic should be set to false and during testing/inference deterministic should be set to true\&. 
.PP
Make sure to use SELU activation function with normalized inputs and weights initialized with Lecun Normal Initialization\&.
.RE
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIInputDataType\fP Type of the input data (arma::colvec, arma::mat, arma::sp_mat or arma::cube)\&. 
.br
\fIOutputDataType\fP Type of the output data (arma::colvec, arma::mat, arma::sp_mat or arma::cube)\&. 
.RE
.PP

.PP
Definition at line 111 of file elu\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBELU\fP ()"

.PP
Create the \fBELU\fP object\&. NOTE: Use this constructor for SELU activation function\&. 
.SS "\fBELU\fP (const double alpha)"

.PP
Create the \fBELU\fP object using the specified parameter\&. The non zero gradient for negative inputs can be adjusted by specifying the \fBELU\fP hyperparameter alpha (alpha > 0)\&.
.PP
\fBNote:\fP
.RS 4
Use this constructor for \fBELU\fP activation function\&. 
.RE
.PP
\fBParameters:\fP
.RS 4
\fIalpha\fP Scale parameter for the negative factor\&. 
.RE
.PP

.SH "Member Function Documentation"
.PP 
.SS "double const& Alpha () const\fC [inline]\fP"

.PP
Get the non zero gradient\&. 
.PP
Definition at line 164 of file elu\&.hpp\&.
.SS "double& Alpha ()\fC [inline]\fP"

.PP
Modify the non zero gradient\&. 
.PP
Definition at line 166 of file elu\&.hpp\&.
.SS "void Backward (const DataType & input, const DataType & gy, DataType & g)"

.PP
Ordinary feed backward pass of a neural network, calculating the function f(x) by propagating x backwards through f\&. Using the results from the feed forward pass\&.
.PP
\fBParameters:\fP
.RS 4
\fIinput\fP The propagated input activation f(x)\&. 
.br
\fIgy\fP The backpropagated error\&. 
.br
\fIg\fP The calculated gradient\&. 
.RE
.PP

.SS "OutputDataType const& Delta () const\fC [inline]\fP"

.PP
Get the delta\&. 
.PP
Definition at line 159 of file elu\&.hpp\&.
.SS "OutputDataType& Delta ()\fC [inline]\fP"

.PP
Modify the delta\&. 
.PP
Definition at line 161 of file elu\&.hpp\&.
.SS "bool Deterministic () const\fC [inline]\fP"

.PP
Get the value of deterministic parameter\&. 
.PP
Definition at line 169 of file elu\&.hpp\&.
.SS "bool& Deterministic ()\fC [inline]\fP"

.PP
Modify the value of deterministic parameter\&. 
.PP
Definition at line 171 of file elu\&.hpp\&.
.SS "void Forward (const InputType & input, OutputType & output)"

.PP
Ordinary feed forward pass of a neural network, evaluating the function f(x) by propagating the activity forward through f\&. 
.PP
\fBParameters:\fP
.RS 4
\fIinput\fP Input data used for evaluating the specified function\&. 
.br
\fIoutput\fP Resulting output activation\&. 
.RE
.PP

.SS "double const& Lambda () const\fC [inline]\fP"

.PP
Get the lambda parameter\&. 
.PP
Definition at line 174 of file elu\&.hpp\&.
.PP
References ELU< InputDataType, OutputDataType >::serialize()\&.
.SS "OutputDataType const& OutputParameter () const\fC [inline]\fP"

.PP
Get the output parameter\&. 
.PP
Definition at line 154 of file elu\&.hpp\&.
.SS "OutputDataType& OutputParameter ()\fC [inline]\fP"

.PP
Modify the output parameter\&. 
.PP
Definition at line 156 of file elu\&.hpp\&.
.SS "void serialize (Archive & ar, const uint32_t)"

.PP
Serialize the layer\&. 
.PP
Referenced by ELU< InputDataType, OutputDataType >::Lambda()\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.

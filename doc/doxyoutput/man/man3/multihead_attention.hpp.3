.TH "/home/aakash/mlpack/src/mlpack/methods/ann/layer/multihead_attention.hpp" 3 "Sun Aug 22 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
/home/aakash/mlpack/src/mlpack/methods/ann/layer/multihead_attention.hpp
.SH SYNOPSIS
.br
.PP
.SS "Classes"

.in +1c
.ti -1c
.RI "class \fBMultiheadAttention< InputDataType, OutputDataType, RegularizerType >\fP"
.br
.RI "Multihead Attention allows the model to jointly attend to information from different representation subspaces at different positions\&. "
.in -1c
.SS "Namespaces"

.in +1c
.ti -1c
.RI " \fBmlpack\fP"
.br
.RI "Linear algebra utility functions, generally performed on matrices or vectors\&. "
.ti -1c
.RI " \fBmlpack::ann\fP"
.br
.RI "Artificial Neural Network\&. "
.in -1c
.SH "Detailed Description"
.PP 

.PP
\fBAuthor:\fP
.RS 4
Mrityunjay Tripathi
.RE
.PP
Definition of the MultiheadAttention class\&.
.PP
.PP
.nf
@article{NIPS'17,
  author  = {Ashish Vaswani, Llion Jones, Noam Shazeer, Niki Parmar,
             Aidan N\&. Gomez, Jakob Uszkoreit, ≈Åukasz Kaiser,
             Illia Polosukhin},
  title   = {Attention Is All You Need},
  year    = {2017},
  url     = {http://arxiv\&.org/abs/1706\&.03762v5}
}
.fi
.PP
.PP
mlpack is free software; you may redistribute it and/or modify it under the terms of the 3-clause BSD license\&. You should have received a copy of the 3-clause BSD license along with mlpack\&. If not, see http://www.opensource.org/licenses/BSD-3-Clause for more information\&. 
.PP
Definition in file \fBmultihead_attention\&.hpp\fP\&.
.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.

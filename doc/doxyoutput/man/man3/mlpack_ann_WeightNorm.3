.TH "WeightNorm< InputDataType, OutputDataType, CustomLayers >" 3 "Thu Jun 24 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
WeightNorm< InputDataType, OutputDataType, CustomLayers > \- Declaration of the \fBWeightNorm\fP layer class\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBWeightNorm\fP (\fBLayerTypes\fP< CustomLayers\&.\&.\&. > layer=\fBLayerTypes\fP< CustomLayers\&.\&.\&. >())"
.br
.RI "Create the \fBWeightNorm\fP layer object\&. "
.ti -1c
.RI "\fB~WeightNorm\fP ()"
.br
.RI "Destructor to release allocated memory\&. "
.ti -1c
.RI "template<typename eT > void \fBBackward\fP (const arma::Mat< eT > &input, const arma::Mat< eT > &gy, arma::Mat< eT > &g)"
.br
.RI "Backward pass through the layer\&. "
.ti -1c
.RI "OutputDataType const  & \fBDelta\fP () const"
.br
.RI "Get the delta\&. "
.ti -1c
.RI "OutputDataType & \fBDelta\fP ()"
.br
.RI "Modify the delta\&. "
.ti -1c
.RI "template<typename eT > void \fBForward\fP (const arma::Mat< eT > &input, arma::Mat< eT > &output)"
.br
.RI "Forward pass of the \fBWeightNorm\fP layer\&. "
.ti -1c
.RI "template<typename eT > void \fBGradient\fP (const arma::Mat< eT > &input, const arma::Mat< eT > &error, arma::Mat< eT > &gradient)"
.br
.RI "Calculate the gradient using the output delta, input activations and the weights of the wrapped layer\&. "
.ti -1c
.RI "OutputDataType const  & \fBGradient\fP () const"
.br
.RI "Get the gradient\&. "
.ti -1c
.RI "OutputDataType & \fBGradient\fP ()"
.br
.RI "Modify the gradient\&. "
.ti -1c
.RI "\fBLayerTypes\fP< CustomLayers\&.\&.\&. > const  & \fBLayer\fP ()"
.br
.RI "Get the wrapped layer\&. "
.ti -1c
.RI "OutputDataType const  & \fBOutputParameter\fP () const"
.br
.RI "Get the output parameter\&. "
.ti -1c
.RI "OutputDataType & \fBOutputParameter\fP ()"
.br
.RI "Modify the output parameter\&. "
.ti -1c
.RI "OutputDataType const  & \fBParameters\fP () const"
.br
.RI "Get the parameters\&. "
.ti -1c
.RI "OutputDataType & \fBParameters\fP ()"
.br
.RI "Modify the parameters\&. "
.ti -1c
.RI "void \fBReset\fP ()"
.br
.RI "Reset the layer parameters\&. "
.ti -1c
.RI "template<typename Archive > void \fBserialize\fP (Archive &ar, const uint32_t)"
.br
.RI "Serialize the layer\&. "
.in -1c
.SH "Detailed Description"
.PP 

.SS "template<typename InputDataType = arma::mat, typename OutputDataType = arma::mat, typename\&.\&.\&. CustomLayers>
.br
class mlpack::ann::WeightNorm< InputDataType, OutputDataType, CustomLayers >"
Declaration of the \fBWeightNorm\fP layer class\&. 

The layer reparameterizes the weight vectors in a neural network, decoupling the length of those weight vectors from their direction\&. This reparameterization does not introduce any dependencies between the examples in a mini-batch\&.
.PP
This class will be a wrapper around existing layers\&. It will just modify the calculation and updation of weights of the layer\&.
.PP
For more information, refer to the following paper,
.PP
.PP
.nf
@inproceedings{Salimans2016WeightNorm,
  title = {Weight Normalization: A Simple Reparameterization to Accelerate
           Training of Deep Neural Networks},
  author = {Tim Salimans, Diederik P\&. Kingma},
  booktitle = {Neural Information Processing Systems 2016},
  year = {2016},
  url  = {https://arxiv\&.org/abs/1602\&.07868},
}
.fi
.PP
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIInputDataType\fP Type of the input data (arma::colvec, arma::mat, arma::sp_mat or arma::cube)\&. 
.br
\fIOutputDataType\fP Type of the output data (arma::colvec, arma::mat, arma::sp_mat or arma::cube)\&. 
.br
\fICustomLayers\fP Additional custom layers that can be added\&. 
.RE
.PP

.PP
Definition at line 211 of file layer_types\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBWeightNorm\fP (\fBLayerTypes\fP< CustomLayers\&.\&.\&. > layer = \fC\fBLayerTypes\fP< CustomLayers\&.\&.\&. >()\fP)"

.PP
Create the \fBWeightNorm\fP layer object\&. 
.PP
\fBParameters:\fP
.RS 4
\fIlayer\fP The layer whose weights are needed to be normalized\&. 
.RE
.PP

.SS "~\fBWeightNorm\fP ()"

.PP
Destructor to release allocated memory\&. 
.SH "Member Function Documentation"
.PP 
.SS "void Backward (const arma::Mat< eT > & input, const arma::Mat< eT > & gy, arma::Mat< eT > & g)"

.PP
Backward pass through the layer\&. This function calls the \fBBackward()\fP function of the wrapped layer\&.
.PP
\fBParameters:\fP
.RS 4
\fIinput\fP The input activations\&. 
.br
\fIgy\fP The backpropagated error\&. 
.br
\fIg\fP The calculated gradient\&. 
.RE
.PP

.SS "OutputDataType const& Delta () const\fC [inline]\fP"

.PP
Get the delta\&. 
.PP
Definition at line 118 of file weight_norm\&.hpp\&.
.SS "OutputDataType& Delta ()\fC [inline]\fP"

.PP
Modify the delta\&. 
.PP
Definition at line 120 of file weight_norm\&.hpp\&.
.SS "void Forward (const arma::Mat< eT > & input, arma::Mat< eT > & output)"

.PP
Forward pass of the \fBWeightNorm\fP layer\&. Calculates the weights of the wrapped layer from the parameter vector v and the scalar parameter g\&. It then calulates the output of the wrapped layer from the calculated weights\&.
.PP
\fBParameters:\fP
.RS 4
\fIinput\fP Input data for the layer\&. 
.br
\fIoutput\fP Resulting output activations\&. 
.RE
.PP

.SS "void Gradient (const arma::Mat< eT > & input, const arma::Mat< eT > & error, arma::Mat< eT > & gradient)"

.PP
Calculate the gradient using the output delta, input activations and the weights of the wrapped layer\&. 
.PP
\fBParameters:\fP
.RS 4
\fIinput\fP The input activations\&. 
.br
\fIerror\fP The calculated error\&. 
.br
\fIgradient\fP The calculated gradient\&. 
.RE
.PP

.SS "OutputDataType const& Gradient () const\fC [inline]\fP"

.PP
Get the gradient\&. 
.PP
Definition at line 123 of file weight_norm\&.hpp\&.
.SS "OutputDataType& Gradient ()\fC [inline]\fP"

.PP
Modify the gradient\&. 
.PP
Definition at line 125 of file weight_norm\&.hpp\&.
.SS "\fBLayerTypes\fP<CustomLayers\&.\&.\&.> const& Layer ()\fC [inline]\fP"

.PP
Get the wrapped layer\&. 
.PP
Definition at line 138 of file weight_norm\&.hpp\&.
.PP
References WeightNorm< InputDataType, OutputDataType, CustomLayers >::serialize()\&.
.SS "OutputDataType const& OutputParameter () const\fC [inline]\fP"

.PP
Get the output parameter\&. 
.PP
Definition at line 128 of file weight_norm\&.hpp\&.
.SS "OutputDataType& OutputParameter ()\fC [inline]\fP"

.PP
Modify the output parameter\&. 
.PP
Definition at line 130 of file weight_norm\&.hpp\&.
.SS "OutputDataType const& Parameters () const\fC [inline]\fP"

.PP
Get the parameters\&. 
.PP
Definition at line 133 of file weight_norm\&.hpp\&.
.SS "OutputDataType& Parameters ()\fC [inline]\fP"

.PP
Modify the parameters\&. 
.PP
Definition at line 135 of file weight_norm\&.hpp\&.
.SS "void Reset ()"

.PP
Reset the layer parameters\&. 
.SS "void serialize (Archive & ar, const uint32_t)"

.PP
Serialize the layer\&. 
.PP
Referenced by WeightNorm< InputDataType, OutputDataType, CustomLayers >::Layer()\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.

.TH "mlpack::rl" 3 "Thu Jun 24 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
mlpack::rl
.SH SYNOPSIS
.br
.PP
.SS "Classes"

.in +1c
.ti -1c
.RI "class \fBAcrobot\fP"
.br
.RI "Implementation of \fBAcrobot\fP game\&. "
.ti -1c
.RI "class \fBAggregatedPolicy\fP"
.br
.ti -1c
.RI "class \fBAsyncLearning\fP"
.br
.RI "Wrapper of various asynchronous learning algorithms, e\&.g\&. "
.ti -1c
.RI "class \fBCartPole\fP"
.br
.RI "Implementation of Cart Pole task\&. "
.ti -1c
.RI "class \fBCategoricalDQN\fP"
.br
.RI "Implementation of the Categorical Deep Q-Learning network\&. "
.ti -1c
.RI "class \fBContinuousActionEnv\fP"
.br
.RI "To use the dummy environment, one may start by specifying the state and action dimensions\&. "
.ti -1c
.RI "class \fBContinuousDoublePoleCart\fP"
.br
.RI "Implementation of Continuous Double Pole Cart Balancing task\&. "
.ti -1c
.RI "class \fBContinuousMountainCar\fP"
.br
.RI "Implementation of Continuous Mountain Car task\&. "
.ti -1c
.RI "class \fBDiscreteActionEnv\fP"
.br
.RI "To use the dummy environment, one may start by specifying the state and action dimensions\&. "
.ti -1c
.RI "class \fBDoublePoleCart\fP"
.br
.RI "Implementation of Double Pole Cart Balancing task\&. "
.ti -1c
.RI "class \fBDuelingDQN\fP"
.br
.RI "Implementation of the Dueling Deep Q-Learning network\&. "
.ti -1c
.RI "class \fBGreedyPolicy\fP"
.br
.RI "Implementation for epsilon greedy policy\&. "
.ti -1c
.RI "class \fBMountainCar\fP"
.br
.RI "Implementation of Mountain Car task\&. "
.ti -1c
.RI "class \fBNStepQLearningWorker\fP"
.br
.RI "Forward declaration of \fBNStepQLearningWorker\fP\&. "
.ti -1c
.RI "class \fBOneStepQLearningWorker\fP"
.br
.RI "Forward declaration of \fBOneStepQLearningWorker\fP\&. "
.ti -1c
.RI "class \fBOneStepSarsaWorker\fP"
.br
.RI "Forward declaration of \fBOneStepSarsaWorker\fP\&. "
.ti -1c
.RI "class \fBPendulum\fP"
.br
.RI "Implementation of \fBPendulum\fP task\&. "
.ti -1c
.RI "class \fBPrioritizedReplay\fP"
.br
.RI "Implementation of prioritized experience replay\&. "
.ti -1c
.RI "class \fBQLearning\fP"
.br
.RI "Implementation of various Q-Learning algorithms, such as DQN, double DQN\&. "
.ti -1c
.RI "class \fBRandomReplay\fP"
.br
.RI "Implementation of random experience replay\&. "
.ti -1c
.RI "class \fBRewardClipping\fP"
.br
.RI "Interface for clipping the reward to some value between the specified maximum and minimum value (Clipping here is implemented as $ g_{\text{clipped}} = \max(g_{\text{min}}, \min(g_{\text{min}}, g))) $\&.) "
.ti -1c
.RI "class \fBSAC\fP"
.br
.RI "Implementation of Soft Actor-Critic, a model-free off-policy actor-critic based deep reinforcement learning algorithm\&. "
.ti -1c
.RI "class \fBSimpleDQN\fP"
.br
.ti -1c
.RI "class \fBSumTree\fP"
.br
.RI "Implementation of \fBSumTree\fP\&. "
.ti -1c
.RI "class \fBTrainingConfig\fP"
.br
.in -1c
.SS "Typedefs"

.in +1c
.ti -1c
.RI "template<typename EnvironmentType , typename NetworkType , typename UpdaterType , typename PolicyType > using \fBNStepQLearning\fP = \fBAsyncLearning\fP< \fBNStepQLearningWorker\fP< EnvironmentType, NetworkType, UpdaterType, PolicyType >, EnvironmentType, NetworkType, UpdaterType, PolicyType >"
.br
.RI "Convenient typedef for async n step q-learning\&. "
.ti -1c
.RI "template<typename EnvironmentType , typename NetworkType , typename UpdaterType , typename PolicyType > using \fBOneStepQLearning\fP = \fBAsyncLearning\fP< \fBOneStepQLearningWorker\fP< EnvironmentType, NetworkType, UpdaterType, PolicyType >, EnvironmentType, NetworkType, UpdaterType, PolicyType >"
.br
.RI "Convenient typedef for async one step q-learning\&. "
.ti -1c
.RI "template<typename EnvironmentType , typename NetworkType , typename UpdaterType , typename PolicyType > using \fBOneStepSarsa\fP = \fBAsyncLearning\fP< \fBOneStepSarsaWorker\fP< EnvironmentType, NetworkType, UpdaterType, PolicyType >, EnvironmentType, NetworkType, UpdaterType, PolicyType >"
.br
.RI "Convenient typedef for async one step Sarsa\&. "
.in -1c
.SH "Typedef Documentation"
.PP 
.SS "using \fBNStepQLearning\fP =  \fBAsyncLearning\fP<\fBNStepQLearningWorker\fP<EnvironmentType, NetworkType, UpdaterType, PolicyType>, EnvironmentType, NetworkType, UpdaterType, PolicyType>"

.PP
Convenient typedef for async n step q-learning\&. 
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIEnvironmentType\fP The type of the reinforcement learning task\&. 
.br
\fINetworkType\fP The type of the network model\&. 
.br
\fIUpdaterType\fP The type of the optimizer\&. 
.br
\fIPolicyType\fP The type of the behavior policy\&. 
.RE
.PP

.PP
Definition at line 233 of file async_learning\&.hpp\&.
.SS "using \fBOneStepQLearning\fP =  \fBAsyncLearning\fP<\fBOneStepQLearningWorker\fP<EnvironmentType, NetworkType, UpdaterType, PolicyType>, EnvironmentType, NetworkType, UpdaterType, PolicyType>"

.PP
Convenient typedef for async one step q-learning\&. 
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIEnvironmentType\fP The type of the reinforcement learning task\&. 
.br
\fINetworkType\fP The type of the network model\&. 
.br
\fIUpdaterType\fP The type of the optimizer\&. 
.br
\fIPolicyType\fP The type of the behavior policy\&. 
.RE
.PP

.PP
Definition at line 197 of file async_learning\&.hpp\&.
.SS "using \fBOneStepSarsa\fP =  \fBAsyncLearning\fP<\fBOneStepSarsaWorker\fP<EnvironmentType, NetworkType, UpdaterType, PolicyType>, EnvironmentType, NetworkType, UpdaterType, PolicyType>"

.PP
Convenient typedef for async one step Sarsa\&. 
.PP
\fBTemplate Parameters:\fP
.RS 4
\fIEnvironmentType\fP The type of the reinforcement learning task\&. 
.br
\fINetworkType\fP The type of the network model\&. 
.br
\fIUpdaterType\fP The type of the optimizer\&. 
.br
\fIPolicyType\fP The type of the behavior policy\&. 
.RE
.PP

.PP
Definition at line 215 of file async_learning\&.hpp\&.
.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.

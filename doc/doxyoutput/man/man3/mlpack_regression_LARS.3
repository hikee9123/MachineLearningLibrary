.TH "LARS" 3 "Thu Jun 24 2021" "Version 3.4.2" "mlpack" \" -*- nroff -*-
.ad l
.nh
.SH NAME
LARS \- An implementation of \fBLARS\fP, a stage-wise homotopy-based algorithm for l1-regularized linear regression (LASSO) and l1+l2 regularized linear regression (Elastic Net)\&.  

.SH SYNOPSIS
.br
.PP
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBLARS\fP (const bool useCholesky=false, const double lambda1=0\&.0, const double lambda2=0\&.0, const double tolerance=1e\-16)"
.br
.RI "Set the parameters to \fBLARS\fP\&. "
.ti -1c
.RI "\fBLARS\fP (const bool useCholesky, const arma::mat &gramMatrix, const double lambda1=0\&.0, const double lambda2=0\&.0, const double tolerance=1e\-16)"
.br
.RI "Set the parameters to \fBLARS\fP, and pass in a precalculated Gram matrix\&. "
.ti -1c
.RI "\fBLARS\fP (const arma::mat &data, const arma::rowvec &responses, const bool transposeData=true, const bool useCholesky=false, const double lambda1=0\&.0, const double lambda2=0\&.0, const double tolerance=1e\-16)"
.br
.RI "Set the parameters to \fBLARS\fP and run training\&. "
.ti -1c
.RI "\fBLARS\fP (const arma::mat &data, const arma::rowvec &responses, const bool transposeData, const bool useCholesky, const arma::mat &gramMatrix, const double lambda1=0\&.0, const double lambda2=0\&.0, const double tolerance=1e\-16)"
.br
.RI "Set the parameters to \fBLARS\fP, pass in a precalculated Gram matrix, and run training\&. "
.ti -1c
.RI "\fBLARS\fP (const \fBLARS\fP &other)"
.br
.RI "Construct the \fBLARS\fP object by copying the given \fBLARS\fP object\&. "
.ti -1c
.RI "\fBLARS\fP (\fBLARS\fP &&other)"
.br
.RI "Construct the \fBLARS\fP object by taking ownership of the given \fBLARS\fP object\&. "
.ti -1c
.RI "const std::vector< size_t > & \fBActiveSet\fP () const"
.br
.RI "Access the set of active dimensions\&. "
.ti -1c
.RI "const arma::vec & \fBBeta\fP () const"
.br
.RI "Access the solution coefficients\&. "
.ti -1c
.RI "const std::vector< arma::vec > & \fBBetaPath\fP () const"
.br
.RI "Access the set of coefficients after each iteration; the solution is the last element\&. "
.ti -1c
.RI "double \fBComputeError\fP (const arma::mat &matX, const arma::rowvec &y, const bool rowMajor=false)"
.br
.RI "Compute cost error of the given data matrix using the currently-trained \fBLARS\fP model\&. "
.ti -1c
.RI "double \fBLambda1\fP () const"
.br
.RI "Get the L1 regularization coefficient\&. "
.ti -1c
.RI "double & \fBLambda1\fP ()"
.br
.RI "Modify the L1 regularization coefficient\&. "
.ti -1c
.RI "double \fBLambda2\fP () const"
.br
.RI "Get the L2 regularization coefficient\&. "
.ti -1c
.RI "double & \fBLambda2\fP ()"
.br
.RI "Modify the L2 regularization coefficient\&. "
.ti -1c
.RI "const std::vector< double > & \fBLambdaPath\fP () const"
.br
.RI "Access the set of values for lambda1 after each iteration; the solution is the last element\&. "
.ti -1c
.RI "const arma::mat & \fBMatUtriCholFactor\fP () const"
.br
.RI "Access the upper triangular cholesky factor\&. "
.ti -1c
.RI "\fBLARS\fP & \fBoperator=\fP (const \fBLARS\fP &other)"
.br
.RI "Copy the given \fBLARS\fP object\&. "
.ti -1c
.RI "\fBLARS\fP & \fBoperator=\fP (\fBLARS\fP &&other)"
.br
.RI "Take ownership of the given \fBLARS\fP object\&. "
.ti -1c
.RI "void \fBPredict\fP (const arma::mat &points, arma::rowvec &predictions, const bool rowMajor=false) const"
.br
.RI "Predict y_i for each data point in the given data matrix using the currently-trained \fBLARS\fP model\&. "
.ti -1c
.RI "template<typename Archive > void \fBserialize\fP (Archive &ar, const uint32_t)"
.br
.RI "Serialize the \fBLARS\fP model\&. "
.ti -1c
.RI "double \fBTolerance\fP () const"
.br
.RI "Get the tolerance for maximum correlation during training\&. "
.ti -1c
.RI "double & \fBTolerance\fP ()"
.br
.RI "Modify the tolerance for maximum correlation during training\&. "
.ti -1c
.RI "double \fBTrain\fP (const arma::mat &data, const arma::rowvec &responses, arma::vec &beta, const bool transposeData=true)"
.br
.RI "Run \fBLARS\fP\&. "
.ti -1c
.RI "double \fBTrain\fP (const arma::mat &data, const arma::rowvec &responses, const bool transposeData=true)"
.br
.RI "Run \fBLARS\fP\&. "
.ti -1c
.RI "bool \fBUseCholesky\fP () const"
.br
.RI "Get whether to use the Cholesky decomposition\&. "
.ti -1c
.RI "bool & \fBUseCholesky\fP ()"
.br
.RI "Modify whether to use the Cholesky decomposition\&. "
.in -1c
.SH "Detailed Description"
.PP 
An implementation of \fBLARS\fP, a stage-wise homotopy-based algorithm for l1-regularized linear regression (LASSO) and l1+l2 regularized linear regression (Elastic Net)\&. 

Let $ X $ be a matrix where each row is a point and each column is a dimension and let $ y $ be a vector of responses\&.
.PP
The Elastic Net problem is to solve
.PP
\[ \min_{\beta} 0.5 || X \beta - y ||_2^2 + \lambda_1 || \beta ||_1 + 0.5 \lambda_2 || \beta ||_2^2 \].PP
where $ \beta $ is the vector of regression coefficients\&.
.PP
If $ \lambda_1 > 0 $ and $ \lambda_2 = 0 $, the problem is the LASSO\&. If $ \lambda_1 > 0 $ and $ \lambda_2 > 0 $, the problem is the elastic net\&. If $ \lambda_1 = 0 $ and $ \lambda_2 > 0 $, the problem is ridge regression\&. If $ \lambda_1 = 0 $ and $ \lambda_2 = 0 $, the problem is unregularized linear regression\&.
.PP
Note: This algorithm is not recommended for use (in terms of efficiency) when $ \lambda_1 $ = 0\&.
.PP
For more details, see the following papers:
.PP
.PP
.nf
@article{efron2004least,
  title={Least angle regression},
  author={Efron, B\&. and Hastie, T\&. and Johnstone, I\&. and Tibshirani, R\&.},
  journal={The Annals of statistics},
  volume={32},
  number={2},
  pages={407--499},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}
.fi
.PP
.PP
.PP
.nf
@article{zou2005regularization,
  title={Regularization and variable selection via the elastic net},
  author={Zou, H\&. and Hastie, T\&.},
  journal={Journal of the Royal Statistical Society Series B},
  volume={67},
  number={2},
  pages={301--320},
  year={2005},
  publisher={Royal Statistical Society}
}
.fi
.PP
 
.PP
Definition at line 89 of file lars\&.hpp\&.
.SH "Constructor & Destructor Documentation"
.PP 
.SS "\fBLARS\fP (const bool useCholesky = \fCfalse\fP, const double lambda1 = \fC0\&.0\fP, const double lambda2 = \fC0\&.0\fP, const double tolerance = \fC1e\-16\fP)"

.PP
Set the parameters to \fBLARS\fP\&. Both lambda1 and lambda2 default to 0\&.
.PP
\fBParameters:\fP
.RS 4
\fIuseCholesky\fP Whether or not to use Cholesky decomposition when solving linear system (as opposed to using the full Gram matrix)\&. 
.br
\fIlambda1\fP Regularization parameter for l1-norm penalty\&. 
.br
\fIlambda2\fP Regularization parameter for l2-norm penalty\&. 
.br
\fItolerance\fP Run until the maximum correlation of elements in (X^T y) is less than this\&. 
.RE
.PP

.SS "\fBLARS\fP (const bool useCholesky, const arma::mat & gramMatrix, const double lambda1 = \fC0\&.0\fP, const double lambda2 = \fC0\&.0\fP, const double tolerance = \fC1e\-16\fP)"

.PP
Set the parameters to \fBLARS\fP, and pass in a precalculated Gram matrix\&. Both lambda1 and lambda2 default to 0\&.
.PP
\fBParameters:\fP
.RS 4
\fIuseCholesky\fP Whether or not to use Cholesky decomposition when solving linear system (as opposed to using the full Gram matrix)\&. 
.br
\fIgramMatrix\fP Gram matrix\&. 
.br
\fIlambda1\fP Regularization parameter for l1-norm penalty\&. 
.br
\fIlambda2\fP Regularization parameter for l2-norm penalty\&. 
.br
\fItolerance\fP Run until the maximum correlation of elements in (X^T y) is less than this\&. 
.RE
.PP

.SS "\fBLARS\fP (const arma::mat & data, const arma::rowvec & responses, const bool transposeData = \fCtrue\fP, const bool useCholesky = \fCfalse\fP, const double lambda1 = \fC0\&.0\fP, const double lambda2 = \fC0\&.0\fP, const double tolerance = \fC1e\-16\fP)"

.PP
Set the parameters to \fBLARS\fP and run training\&. Both lambda1 and lambda2 are set by default to 0\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Input data\&. 
.br
\fIresponses\fP A vector of targets\&. 
.br
\fItransposeData\fP Should be true if the input data is column-major and false otherwise\&. 
.br
\fIuseCholesky\fP Whether or not to use Cholesky decomposition when solving linear system (as opposed to using the full Gram matrix)\&. 
.br
\fIlambda1\fP Regularization parameter for l1-norm penalty\&. 
.br
\fIlambda2\fP Regularization parameter for l2-norm penalty\&. 
.br
\fItolerance\fP Run until the maximum correlation of elements in (X^T y) is less than this\&. 
.RE
.PP

.SS "\fBLARS\fP (const arma::mat & data, const arma::rowvec & responses, const bool transposeData, const bool useCholesky, const arma::mat & gramMatrix, const double lambda1 = \fC0\&.0\fP, const double lambda2 = \fC0\&.0\fP, const double tolerance = \fC1e\-16\fP)"

.PP
Set the parameters to \fBLARS\fP, pass in a precalculated Gram matrix, and run training\&. Both lambda1 and lambda2 are set by default to 0\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Input data\&. 
.br
\fIresponses\fP A vector of targets\&. 
.br
\fItransposeData\fP Should be true if the input data is column-major and false otherwise\&. 
.br
\fIuseCholesky\fP Whether or not to use Cholesky decomposition when solving linear system (as opposed to using the full Gram matrix)\&. 
.br
\fIgramMatrix\fP Gram matrix\&. 
.br
\fIlambda1\fP Regularization parameter for l1-norm penalty\&. 
.br
\fIlambda2\fP Regularization parameter for l2-norm penalty\&. 
.br
\fItolerance\fP Run until the maximum correlation of elements in (X^T y) is less than this\&. 
.RE
.PP

.SS "\fBLARS\fP (const \fBLARS\fP & other)"

.PP
Construct the \fBLARS\fP object by copying the given \fBLARS\fP object\&. 
.PP
\fBParameters:\fP
.RS 4
\fIother\fP \fBLARS\fP object to copy\&. 
.RE
.PP

.SS "\fBLARS\fP (\fBLARS\fP && other)"

.PP
Construct the \fBLARS\fP object by taking ownership of the given \fBLARS\fP object\&. 
.PP
\fBParameters:\fP
.RS 4
\fIother\fP \fBLARS\fP object to take ownership of\&. 
.RE
.PP

.SH "Member Function Documentation"
.PP 
.SS "const std::vector<size_t>& ActiveSet () const\fC [inline]\fP"

.PP
Access the set of active dimensions\&. 
.PP
Definition at line 273 of file lars\&.hpp\&.
.SS "const arma::vec& Beta () const\fC [inline]\fP"

.PP
Access the solution coefficients\&. 
.PP
Definition at line 280 of file lars\&.hpp\&.
.SS "const std::vector<arma::vec>& BetaPath () const\fC [inline]\fP"

.PP
Access the set of coefficients after each iteration; the solution is the last element\&. 
.PP
Definition at line 277 of file lars\&.hpp\&.
.SS "double ComputeError (const arma::mat & matX, const arma::rowvec & y, const bool rowMajor = \fCfalse\fP)"

.PP
Compute cost error of the given data matrix using the currently-trained \fBLARS\fP model\&. Only ||y-beta*X||2 is used to calculate cost error\&.
.PP
\fBParameters:\fP
.RS 4
\fImatX\fP Column-major input data (or row-major input data if rowMajor = true)\&. 
.br
\fIy\fP responses A vector of targets\&. 
.br
\fIrowMajor\fP Should be true if the data points matrix is row-major and false otherwise\&. 
.RE
.PP
\fBReturns:\fP
.RS 4
The minimum cost error\&. 
.RE
.PP

.PP
Referenced by LARS::MatUtriCholFactor()\&.
.SS "double Lambda1 () const\fC [inline]\fP"

.PP
Get the L1 regularization coefficient\&. 
.PP
Definition at line 253 of file lars\&.hpp\&.
.SS "double& Lambda1 ()\fC [inline]\fP"

.PP
Modify the L1 regularization coefficient\&. 
.PP
Definition at line 255 of file lars\&.hpp\&.
.SS "double Lambda2 () const\fC [inline]\fP"

.PP
Get the L2 regularization coefficient\&. 
.PP
Definition at line 258 of file lars\&.hpp\&.
.SS "double& Lambda2 ()\fC [inline]\fP"

.PP
Modify the L2 regularization coefficient\&. 
.PP
Definition at line 260 of file lars\&.hpp\&.
.SS "const std::vector<double>& LambdaPath () const\fC [inline]\fP"

.PP
Access the set of values for lambda1 after each iteration; the solution is the last element\&. 
.PP
Definition at line 284 of file lars\&.hpp\&.
.SS "const arma::mat& MatUtriCholFactor () const\fC [inline]\fP"

.PP
Access the upper triangular cholesky factor\&. 
.PP
Definition at line 287 of file lars\&.hpp\&.
.PP
References LARS::ComputeError(), and LARS::serialize()\&.
.SS "\fBLARS\fP& operator= (const \fBLARS\fP & other)"

.PP
Copy the given \fBLARS\fP object\&. 
.PP
\fBParameters:\fP
.RS 4
\fIother\fP \fBLARS\fP object to copy\&. 
.RE
.PP

.SS "\fBLARS\fP& operator= (\fBLARS\fP && other)"

.PP
Take ownership of the given \fBLARS\fP object\&. 
.PP
\fBParameters:\fP
.RS 4
\fIother\fP \fBLARS\fP object to take ownership of\&. 
.RE
.PP

.SS "void Predict (const arma::mat & points, arma::rowvec & predictions, const bool rowMajor = \fCfalse\fP) const"

.PP
Predict y_i for each data point in the given data matrix using the currently-trained \fBLARS\fP model\&. 
.PP
\fBParameters:\fP
.RS 4
\fIpoints\fP The data points to regress on\&. 
.br
\fIpredictions\fP y, which will contained calculated values on completion\&. 
.br
\fIrowMajor\fP Should be true if the data points matrix is row-major and false otherwise\&. 
.RE
.PP

.SS "void serialize (Archive & ar, const uint32_t)"

.PP
Serialize the \fBLARS\fP model\&. 
.PP
Referenced by LARS::MatUtriCholFactor()\&.
.SS "double Tolerance () const\fC [inline]\fP"

.PP
Get the tolerance for maximum correlation during training\&. 
.PP
Definition at line 268 of file lars\&.hpp\&.
.SS "double& Tolerance ()\fC [inline]\fP"

.PP
Modify the tolerance for maximum correlation during training\&. 
.PP
Definition at line 270 of file lars\&.hpp\&.
.SS "double Train (const arma::mat & data, const arma::rowvec & responses, arma::vec & beta, const bool transposeData = \fCtrue\fP)"

.PP
Run \fBLARS\fP\&. The input matrix (like all mlpack matrices) should be column-major -- each column is an observation and each row is a dimension\&. However, because \fBLARS\fP is more efficient on a row-major matrix, this method will (internally) transpose the matrix\&. If this transposition is not necessary (i\&.e\&., you want to pass in a row-major matrix), pass 'false' for the transposeData parameter\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Column-major input data (or row-major input data if rowMajor = true)\&. 
.br
\fIresponses\fP A vector of targets\&. 
.br
\fIbeta\fP Vector to store the solution (the coefficients) in\&. 
.br
\fItransposeData\fP Set to false if the data is row-major\&. 
.RE
.PP
\fBReturns:\fP
.RS 4
minimum cost error(||y-beta*X||2 is used to calculate error)\&. 
.RE
.PP

.SS "double Train (const arma::mat & data, const arma::rowvec & responses, const bool transposeData = \fCtrue\fP)"

.PP
Run \fBLARS\fP\&. The input matrix (like all mlpack matrices) should be column-major -- each column is an observation and each row is a dimension\&. However, because \fBLARS\fP is more efficient on a row-major matrix, this method will (internally) transpose the matrix\&. If this transposition is not necessary (i\&.e\&., you want to pass in a row-major matrix), pass 'false' for the transposeData parameter\&.
.PP
\fBParameters:\fP
.RS 4
\fIdata\fP Input data\&. 
.br
\fIresponses\fP A vector of targets\&. 
.br
\fItransposeData\fP Should be true if the input data is column-major and false otherwise\&. 
.RE
.PP
\fBReturns:\fP
.RS 4
minimum cost error(||y-beta*X||2 is used to calculate error)\&. 
.RE
.PP

.SS "bool UseCholesky () const\fC [inline]\fP"

.PP
Get whether to use the Cholesky decomposition\&. 
.PP
Definition at line 263 of file lars\&.hpp\&.
.SS "bool& UseCholesky ()\fC [inline]\fP"

.PP
Modify whether to use the Cholesky decomposition\&. 
.PP
Definition at line 265 of file lars\&.hpp\&.

.SH "Author"
.PP 
Generated automatically by Doxygen for mlpack from the source code\&.

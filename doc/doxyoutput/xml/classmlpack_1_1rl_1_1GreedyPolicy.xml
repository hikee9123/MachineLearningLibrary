<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.13">
  <compounddef id="classmlpack_1_1rl_1_1GreedyPolicy" kind="class" language="C++" prot="public">
    <compoundname>mlpack::rl::GreedyPolicy</compoundname>
    <includes refid="greedy__policy_8hpp" local="no">greedy_policy.hpp</includes>
    <templateparamlist>
      <param>
        <type>typename EnvironmentType</type>
      </param>
    </templateparamlist>
      <sectiondef kind="public-type">
      <memberdef kind="typedef" id="classmlpack_1_1rl_1_1GreedyPolicy_1aaf7b2dc5d49d01961601c7c16be76777" prot="public" static="no">
        <type>typename EnvironmentType::Action</type>
        <definition>using ActionType =  typename EnvironmentType::Action</definition>
        <argsstring></argsstring>
        <name>ActionType</name>
        <briefdescription>
<para>Convenient typedef for action. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" line="35" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" bodystart="35" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="private-attrib">
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1GreedyPolicy_1a9ea1a8f5ef3e6d20b362385e1288c72c" prot="private" static="no" mutable="no">
        <type>double</type>
        <definition>double delta</definition>
        <argsstring></argsstring>
        <name>delta</name>
        <briefdescription>
<para>Locally-stored stride for epsilon to anneal. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" line="109" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" bodystart="109" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1GreedyPolicy_1a4904cc82627458fdf6672ccc0b2802c7" prot="private" static="no" mutable="no">
        <type>double</type>
        <definition>double epsilon</definition>
        <argsstring></argsstring>
        <name>epsilon</name>
        <briefdescription>
<para>Locally-stored probability to explore. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" line="103" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" bodystart="103" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1GreedyPolicy_1aca9f803e05b7ef0ec701502c514aa8e2" prot="private" static="no" mutable="no">
        <type>double</type>
        <definition>double minEpsilon</definition>
        <argsstring></argsstring>
        <name>minEpsilon</name>
        <briefdescription>
<para>Locally-stored lower bound for epsilon. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" line="106" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" bodystart="106" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="public-func">
      <memberdef kind="function" id="classmlpack_1_1rl_1_1GreedyPolicy_1a7e04af56c8b5bb57890640e7fcb6b676" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type></type>
        <definition>GreedyPolicy</definition>
        <argsstring>(const double initialEpsilon, const size_t annealInterval, const double minEpsilon, const double decayRate=1.0)</argsstring>
        <name>GreedyPolicy</name>
        <param>
          <type>const double</type>
          <declname>initialEpsilon</declname>
        </param>
        <param>
          <type>const size_t</type>
          <declname>annealInterval</declname>
        </param>
        <param>
          <type>const double</type>
          <declname>minEpsilon</declname>
        </param>
        <param>
          <type>const double</type>
          <declname>decayRate</declname>
          <defval>1.0</defval>
        </param>
        <briefdescription>
<para>Constructor for epsilon greedy policy class. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>initialEpsilon</parametername>
</parameternamelist>
<parameterdescription>
<para>The initial probability to explore (select a random action). </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>annealInterval</parametername>
</parameternamelist>
<parameterdescription>
<para>The steps during which the probability to explore will anneal. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>minEpsilon</parametername>
</parameternamelist>
<parameterdescription>
<para>Epsilon will never be less than this value. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>decayRate</parametername>
</parameternamelist>
<parameterdescription>
<para>How much to change the model in response to the estimated error each time the model weights are updated. </para></parameterdescription>
</parameteritem>
</parameterlist>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" line="48" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" bodystart="48" bodyend="55"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1GreedyPolicy_1a280278726ff7d32f2b7eff5c92a1767a" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>void</type>
        <definition>void Anneal</definition>
        <argsstring>()</argsstring>
        <name>Anneal</name>
        <briefdescription>
<para>Exploration probability will anneal at each step. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" line="90" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" bodystart="90" bodyend="94"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1GreedyPolicy_1a3ababd597760bb1f9782ad2c17aadb41" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>const double &amp;</type>
        <definition>const double&amp; Epsilon</definition>
        <argsstring>() const</argsstring>
        <name>Epsilon</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>Current possibility to explore. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" line="99" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" bodystart="99" bodyend="99"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1GreedyPolicy_1ab0b01a3a4864c348b0c97ba92bae45c4" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type><ref refid="classmlpack_1_1rl_1_1GreedyPolicy_1aaf7b2dc5d49d01961601c7c16be76777" kindref="member">ActionType</ref></type>
        <definition>ActionType Sample</definition>
        <argsstring>(const arma::colvec &amp;actionValue, bool deterministic=false, const bool isNoisy=false)</argsstring>
        <name>Sample</name>
        <param>
          <type>const arma::colvec &amp;</type>
          <declname>actionValue</declname>
        </param>
        <param>
          <type>bool</type>
          <declname>deterministic</declname>
          <defval>false</defval>
        </param>
        <param>
          <type>const bool</type>
          <declname>isNoisy</declname>
          <defval>false</defval>
        </param>
        <briefdescription>
<para>Sample an action based on given action values. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>actionValue</parametername>
</parameternamelist>
<parameterdescription>
<para>Values for each action. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>deterministic</parametername>
</parameternamelist>
<parameterdescription>
<para>Always select the action greedily. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>isNoisy</parametername>
</parameternamelist>
<parameterdescription>
<para>Specifies whether the network used is noisy. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>Sampled action. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" line="65" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" bodystart="65" bodyend="85"/>
        <references refid="namespacemlpack_1_1math_1aad090ac225728d74c1b66bcf52f3ab95" compoundref="random_8hpp" startline="110" endline="113">mlpack::math::RandInt</references>
        <references refid="namespacemlpack_1_1math_1a305db122ade561ba1fe874bd51e9797d" compoundref="random_8hpp" startline="83" endline="86">mlpack::math::Random</references>
      </memberdef>
      </sectiondef>
    <briefdescription>
<para>Implementation for epsilon greedy policy. </para>    </briefdescription>
    <detaileddescription>
<para>In general we will select an action greedily based on the action value, however sometimes we will also randomly select an action to encourage exploration.</para><para><parameterlist kind="templateparam"><parameteritem>
<parameternamelist>
<parametername>EnvironmentType</parametername>
</parameternamelist>
<parameterdescription>
<para>The reinforcement learning task. </para></parameterdescription>
</parameteritem>
</parameterlist>
</para>    </detaileddescription>
    <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" line="32" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp" bodystart="31" bodyend="110"/>
    <listofallmembers>
      <member refid="classmlpack_1_1rl_1_1GreedyPolicy_1aaf7b2dc5d49d01961601c7c16be76777" prot="public" virt="non-virtual"><scope>mlpack::rl::GreedyPolicy</scope><name>ActionType</name></member>
      <member refid="classmlpack_1_1rl_1_1GreedyPolicy_1a280278726ff7d32f2b7eff5c92a1767a" prot="public" virt="non-virtual"><scope>mlpack::rl::GreedyPolicy</scope><name>Anneal</name></member>
      <member refid="classmlpack_1_1rl_1_1GreedyPolicy_1a9ea1a8f5ef3e6d20b362385e1288c72c" prot="private" virt="non-virtual"><scope>mlpack::rl::GreedyPolicy</scope><name>delta</name></member>
      <member refid="classmlpack_1_1rl_1_1GreedyPolicy_1a4904cc82627458fdf6672ccc0b2802c7" prot="private" virt="non-virtual"><scope>mlpack::rl::GreedyPolicy</scope><name>epsilon</name></member>
      <member refid="classmlpack_1_1rl_1_1GreedyPolicy_1a3ababd597760bb1f9782ad2c17aadb41" prot="public" virt="non-virtual"><scope>mlpack::rl::GreedyPolicy</scope><name>Epsilon</name></member>
      <member refid="classmlpack_1_1rl_1_1GreedyPolicy_1a7e04af56c8b5bb57890640e7fcb6b676" prot="public" virt="non-virtual"><scope>mlpack::rl::GreedyPolicy</scope><name>GreedyPolicy</name></member>
      <member refid="classmlpack_1_1rl_1_1GreedyPolicy_1aca9f803e05b7ef0ec701502c514aa8e2" prot="private" virt="non-virtual"><scope>mlpack::rl::GreedyPolicy</scope><name>minEpsilon</name></member>
      <member refid="classmlpack_1_1rl_1_1GreedyPolicy_1ab0b01a3a4864c348b0c97ba92bae45c4" prot="public" virt="non-virtual"><scope>mlpack::rl::GreedyPolicy</scope><name>Sample</name></member>
    </listofallmembers>
  </compounddef>
</doxygen>

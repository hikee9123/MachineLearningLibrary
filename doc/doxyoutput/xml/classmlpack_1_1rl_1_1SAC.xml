<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.13">
  <compounddef id="classmlpack_1_1rl_1_1SAC" kind="class" language="C++" prot="public">
    <compoundname>mlpack::rl::SAC</compoundname>
    <includes refid="sac_8hpp" local="no">sac.hpp</includes>
    <templateparamlist>
      <param>
        <type>typename EnvironmentType</type>
      </param>
      <param>
        <type>typename QNetworkType</type>
      </param>
      <param>
        <type>typename PolicyNetworkType</type>
      </param>
      <param>
        <type>typename UpdaterType</type>
      </param>
      <param>
        <type>typename ReplayType</type>
        <defval><ref refid="classmlpack_1_1rl_1_1RandomReplay" kindref="compound">RandomReplay</ref>&lt;EnvironmentType&gt;</defval>
      </param>
    </templateparamlist>
      <sectiondef kind="public-type">
      <memberdef kind="typedef" id="classmlpack_1_1rl_1_1SAC_1aaf7b2dc5d49d01961601c7c16be76777" prot="public" static="no">
        <type>typename EnvironmentType::Action</type>
        <definition>using ActionType =  typename EnvironmentType::Action</definition>
        <argsstring></argsstring>
        <name>ActionType</name>
        <briefdescription>
<para>Convenient typedef for action. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="71" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="71" bodyend="-1"/>
      </memberdef>
      <memberdef kind="typedef" id="classmlpack_1_1rl_1_1SAC_1ada68ef405b7c331a2bee337614f00088" prot="public" static="no">
        <type>typename EnvironmentType::State</type>
        <definition>using StateType =  typename EnvironmentType::State</definition>
        <argsstring></argsstring>
        <name>StateType</name>
        <briefdescription>
<para>Convenient typedef for state. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="68" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="68" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="private-attrib">
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1a99189868c044162e6d669fd832ecdc81" prot="private" static="no" mutable="no">
        <type><ref refid="classmlpack_1_1rl_1_1SAC_1aaf7b2dc5d49d01961601c7c16be76777" kindref="member">ActionType</ref></type>
        <definition>ActionType action</definition>
        <argsstring></argsstring>
        <name>action</name>
        <briefdescription>
<para>Locally-stored action of the agent. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="187" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="187" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1a9256888813dc52f6cc0ac1a7955a8f83" prot="private" static="no" mutable="no">
        <type><ref refid="classmlpack_1_1rl_1_1TrainingConfig" kindref="compound">TrainingConfig</ref> &amp;</type>
        <definition>TrainingConfig&amp; config</definition>
        <argsstring></argsstring>
        <name>config</name>
        <briefdescription>
<para>Locally-stored hyper-parameters. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="147" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="147" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1a02bda2c80b22d0c2507c7a74febb93bf" prot="private" static="no" mutable="no">
        <type>bool</type>
        <definition>bool deterministic</definition>
        <argsstring></argsstring>
        <name>deterministic</name>
        <briefdescription>
<para>Locally-stored flag indicating training mode or test mode. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="190" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="190" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1a08976b7e3e12e32740a2e28fadd21c00" prot="private" static="no" mutable="no">
        <type>EnvironmentType</type>
        <definition>EnvironmentType environment</definition>
        <argsstring></argsstring>
        <name>environment</name>
        <briefdescription>
<para>Locally-stored reinforcement learning task. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="178" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="178" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1a5575a0c7b445e0b99a17ef7cad40a4f3" prot="private" static="no" mutable="no">
        <type>QNetworkType &amp;</type>
        <definition>QNetworkType&amp; learningQ1Network</definition>
        <argsstring></argsstring>
        <name>learningQ1Network</name>
        <briefdescription>
<para>Locally-stored learning Q1 and Q2 network. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="150" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="150" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1aaf0b9f35db473a1d26ca927efbdf489e" prot="private" static="no" mutable="no">
        <type>QNetworkType</type>
        <definition>QNetworkType learningQ2Network</definition>
        <argsstring></argsstring>
        <name>learningQ2Network</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="151" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="151" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1a7a19d447982f361f9a7e553d6bc2fc4a" prot="private" static="no" mutable="no">
        <type><ref refid="classmlpack_1_1ann_1_1MeanSquaredError" kindref="compound">mlpack::ann::MeanSquaredError</ref></type>
        <definition>mlpack::ann::MeanSquaredError lossFunction</definition>
        <argsstring></argsstring>
        <name>lossFunction</name>
        <briefdescription>
<para>Locally-stored loss function. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="193" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="193" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1a9f56ee6addb82a730b7ecc391a19bbc4" prot="private" static="no" mutable="no">
        <type>PolicyNetworkType &amp;</type>
        <definition>PolicyNetworkType&amp; policyNetwork</definition>
        <argsstring></argsstring>
        <name>policyNetwork</name>
        <briefdescription>
<para>Locally-stored policy network. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="158" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="158" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1ab9fe105911232943fa3194882160a476" prot="private" static="no" mutable="no">
        <type>UpdaterType</type>
        <definition>UpdaterType policyNetworkUpdater</definition>
        <argsstring></argsstring>
        <name>policyNetworkUpdater</name>
        <briefdescription>
<para>Locally-stored updater. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="171" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="171" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1a54884dd97872655f18333f83766387f2" prot="private" static="no" mutable="no">
        <type>UpdaterType</type>
        <definition>UpdaterType qNetworkUpdater</definition>
        <argsstring></argsstring>
        <name>qNetworkUpdater</name>
        <briefdescription>
<para>Locally-stored updater. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="164" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="164" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1a6bb6f1ff61cda292d713c849f4448935" prot="private" static="no" mutable="no">
        <type>ReplayType &amp;</type>
        <definition>ReplayType&amp; replayMethod</definition>
        <argsstring></argsstring>
        <name>replayMethod</name>
        <briefdescription>
<para>Locally-stored experience method. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="161" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="161" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1a4d1aa26dcfa648e02cbb0964cddbdbfe" prot="private" static="no" mutable="no">
        <type><ref refid="classmlpack_1_1rl_1_1SAC_1ada68ef405b7c331a2bee337614f00088" kindref="member">StateType</ref></type>
        <definition>StateType state</definition>
        <argsstring></argsstring>
        <name>state</name>
        <briefdescription>
<para>Locally-stored current state of the agent. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="184" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="184" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1a30177eb4d11157f4c49c95c2a7bf835c" prot="private" static="no" mutable="no">
        <type>QNetworkType</type>
        <definition>QNetworkType targetQ1Network</definition>
        <argsstring></argsstring>
        <name>targetQ1Network</name>
        <briefdescription>
<para>Locally-stored target Q1 and Q2 network. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="154" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="154" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1afbc3f899c027dd989180b6d94c5049c6" prot="private" static="no" mutable="no">
        <type>QNetworkType</type>
        <definition>QNetworkType targetQ2Network</definition>
        <argsstring></argsstring>
        <name>targetQ2Network</name>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="155" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="155" bodyend="-1"/>
      </memberdef>
      <memberdef kind="variable" id="classmlpack_1_1rl_1_1SAC_1af40d4ff06d1593b3cdb96afe98cfe209" prot="private" static="no" mutable="no">
        <type>size_t</type>
        <definition>size_t totalSteps</definition>
        <argsstring></argsstring>
        <name>totalSteps</name>
        <briefdescription>
<para>Total steps from the beginning of the task. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="181" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="181" bodyend="-1"/>
      </memberdef>
      </sectiondef>
      <sectiondef kind="public-func">
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1a382013c48f00c0cd5e682edb92a01f16" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type></type>
        <definition>SAC</definition>
        <argsstring>(TrainingConfig &amp;config, QNetworkType &amp;learningQ1Network, PolicyNetworkType &amp;policyNetwork, ReplayType &amp;replayMethod, UpdaterType qNetworkUpdater=UpdaterType(), UpdaterType policyNetworkUpdater=UpdaterType(), EnvironmentType environment=EnvironmentType())</argsstring>
        <name>SAC</name>
        <param>
          <type><ref refid="classmlpack_1_1rl_1_1TrainingConfig" kindref="compound">TrainingConfig</ref> &amp;</type>
          <declname>config</declname>
        </param>
        <param>
          <type>QNetworkType &amp;</type>
          <declname>learningQ1Network</declname>
        </param>
        <param>
          <type>PolicyNetworkType &amp;</type>
          <declname>policyNetwork</declname>
        </param>
        <param>
          <type>ReplayType &amp;</type>
          <declname>replayMethod</declname>
        </param>
        <param>
          <type>UpdaterType</type>
          <declname>qNetworkUpdater</declname>
          <defval>UpdaterType()</defval>
        </param>
        <param>
          <type>UpdaterType</type>
          <declname>policyNetworkUpdater</declname>
          <defval>UpdaterType()</defval>
        </param>
        <param>
          <type>EnvironmentType</type>
          <declname>environment</declname>
          <defval>EnvironmentType()</defval>
        </param>
        <briefdescription>
<para>Create the <ref refid="classmlpack_1_1rl_1_1SAC" kindref="compound">SAC</ref> object with given settings. </para>        </briefdescription>
        <detaileddescription>
<para>If you want to pass in a parameter and discard the original parameter object, you can directly pass the parameter, as the constructor takes a reference. This avoids unnecessary copy.</para><para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>config</parametername>
</parameternamelist>
<parameterdescription>
<para>Hyper-parameters for training. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>learningQ1Network</parametername>
</parameternamelist>
<parameterdescription>
<para>The network to compute action value. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>policyNetwork</parametername>
</parameternamelist>
<parameterdescription>
<para>The network to produce an action given a state. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>replayMethod</parametername>
</parameternamelist>
<parameterdescription>
<para>Experience replay method. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>qNetworkUpdater</parametername>
</parameternamelist>
<parameterdescription>
<para>How to apply gradients to Q network when training. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>policyNetworkUpdater</parametername>
</parameternamelist>
<parameterdescription>
<para>How to apply gradients to policy network when training. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>environment</parametername>
</parameternamelist>
<parameterdescription>
<para>Reinforcement learning task. </para></parameterdescription>
</parameteritem>
</parameterlist>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="89" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1abf0c5202e5e579fc47c332f2490fbb8f" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type></type>
        <definition>~SAC</definition>
        <argsstring>()</argsstring>
        <name>~SAC</name>
        <briefdescription>
<para>Clean memory. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="100" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1a0d32caed9517e5d2014238a22f78352d" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>const <ref refid="classmlpack_1_1rl_1_1SAC_1aaf7b2dc5d49d01961601c7c16be76777" kindref="member">ActionType</ref> &amp;</type>
        <definition>const ActionType&amp; Action</definition>
        <argsstring>() const</argsstring>
        <name>Action</name>
        <briefdescription>
<para>Get the action of the agent. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="137" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="137" bodyend="137"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1a42d4ee3da432cff20d3a41b8b1ec801c" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>bool &amp;</type>
        <definition>bool&amp; Deterministic</definition>
        <argsstring>()</argsstring>
        <name>Deterministic</name>
        <briefdescription>
<para>Modify the training mode / test mode indicator. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="140" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="140" bodyend="140"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1a5d262f7871c5cc8b532971fb644f0abf" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>const bool &amp;</type>
        <definition>const bool&amp; Deterministic</definition>
        <argsstring>() const</argsstring>
        <name>Deterministic</name>
        <briefdescription>
<para>Get the indicator of training mode / test mode. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="142" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="142" bodyend="142"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1a1fb26736f2d90010f882f9628cd26612" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>double</type>
        <definition>double Episode</definition>
        <argsstring>()</argsstring>
        <name>Episode</name>
        <briefdescription>
<para>Execute an episode. </para>        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>Return of the episode. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="124" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1abd126acd7f564c8326dc765232624ae4" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>void</type>
        <definition>void SelectAction</definition>
        <argsstring>()</argsstring>
        <name>SelectAction</name>
        <briefdescription>
<para>Select an action, given an agent. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="118" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1a0e8b07b1eb04d72c36e95f795340d5c6" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>void</type>
        <definition>void SoftUpdate</definition>
        <argsstring>(double rho)</argsstring>
        <name>SoftUpdate</name>
        <param>
          <type>double</type>
          <declname>rho</declname>
        </param>
        <briefdescription>
<para>Softly update the learning Q network parameters to the target Q network parameters. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>rho</parametername>
</parameternamelist>
<parameterdescription>
<para>How &quot;softly&quot; should the parameters be copied. </para></parameterdescription>
</parameteritem>
</parameterlist>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="108" column="1"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1ad7a595de4a1a67da528603c20f80315f" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type><ref refid="classmlpack_1_1rl_1_1SAC_1ada68ef405b7c331a2bee337614f00088" kindref="member">StateType</ref> &amp;</type>
        <definition>StateType&amp; State</definition>
        <argsstring>()</argsstring>
        <name>State</name>
        <briefdescription>
<para>Modify the state of the agent. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="132" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="132" bodyend="132"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1afa3e388ae5e024c8ec49fd4d1ef725ad" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>const <ref refid="classmlpack_1_1rl_1_1SAC_1ada68ef405b7c331a2bee337614f00088" kindref="member">StateType</ref> &amp;</type>
        <definition>const StateType&amp; State</definition>
        <argsstring>() const</argsstring>
        <name>State</name>
        <briefdescription>
<para>Get the state of the agent. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="134" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="134" bodyend="134"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1abaf0bb243c2e643c57654b8e65058fa0" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>size_t &amp;</type>
        <definition>size_t&amp; TotalSteps</definition>
        <argsstring>()</argsstring>
        <name>TotalSteps</name>
        <briefdescription>
<para>Modify total steps from beginning. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="127" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="127" bodyend="127"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1a689af4e6e564ab01f40e6ec49638bdaf" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>const size_t &amp;</type>
        <definition>const size_t&amp; TotalSteps</definition>
        <argsstring>() const</argsstring>
        <name>TotalSteps</name>
        <briefdescription>
<para>Get total steps from beginning. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="129" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="129" bodyend="129"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1SAC_1aec0783b5a136e042adcc47bae4fe5291" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>void</type>
        <definition>void Update</definition>
        <argsstring>()</argsstring>
        <name>Update</name>
        <briefdescription>
<para>Update the Q and policy networks. </para>        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="113" column="1"/>
      </memberdef>
      </sectiondef>
    <briefdescription>
<para>Implementation of Soft Actor-Critic, a model-free off-policy actor-critic based deep reinforcement learning algorithm. </para>    </briefdescription>
    <detaileddescription>
<para>For more details, see the following: <programlisting><codeline><highlight class="normal">@misc{haarnoja2018soft,</highlight></codeline>
<codeline><highlight class="normal"><sp/>author<sp/><sp/><sp/><sp/>=<sp/>{Tuomas<sp/>Haarnoja<sp/>and</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Aurick<sp/>Zhou<sp/>and</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Kristian<sp/>Hartikainen<sp/>and</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>George<sp/>Tucker<sp/>and</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Sehoon<sp/>Ha<sp/>and</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Jie<sp/>Tan<sp/>and</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Vikash<sp/>Kumar<sp/>and</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Henry<sp/>Zhu<sp/>and</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Abhishek<sp/>Gupta<sp/>and</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Pieter<sp/>Abbeel<sp/>and</highlight></codeline>
<codeline><highlight class="normal"><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/><sp/>Sergey<sp/>Levine},</highlight></codeline>
<codeline><highlight class="normal"><sp/>title<sp/><sp/><sp/><sp/><sp/>=<sp/>{Soft<sp/>Actor-Critic<sp/>Algorithms<sp/>and<sp/>Applications},</highlight></codeline>
<codeline><highlight class="normal"><sp/>year<sp/><sp/><sp/><sp/><sp/><sp/>=<sp/>{2018},</highlight></codeline>
<codeline><highlight class="normal"><sp/>url<sp/><sp/><sp/><sp/><sp/><sp/><sp/>=<sp/>{https://arxiv.org/abs/1812.05905}</highlight></codeline>
<codeline><highlight class="normal">}</highlight></codeline>
</programlisting></para><para><parameterlist kind="templateparam"><parameteritem>
<parameternamelist>
<parametername>EnvironmentType</parametername>
</parameternamelist>
<parameterdescription>
<para>The environment of the reinforcement learning task. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>NetworkType</parametername>
</parameternamelist>
<parameterdescription>
<para>The network to compute action value. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>UpdaterType</parametername>
</parameternamelist>
<parameterdescription>
<para>How to apply gradients when training. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>ReplayType</parametername>
</parameternamelist>
<parameterdescription>
<para>Experience replay method. </para></parameterdescription>
</parameteritem>
</parameterlist>
</para>    </detaileddescription>
    <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" line="65" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/sac.hpp" bodystart="64" bodyend="194"/>
    <listofallmembers>
      <member refid="classmlpack_1_1rl_1_1SAC_1a99189868c044162e6d669fd832ecdc81" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>action</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a0d32caed9517e5d2014238a22f78352d" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>Action</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1aaf7b2dc5d49d01961601c7c16be76777" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>ActionType</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a9256888813dc52f6cc0ac1a7955a8f83" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>config</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a02bda2c80b22d0c2507c7a74febb93bf" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>deterministic</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a42d4ee3da432cff20d3a41b8b1ec801c" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>Deterministic</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a5d262f7871c5cc8b532971fb644f0abf" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>Deterministic</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a08976b7e3e12e32740a2e28fadd21c00" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>environment</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a1fb26736f2d90010f882f9628cd26612" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>Episode</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a5575a0c7b445e0b99a17ef7cad40a4f3" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>learningQ1Network</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1aaf0b9f35db473a1d26ca927efbdf489e" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>learningQ2Network</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a7a19d447982f361f9a7e553d6bc2fc4a" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>lossFunction</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a9f56ee6addb82a730b7ecc391a19bbc4" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>policyNetwork</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1ab9fe105911232943fa3194882160a476" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>policyNetworkUpdater</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a54884dd97872655f18333f83766387f2" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>qNetworkUpdater</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a6bb6f1ff61cda292d713c849f4448935" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>replayMethod</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a382013c48f00c0cd5e682edb92a01f16" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>SAC</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1abd126acd7f564c8326dc765232624ae4" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>SelectAction</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a0e8b07b1eb04d72c36e95f795340d5c6" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>SoftUpdate</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1ad7a595de4a1a67da528603c20f80315f" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>State</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1afa3e388ae5e024c8ec49fd4d1ef725ad" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>State</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a4d1aa26dcfa648e02cbb0964cddbdbfe" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>state</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1ada68ef405b7c331a2bee337614f00088" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>StateType</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a30177eb4d11157f4c49c95c2a7bf835c" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>targetQ1Network</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1afbc3f899c027dd989180b6d94c5049c6" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>targetQ2Network</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1abaf0bb243c2e643c57654b8e65058fa0" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>TotalSteps</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1a689af4e6e564ab01f40e6ec49638bdaf" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>TotalSteps</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1af40d4ff06d1593b3cdb96afe98cfe209" prot="private" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>totalSteps</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1aec0783b5a136e042adcc47bae4fe5291" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>Update</name></member>
      <member refid="classmlpack_1_1rl_1_1SAC_1abf0c5202e5e579fc47c332f2490fbb8f" prot="public" virt="non-virtual"><scope>mlpack::rl::SAC</scope><name>~SAC</name></member>
    </listofallmembers>
  </compounddef>
</doxygen>

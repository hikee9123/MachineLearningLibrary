<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.13">
  <compounddef id="classmlpack_1_1rl_1_1DiscreteActionEnv" kind="class" language="C++" prot="public">
    <compoundname>mlpack::rl::DiscreteActionEnv</compoundname>
    <includes refid="env__type_8hpp" local="no">env_type.hpp</includes>
    <innerclass refid="classmlpack_1_1rl_1_1DiscreteActionEnv_1_1Action" prot="public">mlpack::rl::DiscreteActionEnv::Action</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1DiscreteActionEnv_1_1State" prot="public">mlpack::rl::DiscreteActionEnv::State</innerclass>
      <sectiondef kind="public-func">
      <memberdef kind="function" id="classmlpack_1_1rl_1_1DiscreteActionEnv_1aa9f537249fa0c1e62b38197996ab4c6a" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type><ref refid="classmlpack_1_1rl_1_1DiscreteActionEnv_1_1State" kindref="compound">State</ref></type>
        <definition>State InitialSample</definition>
        <argsstring>()</argsstring>
        <name>InitialSample</name>
        <briefdescription>
<para>Dummy function to mimic initial sampling in an environment. </para>        </briefdescription>
        <detaileddescription>
<para><simplesect kind="return"><para>the dummy state. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/environment/env_type.hpp" line="99" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/environment/env_type.hpp" bodystart="99" bodyend="99"/>
        <references refid="classmlpack_1_1rl_1_1DiscreteActionEnv_1_1State_1a790355057d12e9c1ce7643551c16fecd" compoundref="env__type_8hpp" startline="44" endline="45">DiscreteActionEnv::State::State</references>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1DiscreteActionEnv_1a7061ffc3be37983a46a312973a375c0a" prot="public" static="no" const="yes" explicit="no" inline="yes" virt="non-virtual">
        <type>bool</type>
        <definition>bool IsTerminal</definition>
        <argsstring>(const State &amp;) const</argsstring>
        <name>IsTerminal</name>
        <param>
          <type>const <ref refid="classmlpack_1_1rl_1_1DiscreteActionEnv_1_1State" kindref="compound">State</ref> &amp;</type>
        </param>
        <briefdescription>
<para>Dummy function to find terminal state. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>*</parametername>
</parameternamelist>
<parameterdescription>
<para>(state) The current state. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>It&apos;s of no use but so lets keep it false. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/environment/env_type.hpp" line="106" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/environment/env_type.hpp" bodystart="106" bodyend="106"/>
      </memberdef>
      <memberdef kind="function" id="classmlpack_1_1rl_1_1DiscreteActionEnv_1a1e46b501bcca9ffdaab87a8cd6002d35" prot="public" static="no" const="no" explicit="no" inline="yes" virt="non-virtual">
        <type>double</type>
        <definition>double Sample</definition>
        <argsstring>(const State &amp;, const Action &amp;, State &amp;)</argsstring>
        <name>Sample</name>
        <param>
          <type>const <ref refid="classmlpack_1_1rl_1_1DiscreteActionEnv_1_1State" kindref="compound">State</ref> &amp;</type>
        </param>
        <param>
          <type>const <ref refid="classmlpack_1_1rl_1_1DiscreteActionEnv_1_1Action" kindref="compound">Action</ref> &amp;</type>
        </param>
        <param>
          <type><ref refid="classmlpack_1_1rl_1_1DiscreteActionEnv_1_1State" kindref="compound">State</ref> &amp;</type>
        </param>
        <briefdescription>
<para>Dummy function to mimic sampling in an environment. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="param"><parameteritem>
<parameternamelist>
<parametername>*</parametername>
</parameternamelist>
<parameterdescription>
<para>(state) The current state. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>*</parametername>
</parameternamelist>
<parameterdescription>
<para>(action) The current action. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>*</parametername>
</parameternamelist>
<parameterdescription>
<para>(nextState) The next state. </para></parameterdescription>
</parameteritem>
</parameterlist>
<simplesect kind="return"><para>It&apos;s of no use so lets keep it 0. </para></simplesect>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/environment/env_type.hpp" line="89" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/environment/env_type.hpp" bodystart="89" bodyend="92"/>
      </memberdef>
      </sectiondef>
    <briefdescription>
<para>To use the dummy environment, one may start by specifying the state and action dimensions. </para>    </briefdescription>
    <detaileddescription>
<para>Eg: <programlisting><codeline><highlight class="normal">DiscreteActionEnv::State::dimension<sp/>=<sp/>4;</highlight></codeline>
<codeline><highlight class="normal">DiscreteActionEnv::Action::size<sp/>=<sp/>2;</highlight></codeline>
</programlisting></para><para>Now the <ref refid="classmlpack_1_1rl_1_1DiscreteActionEnv" kindref="compound">DiscreteActionEnv</ref> class can be used as an EnvironmentType in RL methods just as any other mlpack&apos;s implementation of gym environments. </para>    </detaileddescription>
    <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/environment/env_type.hpp" line="33" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/environment/env_type.hpp" bodystart="32" bodyend="107"/>
    <listofallmembers>
      <member refid="classmlpack_1_1rl_1_1DiscreteActionEnv_1aa9f537249fa0c1e62b38197996ab4c6a" prot="public" virt="non-virtual"><scope>mlpack::rl::DiscreteActionEnv</scope><name>InitialSample</name></member>
      <member refid="classmlpack_1_1rl_1_1DiscreteActionEnv_1a7061ffc3be37983a46a312973a375c0a" prot="public" virt="non-virtual"><scope>mlpack::rl::DiscreteActionEnv</scope><name>IsTerminal</name></member>
      <member refid="classmlpack_1_1rl_1_1DiscreteActionEnv_1a1e46b501bcca9ffdaab87a8cd6002d35" prot="public" virt="non-virtual"><scope>mlpack::rl::DiscreteActionEnv</scope><name>Sample</name></member>
    </listofallmembers>
  </compounddef>
</doxygen>

<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.8.13">
  <compounddef id="namespacemlpack_1_1rl" kind="namespace" language="C++">
    <compoundname>mlpack::rl</compoundname>
    <innerclass refid="classmlpack_1_1rl_1_1Acrobot" prot="public">mlpack::rl::Acrobot</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1AggregatedPolicy" prot="public">mlpack::rl::AggregatedPolicy</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1AsyncLearning" prot="public">mlpack::rl::AsyncLearning</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1CartPole" prot="public">mlpack::rl::CartPole</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1CategoricalDQN" prot="public">mlpack::rl::CategoricalDQN</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1ContinuousActionEnv" prot="public">mlpack::rl::ContinuousActionEnv</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1ContinuousDoublePoleCart" prot="public">mlpack::rl::ContinuousDoublePoleCart</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1ContinuousMountainCar" prot="public">mlpack::rl::ContinuousMountainCar</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1DiscreteActionEnv" prot="public">mlpack::rl::DiscreteActionEnv</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1DoublePoleCart" prot="public">mlpack::rl::DoublePoleCart</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1DuelingDQN" prot="public">mlpack::rl::DuelingDQN</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1GreedyPolicy" prot="public">mlpack::rl::GreedyPolicy</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1MountainCar" prot="public">mlpack::rl::MountainCar</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1NStepQLearningWorker" prot="public">mlpack::rl::NStepQLearningWorker</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1OneStepQLearningWorker" prot="public">mlpack::rl::OneStepQLearningWorker</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1OneStepSarsaWorker" prot="public">mlpack::rl::OneStepSarsaWorker</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1Pendulum" prot="public">mlpack::rl::Pendulum</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1PrioritizedReplay" prot="public">mlpack::rl::PrioritizedReplay</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1QLearning" prot="public">mlpack::rl::QLearning</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1RandomReplay" prot="public">mlpack::rl::RandomReplay</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1RewardClipping" prot="public">mlpack::rl::RewardClipping</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1SAC" prot="public">mlpack::rl::SAC</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1SimpleDQN" prot="public">mlpack::rl::SimpleDQN</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1SumTree" prot="public">mlpack::rl::SumTree</innerclass>
    <innerclass refid="classmlpack_1_1rl_1_1TrainingConfig" prot="public">mlpack::rl::TrainingConfig</innerclass>
      <sectiondef kind="typedef">
      <memberdef kind="typedef" id="namespacemlpack_1_1rl_1a5af8bcbe29f6b50332ea2ac7d7dd521b" prot="public" static="no">
        <type><ref refid="classmlpack_1_1rl_1_1AsyncLearning" kindref="compound">AsyncLearning</ref>&lt; <ref refid="classmlpack_1_1rl_1_1NStepQLearningWorker" kindref="compound">NStepQLearningWorker</ref>&lt; EnvironmentType, NetworkType, UpdaterType, PolicyType &gt;, EnvironmentType, NetworkType, UpdaterType, PolicyType &gt;</type>
        <definition>using NStepQLearning =  AsyncLearning&lt;NStepQLearningWorker&lt;EnvironmentType, NetworkType, UpdaterType, PolicyType&gt;, EnvironmentType, NetworkType, UpdaterType, PolicyType&gt;</definition>
        <argsstring></argsstring>
        <name>NStepQLearning</name>
        <briefdescription>
<para>Convenient typedef for async n step q-learning. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="templateparam"><parameteritem>
<parameternamelist>
<parametername>EnvironmentType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the reinforcement learning task. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>NetworkType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the network model. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>UpdaterType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the optimizer. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>PolicyType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the behavior policy. </para></parameterdescription>
</parameteritem>
</parameterlist>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/async_learning.hpp" line="231" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/async_learning.hpp" bodystart="233" bodyend="-1"/>
      </memberdef>
      <memberdef kind="typedef" id="namespacemlpack_1_1rl_1a3c2fb6897a13583e67422ad12286f6d1" prot="public" static="no">
        <type><ref refid="classmlpack_1_1rl_1_1AsyncLearning" kindref="compound">AsyncLearning</ref>&lt; <ref refid="classmlpack_1_1rl_1_1OneStepQLearningWorker" kindref="compound">OneStepQLearningWorker</ref>&lt; EnvironmentType, NetworkType, UpdaterType, PolicyType &gt;, EnvironmentType, NetworkType, UpdaterType, PolicyType &gt;</type>
        <definition>using OneStepQLearning =  AsyncLearning&lt;OneStepQLearningWorker&lt;EnvironmentType, NetworkType, UpdaterType, PolicyType&gt;, EnvironmentType, NetworkType, UpdaterType, PolicyType&gt;</definition>
        <argsstring></argsstring>
        <name>OneStepQLearning</name>
        <briefdescription>
<para>Convenient typedef for async one step q-learning. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="templateparam"><parameteritem>
<parameternamelist>
<parametername>EnvironmentType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the reinforcement learning task. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>NetworkType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the network model. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>UpdaterType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the optimizer. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>PolicyType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the behavior policy. </para></parameterdescription>
</parameteritem>
</parameterlist>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/async_learning.hpp" line="195" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/async_learning.hpp" bodystart="197" bodyend="-1"/>
      </memberdef>
      <memberdef kind="typedef" id="namespacemlpack_1_1rl_1ae44ae2a1270ac28a3d57dd93439839a4" prot="public" static="no">
        <type><ref refid="classmlpack_1_1rl_1_1AsyncLearning" kindref="compound">AsyncLearning</ref>&lt; <ref refid="classmlpack_1_1rl_1_1OneStepSarsaWorker" kindref="compound">OneStepSarsaWorker</ref>&lt; EnvironmentType, NetworkType, UpdaterType, PolicyType &gt;, EnvironmentType, NetworkType, UpdaterType, PolicyType &gt;</type>
        <definition>using OneStepSarsa =  AsyncLearning&lt;OneStepSarsaWorker&lt;EnvironmentType, NetworkType, UpdaterType, PolicyType&gt;, EnvironmentType, NetworkType, UpdaterType, PolicyType&gt;</definition>
        <argsstring></argsstring>
        <name>OneStepSarsa</name>
        <briefdescription>
<para>Convenient typedef for async one step Sarsa. </para>        </briefdescription>
        <detaileddescription>
<para><parameterlist kind="templateparam"><parameteritem>
<parameternamelist>
<parametername>EnvironmentType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the reinforcement learning task. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>NetworkType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the network model. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>UpdaterType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the optimizer. </para></parameterdescription>
</parameteritem>
<parameteritem>
<parameternamelist>
<parametername>PolicyType</parametername>
</parameternamelist>
<parameterdescription>
<para>The type of the behavior policy. </para></parameterdescription>
</parameteritem>
</parameterlist>
</para>        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/async_learning.hpp" line="213" column="1" bodyfile="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/async_learning.hpp" bodystart="215" bodyend="-1"/>
      </memberdef>
      </sectiondef>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
    </detaileddescription>
    <location file="/home/aakash/mlpack/src/mlpack/methods/reinforcement_learning/async_learning.hpp" line="24" column="1"/>
  </compounddef>
</doxygen>

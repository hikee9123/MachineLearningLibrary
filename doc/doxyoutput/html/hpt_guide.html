<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>mlpack: Hyper-Parameter Tuning</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="extra-stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">mlpack
   &#160;<span id="projectnumber">3.4.2</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Hyper-Parameter Tuning </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h1><a class="anchor" id="hptintro"></a>
Introduction</h1>
<p><b>mlpack</b> implements a generic hyperparameter tuner that is able to tune both continuous and discrete parameters of various different algorithms. This is an important task&mdash;the performance of many machine learning algorithms can be highly dependent on the hyperparameters that are chosen for that algorithm. (One example: the choice of <img class="formulaInl" alt="$k$" src="form_174.png"/> for a <img class="formulaInl" alt="$k$" src="form_174.png"/>-nearest-neighbors classifier.)</p>
<p>This hyper-parameter tuner is built on the same general concept as the cross-validation classes (see the <a class="el" href="cv.html">cross-validation tutorial</a>): given some machine learning algorithm, some data, some performance measure, and a set of hyperparameters, attempt to find the hyperparameter set that best optimizes the performance measure on the given data with the given algorithm.</p>
<p><b>mlpack's</b> implementation of hyperparameter tuning is flexible, and is built in a way that supports many algorithms and many optimizers. At the time of this writing, complex hyperparameter optimization techniques are not available, but the hyperparameter tuner does support these, should they be implemented in the future.</p>
<p>In this tutorial we will see the usage examples of the hyper-parameter tuning module, and also more details about the <code><a class="el" href="classmlpack_1_1hpt_1_1HyperParameterTuner.html" title="The class HyperParameterTuner for the given MLAlgorithm utilizes the provided Optimizer to find the v...">HyperParameterTuner</a></code> class.</p>
<h1><a class="anchor" id="hptbasic"></a>
Basic Usage</h1>
<p>The interface of the hyper-parameter tuning module is quite similar to the interface of the <a class="el" href="cv.html">cross-validation module</a>. To construct a <code><a class="el" href="classmlpack_1_1hpt_1_1HyperParameterTuner.html" title="The class HyperParameterTuner for the given MLAlgorithm utilizes the provided Optimizer to find the v...">HyperParameterTuner</a></code> object you need to specify as template parameters what machine learning algorithm, cross-validation strategy, performance measure, and optimization strategy (<code>ens::GridSearch</code> will be used by default) you are going to use. Then, you must pass the same arguments as for the cross-validation classes: the data and labels (or responses) to use are given to the constructor, and the possible hyperparameter values are given to the <code><a class="el" href="classmlpack_1_1hpt_1_1HyperParameterTuner.html#a4e04da235ec0434d69613c547b20dbea" title="Find the best hyper-parameters by using the given Optimizer. ">HyperParameterTuner::Optimize()</a></code> method, which returns the best algorithm configuration as a <code>std::tuple&lt;&gt;</code>.</p>
<p>Let's see some examples.</p>
<p>Suppose we have the following data to train and validate on. </p><div class="fragment"><div class="line"><span class="comment">// 100-point 5-dimensional random dataset.</span></div><div class="line">arma::mat data = arma::randu&lt;arma::mat&gt;(5, 100);</div><div class="line"><span class="comment">// Noisy responses retrieved by a random linear transformation of data.</span></div><div class="line">arma::rowvec responses = arma::randu&lt;arma::rowvec&gt;(5) * data +</div><div class="line">    0.1 * arma::randn&lt;arma::rowvec&gt;(100);</div></div><!-- fragment --><p>Given the dataset above, we can use the following code to try to find a good <code>lambda</code> value for <a class="el" href="classmlpack_1_1regression_1_1LinearRegression.html">LinearRegression</a>. Here we use <a class="el" href="classmlpack_1_1cv_1_1SimpleCV.html">SimpleCV</a> instead of k-fold cross-validation to save computation time.</p>
<div class="fragment"><div class="line"><span class="comment">// Using 80% of data for training and remaining 20% for assessing MSE.</span></div><div class="line"><span class="keywordtype">double</span> validationSize = 0.2;</div><div class="line">HyperParameterTuner&lt;LinearRegression, MSE, SimpleCV&gt; hpt(validationSize,</div><div class="line">    data, responses);</div><div class="line"></div><div class="line"><span class="comment">// Finding a good value for lambda from the discrete set of values 0.0, 0.001,</span></div><div class="line"><span class="comment">// 0.01, 0.1, and 1.0.</span></div><div class="line">arma::vec lambdas{0.0, 0.001, 0.01, 0.1, 1.0};</div><div class="line"><span class="keywordtype">double</span> bestLambda;</div><div class="line">std::tie(bestLambda) = hpt.Optimize(lambdas);</div></div><!-- fragment --><p>In this example we have used <code>ens::GridSearch</code> (the default optimizer) to find a good value for the <code>lambda</code> hyper-parameter. For that we have specified what values should be tried.</p>
<h1><a class="anchor" id="hptfixed"></a>
Fixed Arguments</h1>
<p>When some hyper-parameters should not be optimized, you can specify values for them with the <code><a class="el" href="namespacemlpack_1_1hpt.html#ad773f4d1def8deb412ffbf37bdf289ec" title="Mark the given argument as one that should be fixed. ">Fixed()</a></code> method as in the following example of trying to find good <code>lambda1</code> and <code>lambda2</code> values for <a class="el" href="classmlpack_1_1regression_1_1LARS.html">LARS</a> (least-angle regression).</p>
<div class="fragment"><div class="line">HyperParameterTuner&lt;LARS, MSE, SimpleCV&gt; hpt2(validationSize, data,</div><div class="line">    responses);</div><div class="line"></div><div class="line"><span class="comment">// The hyper-parameter tuner should not try to change the transposeData or</span></div><div class="line"><span class="comment">// useCholesky parameters.</span></div><div class="line"><span class="keywordtype">bool</span> transposeData = <span class="keyword">true</span>;</div><div class="line"><span class="keywordtype">bool</span> useCholesky = <span class="keyword">false</span>;</div><div class="line"></div><div class="line"><span class="comment">// We wish only to search for the best lambda1 and lambda2 values.</span></div><div class="line">arma::vec lambda1Set{0.0, 0.001, 0.01, 0.1, 1.0};</div><div class="line">arma::vec lambda2Set{0.0, 0.002, 0.02, 0.2, 2.0};</div><div class="line"></div><div class="line"><span class="keywordtype">double</span> bestLambda1, bestLambda2;</div><div class="line">std::tie(bestLambda1, bestLambda2) = hpt2.Optimize(<a class="code" href="namespacemlpack_1_1hpt.html#ad773f4d1def8deb412ffbf37bdf289ec">Fixed</a>(transposeData),</div><div class="line">    <a class="code" href="namespacemlpack_1_1hpt.html#ad773f4d1def8deb412ffbf37bdf289ec">Fixed</a>(useCholesky), lambda1Set, lambda2Set);</div></div><!-- fragment --><p>Note that for the call to <code>hpt2.Optimize()</code>, we have used the same order of arguments as they appear in the corresponding <a class="el" href="classmlpack_1_1regression_1_1LARS.html">LARS</a> constructor:</p>
<div class="fragment"><div class="line">LARS(<span class="keyword">const</span> arma::mat&amp; data,</div><div class="line">     <span class="keyword">const</span> arma::rowvec&amp; responses,</div><div class="line">     <span class="keyword">const</span> <span class="keywordtype">bool</span> transposeData = <span class="keyword">true</span>,</div><div class="line">     <span class="keyword">const</span> <span class="keywordtype">bool</span> useCholesky = <span class="keyword">false</span>,</div><div class="line">     <span class="keyword">const</span> <span class="keywordtype">double</span> lambda1 = 0.0,</div><div class="line">     <span class="keyword">const</span> <span class="keywordtype">double</span> lambda2 = 0.0,</div><div class="line">     <span class="keyword">const</span> <span class="keywordtype">double</span> tolerance = 1e-16);</div></div><!-- fragment --><h1><a class="anchor" id="hptgradient"></a>
Gradient-Based Optimization</h1>
<p>In some cases we may wish to optimize a hyperparameter over the space of all possible real values, instead of providing a grid in which to search. Alternately, we may know approximately optimal values from a grid search for real-valued hyperparameters, but wish to further tune those values.</p>
<p>In this case, we can use a gradient-based optimizer for hyperparameter search. In the following example, we try to optimize the <code>lambda1</code> and <code>lambda2</code> hyper-parameters for <a class="el" href="classmlpack_1_1regression_1_1LARS.html">LARS</a> with the <code>ens::GradientDescent</code> optimizer.</p>
<div class="fragment"><div class="line">HyperParameterTuner&lt;LARS, MSE, SimpleCV, GradientDescent&gt; hpt3(validationSize,</div><div class="line">    data, responses);</div><div class="line"></div><div class="line"><span class="comment">// GradientDescent can be adjusted in the following way.</span></div><div class="line">hpt3.Optimizer().StepSize() = 0.1;</div><div class="line">hpt3.Optimizer().Tolerance() = 1e-15;</div><div class="line"></div><div class="line"><span class="comment">// We can set up values used for calculating gradients.</span></div><div class="line">hpt3.RelativeDelta() = 0.01;</div><div class="line">hpt3.MinDelta() = 1e-10;</div><div class="line"></div><div class="line"><span class="keywordtype">double</span> initialLambda1 = 0.001;</div><div class="line"><span class="keywordtype">double</span> initialLambda2 = 0.002;</div><div class="line"></div><div class="line"><span class="keywordtype">double</span> bestGDLambda1, bestGDLambda2;</div><div class="line">std::tie(bestGDLambda1, bestGDLambda2) = hpt3.Optimize(<a class="code" href="namespacemlpack_1_1hpt.html#ad773f4d1def8deb412ffbf37bdf289ec">Fixed</a>(transposeData),</div><div class="line">    <a class="code" href="namespacemlpack_1_1hpt.html#ad773f4d1def8deb412ffbf37bdf289ec">Fixed</a>(useCholesky), initialLambda1, initialLambda2);</div></div><!-- fragment --><h1><a class="anchor" id="hpt_class"></a>
The HyperParameterTuner class</h1>
<p>The <code><a class="el" href="classmlpack_1_1hpt_1_1HyperParameterTuner.html" title="The class HyperParameterTuner for the given MLAlgorithm utilizes the provided Optimizer to find the v...">HyperParameterTuner</a></code> class is very similar to the <a class="el" href="classmlpack_1_1cv_1_1KFoldCV.html">KFoldCV</a> and <a class="el" href="classmlpack_1_1cv_1_1SimpleCV.html">SimpleCV</a> classes (see the <a class="el" href="cv.html">cross-validation tutorial</a> for more information on those two classes), but there are a few important differences.</p>
<p>First, the <code><a class="el" href="classmlpack_1_1hpt_1_1HyperParameterTuner.html" title="The class HyperParameterTuner for the given MLAlgorithm utilizes the provided Optimizer to find the v...">HyperParameterTuner</a></code> accepts five different hyperparameters; only the first three of these are required:</p>
<ul>
<li><code>MLAlgorithm</code> This is the algorithm to be used.</li>
<li><code>Metric</code> This is the performance measure to be used; see <a class="el" href="cv.html#cvbasic_metrics">Performance measures</a> for more information.</li>
<li><code>CVType</code> This is the type of cross-validation to be used for evaluating the performance measure; this should be <a class="el" href="classmlpack_1_1cv_1_1KFoldCV.html">KFoldCV</a> or <a class="el" href="classmlpack_1_1cv_1_1SimpleCV.html">SimpleCV</a>.</li>
<li><code>OptimizerType</code> This is the type of optimizer to use; it can be <code>GridSearch</code> or a gradient-based optimizer.</li>
<li><code>MatType</code> This is the type of data matrix to use. The default is <code>arma::mat</code>. This only needs to be changed if you are specifically using sparse data, or if you want to use a numeric type other than <code>double</code>.</li>
</ul>
<p>The last two template parameters are automatically inferred by the <code><a class="el" href="classmlpack_1_1hpt_1_1HyperParameterTuner.html" title="The class HyperParameterTuner for the given MLAlgorithm utilizes the provided Optimizer to find the v...">HyperParameterTuner</a></code> and should not need to be manually specified, unless an unconventional data type like <code>arma::fmat</code> is being used for data points.</p>
<p>Typically, <a class="el" href="classmlpack_1_1cv_1_1SimpleCV.html">SimpleCV</a> is a good choice for <code>CVType</code> because it takes so much less time to compute than full <a class="el" href="classmlpack_1_1cv_1_1KFoldCV.html">KFoldCV</a>; however, the disadvantage is that <a class="el" href="classmlpack_1_1cv_1_1SimpleCV.html">SimpleCV</a> might give a somewhat more noisy estimate of the performance measure on unseen test data.</p>
<p>The constructor for the <code><a class="el" href="classmlpack_1_1hpt_1_1HyperParameterTuner.html" title="The class HyperParameterTuner for the given MLAlgorithm utilizes the provided Optimizer to find the v...">HyperParameterTuner</a></code> is called with exactly the same arguments as the corresponding <code>CVType</code> that has been chosen. For more information on that, please see the <a class="el" href="cv.html#cvbasic_api">cross-validation constructor tutorial</a>. As an example, if we are using <a class="el" href="classmlpack_1_1cv_1_1SimpleCV.html">SimpleCV</a> and wish to hold out 20% of the dataset as a validation set, we might construct a <code><a class="el" href="classmlpack_1_1hpt_1_1HyperParameterTuner.html" title="The class HyperParameterTuner for the given MLAlgorithm utilizes the provided Optimizer to find the v...">HyperParameterTuner</a></code> like this:</p>
<div class="fragment"><div class="line"><span class="comment">// We will use LinearRegression as the MLAlgorithm, and MSE as the performance</span></div><div class="line"><span class="comment">// measure.  Our dataset is &#39;dataset&#39; and the responses are &#39;responses&#39;.</span></div><div class="line">HyperParameterTuner&lt;LinearRegression, MSE, SimpleCV&gt; hpt(0.2, dataset,</div><div class="line">    responses);</div></div><!-- fragment --><p>Next, we must set up the hyperparameters to be optimized. If we are doing a grid search with the <code>ens::GridSearch</code> optimizer (the default), then we only need to pass a <code>std::vector</code> (for non-numeric hyperparameters) or an <code>arma::vec</code> (for numeric hyperparameters) containing all of the possible choices that we wish to search over.</p>
<p>For instance, a set of numeric values might be chosen like this, for the <code>lambda</code> parameter (of type <code>double</code>):</p>
<div class="fragment"><div class="line">arma::vec lambdaSet = arma::vec(<span class="stringliteral">&quot;0.0 0.1 0.5 1.0&quot;</span>);</div></div><!-- fragment --><p>Similarly, a set of non-numeric values might be chosen like this, for the <code>intercept</code> parameter:</p>
<div class="fragment"><div class="line">std::vector&lt;bool&gt; interceptSet = { <span class="keyword">false</span>, <span class="keyword">true</span> };</div></div><!-- fragment --><p>Once all of these are set up, the <code><a class="el" href="classmlpack_1_1hpt_1_1HyperParameterTuner.html#a4e04da235ec0434d69613c547b20dbea" title="Find the best hyper-parameters by using the given Optimizer. ">HyperParameterTuner::Optimize()</a></code> method may be called to find the best set of hyperparameters:</p>
<div class="fragment"><div class="line"><span class="keywordtype">bool</span> intercept;</div><div class="line"><span class="keywordtype">double</span> lambda;</div><div class="line">std::tie(lambda, intercept) = hpt.Optimize(lambdaSet, interceptSet);</div></div><!-- fragment --><p>Alternately, the <code><a class="el" href="namespacemlpack_1_1hpt.html#ad773f4d1def8deb412ffbf37bdf289ec" title="Mark the given argument as one that should be fixed. ">Fixed()</a></code> method (detailed in the <a class="el" href="hpt_guide.html#hptfixed">Fixed arguments</a> section) can be used to fix the values of some parameters.</p>
<p>For continuous optimizers like <code>ens::GradientDescent</code>, a range does not need to be specified but instead only a single value. See the <a class="el" href="hpt_guide.html#hptgradient">Gradient-Based Optimization</a> section for more details.</p>
<h1><a class="anchor" id="hptfurther"></a>
Further documentation</h1>
<p>For more information on the <code><a class="el" href="classmlpack_1_1hpt_1_1HyperParameterTuner.html" title="The class HyperParameterTuner for the given MLAlgorithm utilizes the provided Optimizer to find the v...">HyperParameterTuner</a></code> class, see the <a class="el" href="classmlpack_1_1hpt_1_1HyperParameterTuner.html" title="The class HyperParameterTuner for the given MLAlgorithm utilizes the provided Optimizer to find the v...">mlpack::hpt::HyperParameterTuner</a> class documentation and the <a class="el" href="cv.html">cross-validation tutorial</a>. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
<script type="text/javascript">
var x = document.querySelectorAll("img.formulaDsp");
var i;
for (i = 0; i < x.length; i++)
{
  x[i].width = x[i].offsetWidth / 4;
}
</script>
</html>
